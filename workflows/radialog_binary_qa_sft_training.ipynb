{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "234001954bda438da7983b75792a799a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 69 files:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% Set script for interactive development and import modules\n",
    "from RewardingVisualDoubt import infrastructure, training\n",
    "\n",
    "infrastructure.make_ipython_reactive_to_changing_codebase()\n",
    "infrastructure.supress_known_warnings()\n",
    "\n",
    "import pathlib as path\n",
    "import typing as t\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import accelerate\n",
    "import dataclasses\n",
    "import transformers\n",
    "import wandb\n",
    "\n",
    "from trl import PPOConfig, PPOTrainer\n",
    "\n",
    "from RewardingVisualDoubt import dataset, prompter, shared, vllm, response, reward\n",
    "from RewardingVisualDoubt import training as training\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"da3cb086bbc110c16cbc5ba4c284a19b0b461710\"\n",
    "\n",
    "from LLAVA_Biovil.llava.mm_utils import KeywordsStoppingCriteria\n",
    "\n",
    "STOP_STR = prompter.Seperator.END_OF_SEQUENCE_SEPERATOR.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model in non-trainable mode...\n",
      "Precision: 4bit quantized\n",
      "Model base:  liuhaotian/llava-v1.5-7b\n",
      "Loading LLaVA from base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7084e1b76aab41138c79b6d328bc019c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7008c45a8ea8461e97c2404af308bf22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading additional LLaVA weights...\n",
      "Using downloaded and verified file: /tmp/biovil_t_image_model_proj_size_128.pt\n",
      "Loaded additional vision tower weights...\n",
      "Adding pretrained RaDialog LoRA adapters to the model...\n"
     ]
    }
   ],
   "source": [
    "device_str = (\n",
    "    shared.torch_devices.cuda.value if torch.cuda.is_available() else shared.torch_devices.cpu.value\n",
    ")\n",
    "device = torch.device(device_str)\n",
    "\n",
    "model = vllm.load_pretrained_llava_model_for_sft_training(device_str=device_str, precision=\"4bit\")\n",
    "\n",
    "tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "padding_tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "padding_tokenizer.padding_side = \"left\"\n",
    "padding_tokenizer.pad_token = padding_tokenizer.bos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the datasets and the dataloaders...\n",
      "Loading mimic_cxr_df from cache\n",
      "Loading balanced_binary_qa_mimic_cxr_df from cache\n",
      "Loading mimic_cxr_df from cache\n",
      "Loading balanced_binary_qa_mimic_cxr_df from cache\n"
     ]
    }
   ],
   "source": [
    "######################################## 2. Load the datasets and the dataloaders ########################################\n",
    "\n",
    "\n",
    "DEFAULT_BATCH_SIZE = 4\n",
    "DEFAULT_OUTPUT_DIR = path.Path(\"output\")\n",
    "\n",
    "\n",
    "print(\"Loading the datasets and the dataloaders...\")\n",
    "dataset_train = dataset.get_binary_qa_prompted_mimic_cxr_llava_model_input_dataset_for_sft(\n",
    "    split=dataset.DatasetSplit.TRAIN,\n",
    "    tokenizer=tokenizer,\n",
    "    prompter=prompter.build_binary_qa_prompt_with_response_and_confidence_for_sft,\n",
    ")\n",
    "dataset_eval = dataset.get_binary_qa_prompted_mimic_cxr_llava_model_input_dataset_for_sft(\n",
    "    split=dataset.DatasetSplit.VALIDATION,\n",
    "    tokenizer=tokenizer,\n",
    "    prompter=prompter.build_binary_qa_prompt_with_response_and_confidence_for_sft,\n",
    ")\n",
    "\n",
    "padding_tokenizer.pad_token = padding_tokenizer.bos_token  # TODO how about this?\n",
    "\n",
    "dataloader_train = dataset.get_mimic_cxr_llava_model_input_dataloader_for_sft(\n",
    "    dataset=dataset_train,\n",
    "    batch_size=DEFAULT_BATCH_SIZE,\n",
    "    padding_tokenizer=padding_tokenizer,\n",
    "    num_workers=8,  # Let Torch decide.\n",
    ")\n",
    "\n",
    "dataloader_eval = dataset.get_mimic_cxr_llava_model_input_dataloader_for_sft(\n",
    "    dataset=dataset_eval,\n",
    "    batch_size=DEFAULT_BATCH_SIZE,\n",
    "    padding_tokenizer=padding_tokenizer,\n",
    "    num_workers=8,  # Let Torch decide.\n",
    ")\n",
    "\n",
    "eval_batch_iterator = iter(dataloader_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Optimizer & Scheduler ----\n",
    "NUM_EPOCHS = 1\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "lr_scheduler = transformers.get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=len(dataloader_train) * NUM_EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/135572 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/135572 [00:01<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# ---- Training & Checkpointing ----\n",
    "NUM_EPOCHS = 1\n",
    "CHECKPOINT_DIR = \"training_checkpoints\"\n",
    "GRAD_ACCUM_STEPS = 8  # TODO select value\n",
    "LOGGING_STEPS = 100\n",
    "STEPS_UNTIL_CHECKPOINT = 1000\n",
    "NUM_BATCHES_TO_EVALUATE = 15\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "model.train()\n",
    "best_val_loss = float(\"inf\")\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    loop = tqdm(dataloader_train, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "\n",
    "    for step, batch in enumerate(loop):\n",
    "        batch: dataset.MimicCxrLlavaModelInputBatchDictForSFT = batch\n",
    "        batch_llava_model_input_dict = batch[\"batch_llava_model_input_dict\"]\n",
    "        batch_llava_model_input_dict = dataset.move_llava_model_input_dict_to_device(\n",
    "            batch_llava_model_input_dict, device\n",
    "        )\n",
    "        input_ids, images = (\n",
    "            batch_llava_model_input_dict[\"text_prompt_input_ids\"],\n",
    "            batch_llava_model_input_dict[\"images\"],\n",
    "        )\n",
    "        labels = batch[\"batch_expected_output_ids\"].to(device)\n",
    "        attention_mask = batch[\"batch_attention_mask\"].to(device)\n",
    "\n",
    "        torch.all(\n",
    "            (input_ids == labels) | (labels == -100)\n",
    "        ).item()  # Verify that labels only differ when -100 token is present\n",
    "\n",
    "        # ---- Forward & Backward Pass ----\n",
    "        outputs = model(\n",
    "            input_ids=input_ids, images=images, attention_mask=attention_mask, labels=labels\n",
    "        )  # batch has input_ids, attention_mask, labels\n",
    "        loss = outputs.loss / GRAD_ACCUM_STEPS\n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # ---- Logging ----\n",
    "        if step % LOGGING_STEPS == 0:\n",
    "            wandb.log({\"train_loss\": loss.item() * GRAD_ACCUM_STEPS})\n",
    "\n",
    "        # ---- Validation & Checkpointing ----\n",
    "        if (step + 1) % STEPS_UNTIL_CHECKPOINT == 0:\n",
    "            print(f\"Arrived at checkpoint {step + 1}.\")\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            for _ in range(NUM_BATCHES_TO_EVALUATE):\n",
    "                try:\n",
    "                    eval_batch = next(eval_batch_iterator)\n",
    "                except StopIteration:\n",
    "                    eval_batch_iterator = iter(dataloader_eval)\n",
    "                    eval_batch = next(eval_batch_iterator)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    val_outputs = model(**eval_batch)\n",
    "                    val_loss = val_outputs.loss.item()\n",
    "                    val_losses.append(val_loss)\n",
    "\n",
    "            avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "            wandb.log({\"val_loss\": avg_val_loss})\n",
    "\n",
    "            # ---- Checkpoint Saving ----\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                checkpoint_path = os.path.join(\n",
    "                    CHECKPOINT_DIR, f\"best_model_epoch{epoch}_step{step}.pth\"\n",
    "                )\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(f\"ðŸ”¥ Saved best model at {checkpoint_path}\")\n",
    "\n",
    "            model.train()  # Resume training\n",
    "\n",
    "wandb.finish()\n",
    "# torch.save(model.state_dict(), \"llava_lora_final.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mimic_cxr_df from cache\n",
      "Loading balanced_binary_qa_mimic_cxr_df from cache\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=18460230, study_id=53631792, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p18/p18460230/s53631792/369dc5bd-70bd89d0-2d90fa80-f319ec1d-fb2802aa.jpg', disease=<ChexpertFinding.PLEURAL_EFFUSION: 'Pleural Effusion'>, label=<ChexpertLabel.POSITIVE: 1.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pleural Effusion?  ASSISTANT:\"]\n",
      "Label: tensor([1.])\n",
      "File_idx 0, ASSISTANT:  Yes, the patient has pleural effusion.\n",
      "File_idx 0, ASSISTANT (after confidence request):  {\"confidence\": 9}\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=13263843, study_id=52138943, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p13/p13263843/s52138943/de739d0b-2345495b-255f0e3b-00ccbf4c-ab4d3400.jpg', disease=<ChexpertFinding.PLEURAL_EFFUSION: 'Pleural Effusion'>, label=<ChexpertLabel.POSITIVE: 1.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pleural Effusion?  ASSISTANT:\"]\n",
      "Label: tensor([1.])\n",
      "File_idx 1, ASSISTANT:  Yes, the patient has pleural effusion.\n",
      "File_idx 1, ASSISTANT (after confidence request):  {\"confidence\": 9}\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=16050730, study_id=57637607, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p16/p16050730/s57637607/adb48138-344feb7e-14e31d10-2639c54e-0b5a95d7.jpg', disease=<ChexpertFinding.PNEUMOTHORAX: 'Pneumothorax'>, label=<ChexpertLabel.NEGATIVE: 0.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pneumothorax?  ASSISTANT:\"]\n",
      "Label: tensor([0.])\n",
      "File_idx 2, ASSISTANT:  No, the patient does not have pneumothorax.\n",
      "File_idx 2, ASSISTANT (after confidence request):  {\"confidence\": 9}\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=11569093, study_id=57204814, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p11/p11569093/s57204814/0d2a50a2-3711662a-d7838521-4dc58d09-3732a3ad.jpg', disease=<ChexpertFinding.SUPPORT_DEVICES: 'Support Devices'>, label=<ChexpertLabel.POSITIVE: 1.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Support Devices?  ASSISTANT:\"]\n",
      "Label: tensor([1.])\n",
      "File_idx 3, ASSISTANT:  Yes, the patient has support devices.\n",
      "File_idx 3, ASSISTANT (after confidence request):  {\"confidence\": 8}\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=15192710, study_id=58836461, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p15/p15192710/s58836461/dc93422b-fd5ec685-19eb4eba-fb31f8d0-b60d8b47.jpg', disease=<ChexpertFinding.PNEUMOTHORAX: 'Pneumothorax'>, label=<ChexpertLabel.NEGATIVE: 0.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pneumothorax?  ASSISTANT:\"]\n",
      "Label: tensor([0.])\n",
      "File_idx 4, ASSISTANT:  No, the image shows no pneumothorax.\n",
      "File_idx 4, ASSISTANT (after confidence request):  {\"confidence\": 9}\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=16622813, study_id=50921860, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p16/p16622813/s50921860/066a59e3-316782a3-2d4238bc-d5354678-1ec6dcd9.jpg', disease=<ChexpertFinding.CARDIOMEGALY: 'Cardiomegaly'>, label=<ChexpertLabel.NEGATIVE: 0.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Cardiomegaly?  ASSISTANT:\"]\n",
      "Label: tensor([0.])\n",
      "File_idx 5, ASSISTANT:  No, the patient does not have cardiomegaly.\n",
      "File_idx 5, ASSISTANT (after confidence request):  {\"confidence\": 9}\n"
     ]
    }
   ],
   "source": [
    "from RewardingVisualDoubt import inference\n",
    "\n",
    "tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "padding_tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "padding_tokenizer.padding_side = \"left\"\n",
    "padding_tokenizer.pad_token_id = padding_tokenizer.bos_token_id\n",
    "dataset_test = dataset.get_binary_qa_prompted_mimic_cxr_llava_model_input_dataset(\n",
    "    split=dataset.DatasetSplit.TEST,\n",
    "    tokenizer=tokenizer,\n",
    "    prompter=prompter.build_binary_qa_instruction_from_disease_under_study,\n",
    ")\n",
    "dataloader_test = dataset.get_mimic_cxr_llava_model_input_dataloader(\n",
    "    dataset=dataset_test, batch_size=1, padding_tokenizer=padding_tokenizer, num_workers=8\n",
    ")\n",
    "\n",
    "for idx, batch in enumerate(dataloader_test):\n",
    "    batch = t.cast(dataset.MimicCxrLlavaModelInputBatchDict, batch)\n",
    "    batch_llava_model_input_dict = batch[\"batch_llava_model_input_dict\"]\n",
    "    batch_llava_model_input_dict = dataset.move_llava_model_input_dict_to_device(\n",
    "        batch_llava_model_input_dict, torch.device(shared.torch_devices.cuda.value)\n",
    "    )\n",
    "    input_ids, images = (\n",
    "        batch_llava_model_input_dict[\"text_prompt_input_ids\"],\n",
    "        batch_llava_model_input_dict[\"images\"],\n",
    "    )\n",
    "    stopping_criteria = KeywordsStoppingCriteria([STOP_STR], tokenizer, input_ids)\n",
    "    pred = inference.generate_radialog_answer_for_binary_qa_for_single_study(\n",
    "        model, tokenizer, input_ids, images, stopping_criteria\n",
    "    )\n",
    "    confidence_request_prompt = (\n",
    "        batch[\"batch_prompts\"][0]\n",
    "        + \" \"\n",
    "        + pred\n",
    "        + \" \"\n",
    "        + prompter.build_post_generation_user_confidence_request()  # POST_GENERATION_CONFIDENCE_REQUEST_8\n",
    "    )\n",
    "    confidence_request_input_ids = torch.unsqueeze(\n",
    "        torch.IntTensor(tokenizer(confidence_request_prompt)[\"input_ids\"]), 0\n",
    "    ).to(device)\n",
    "    stopping_criteria = KeywordsStoppingCriteria(\n",
    "        [STOP_STR], tokenizer, confidence_request_input_ids\n",
    "    )\n",
    "    pred_with_confidence = inference.generate_radialog_answer_for_binary_qa_for_single_study(\n",
    "        model, tokenizer, confidence_request_input_ids, images, stopping_criteria\n",
    "    )\n",
    "    print(f\"\\n Metadata: {batch['batch_mimic_cxr_datapoint_metadata']}\")\n",
    "    print(f\"Prompt: {batch['batch_prompts']}\")\n",
    "    print(f\"Label:\", batch[\"batch_labels\"])\n",
    "    print(f\"File_idx {idx}, ASSISTANT: \", pred)\n",
    "    print(f\"File_idx {idx}, ASSISTANT (after confidence request): \", pred_with_confidence)\n",
    "    if idx == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archived Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_prompt = \"Hello, how are you?\"\n",
    "labels = \"Say, lost in London?\"\n",
    "input_ids = tokenizer(my_prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "labels = tokenizer(labels, return_tensors=\"pt\").input_ids.to(device)\n",
    "resp = model(input_ids=input_ids, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.5176, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resp.loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava_hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
