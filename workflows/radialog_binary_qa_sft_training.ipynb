{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58aeb05313234054afd48aca79c4b0da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 69 files:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% Set script for interactive development and import modules\n",
    "from RewardingVisualDoubt import infrastructure, training\n",
    "\n",
    "infrastructure.make_ipython_reactive_to_changing_codebase()\n",
    "infrastructure.supress_known_warnings()\n",
    "\n",
    "import pathlib as path\n",
    "import typing as t\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import accelerate\n",
    "import dataclasses\n",
    "import transformers\n",
    "import wandb\n",
    "\n",
    "from trl import PPOConfig, PPOTrainer\n",
    "\n",
    "from RewardingVisualDoubt import dataset, prompter, shared, vllm, response, reward\n",
    "from RewardingVisualDoubt import training as training\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"da3cb086bbc110c16cbc5ba4c284a19b0b461710\"\n",
    "\n",
    "from LLAVA_Biovil.llava.mm_utils import KeywordsStoppingCriteria\n",
    "\n",
    "STOP_STR = prompter.Seperator.END_OF_SEQUENCE_SEPERATOR.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model in non-trainable mode...\n",
      "Precision: 4bit quantized\n",
      "Model base:  liuhaotian/llava-v1.5-7b\n",
      "Loading LLaVA from base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e66ca5aaee4347b08dbdc62b2f81a296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6deae5de6cd442be99d98660e1ba0c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading additional LLaVA weights...\n",
      "Using downloaded and verified file: /tmp/biovil_t_image_model_proj_size_128.pt\n",
      "Loaded additional vision tower weights...\n",
      "Adding pretrained RaDialog LoRA adapters to the model...\n"
     ]
    }
   ],
   "source": [
    "device_str = (\n",
    "    shared.torch_devices.cuda.value if torch.cuda.is_available() else shared.torch_devices.cpu.value\n",
    ")\n",
    "device = torch.device(device_str)\n",
    "\n",
    "model = vllm.load_pretrained_llava_model_for_sft_training(device_str=device_str, precision=\"4bit\")\n",
    "\n",
    "tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "padding_tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "padding_tokenizer.padding_side = \"left\"\n",
    "padding_tokenizer.pad_token = padding_tokenizer.bos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the datasets and the dataloaders...\n",
      "Loading mimic_cxr_df from cache\n",
      "Loading balanced_binary_qa_mimic_cxr_df from cache\n",
      "Loading mimic_cxr_df from cache\n",
      "Loading balanced_binary_qa_mimic_cxr_df from cache\n"
     ]
    }
   ],
   "source": [
    "######################################## 2. Load the datasets and the dataloaders ########################################\n",
    "\n",
    "DEFAULT_BATCH_SIZE = 8\n",
    "DEFAULT_OUTPUT_DIR = path.Path(\"output\")\n",
    "\n",
    "\n",
    "print(\"Loading the datasets and the dataloaders...\")\n",
    "dataset_train = dataset.get_binary_qa_prompted_mimic_cxr_llava_model_input_dataset_for_sft(\n",
    "    split=dataset.DatasetSplit.TRAIN,\n",
    "    tokenizer=tokenizer,\n",
    "    prompter=prompter.build_binary_qa_prompt_with_response_and_confidence_for_sft,\n",
    ")\n",
    "dataset_eval = dataset.get_binary_qa_prompted_mimic_cxr_llava_model_input_dataset_for_sft(\n",
    "    split=dataset.DatasetSplit.VALIDATION,\n",
    "    tokenizer=tokenizer,\n",
    "    prompter=prompter.build_binary_qa_prompt_with_response_and_confidence_for_sft,\n",
    ")\n",
    "\n",
    "padding_tokenizer.pad_token = padding_tokenizer.bos_token  # TODO how about this?\n",
    "\n",
    "dataloader_train = dataset.get_mimic_cxr_llava_model_input_dataloader_for_sft(\n",
    "    dataset=dataset_train,\n",
    "    batch_size=DEFAULT_BATCH_SIZE,\n",
    "    padding_tokenizer=padding_tokenizer,\n",
    "    num_workers=8,  # Let Torch decide.\n",
    "    simplified_batch=False,\n",
    ")\n",
    "\n",
    "dataloader_eval = dataset.get_mimic_cxr_llava_model_input_dataloader_for_sft(\n",
    "    dataset=dataset_eval,\n",
    "    batch_size=DEFAULT_BATCH_SIZE * 2,\n",
    "    padding_tokenizer=padding_tokenizer,\n",
    "    num_workers=8,  # Let Torch decide.\n",
    "    simplified_batch=False,\n",
    ")\n",
    "\n",
    "eval_batch_iterator = iter(dataloader_eval)\n",
    "# batch = next(eval_batch_iterator)\n",
    "# for k, v in batch.items():\n",
    "#     batch[k] = v.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch: dataset.MimicCxrLlavaModelInputBatchDictForSFT = next(eval_batch_iterator)\n",
    "batch_llava_model_input_dict = batch[\"batch_llava_model_input_dict\"]\n",
    "batch_llava_model_input_dict = dataset.move_llava_model_input_dict_to_device(\n",
    "    batch_llava_model_input_dict, device\n",
    ")\n",
    "input_ids, images = (\n",
    "    batch_llava_model_input_dict[\"text_prompt_input_ids\"],\n",
    "    batch_llava_model_input_dict[\"images\"],\n",
    ")\n",
    "labels = batch[\"batch_expected_output_ids\"].to(device)\n",
    "attention_mask = batch[\"batch_attention_mask\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = model(input_ids=input_ids, images=images, attention_mask=attention_mask, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33monurdenizguler\u001b[0m (\u001b[33monurdenizguler-technical-university-of-munich\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/guests/deniz_gueler/repos/RewardingVisualDoubt/workflows/wandb/run-20250319_134803-c8mweado</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/onurdenizguler-technical-university-of-munich/radialog-confidence-score-sft/runs/c8mweado' target=\"_blank\">hearty-jazz-1</a></strong> to <a href='https://wandb.ai/onurdenizguler-technical-university-of-munich/radialog-confidence-score-sft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/onurdenizguler-technical-university-of-munich/radialog-confidence-score-sft' target=\"_blank\">https://wandb.ai/onurdenizguler-technical-university-of-munich/radialog-confidence-score-sft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/onurdenizguler-technical-university-of-munich/radialog-confidence-score-sft/runs/c8mweado' target=\"_blank\">https://wandb.ai/onurdenizguler-technical-university-of-munich/radialog-confidence-score-sft/runs/c8mweado</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [04:09,  9.23s/it]   | 27/135572 [04:00<333:25:27,  8.86s/it]\n",
      "Epoch 1/1:   0%|          | 27/135572 [04:09<347:25:00,  9.23s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 52\u001b[0m\n\u001b[1;32m     48\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m     49\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids, images\u001b[38;5;241m=\u001b[39mimages, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, labels\u001b[38;5;241m=\u001b[39mlabels\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     51\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m/\u001b[39m GRAD_ACCUM_STEPS\n\u001b[0;32m---> 52\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m GRAD_ACCUM_STEPS \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     55\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/llava_hf/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llava_hf/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ---- Training & Checkpointing ----\n",
    "# A BATCH OF 8 SAMPLES TAKES 10sec to take a training step\n",
    "NUM_EPOCHS = 1\n",
    "CHECKPOINT_DIR = \"training_checkpoints\"\n",
    "GRAD_ACCUM_STEPS = 4  # TODO select value\n",
    "LOGGING_STEPS = 1\n",
    "STEPS_UNTIL_CHECKPOINT = 1000\n",
    "NUM_BATCHES_TO_EVALUATE = 15\n",
    "LR = 5e-5\n",
    "\n",
    "# ---- Optimizer & Scheduler ----\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "lr_scheduler = transformers.get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=len(dataloader_train) * NUM_EPOCHS,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "model.train()\n",
    "best_val_loss = float(\"inf\")\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "wandb.init(project=\"radialog-confidence-score-sft\")\n",
    "\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    loop = tqdm(dataloader_train, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "\n",
    "    for step, batch in tqdm(enumerate(loop)):\n",
    "        batch: dataset.MimicCxrLlavaModelInputBatchDictForSFT = batch\n",
    "        batch_llava_model_input_dict = batch[\"batch_llava_model_input_dict\"]\n",
    "        batch_llava_model_input_dict = dataset.move_llava_model_input_dict_to_device(\n",
    "            batch_llava_model_input_dict, device\n",
    "        )\n",
    "        input_ids, images = (\n",
    "            batch_llava_model_input_dict[\"text_prompt_input_ids\"],\n",
    "            batch_llava_model_input_dict[\"images\"],\n",
    "        )\n",
    "        labels = batch[\"batch_expected_output_ids\"].to(device)\n",
    "        attention_mask = batch[\"batch_attention_mask\"].to(device)\n",
    "\n",
    "        torch.all(\n",
    "            (input_ids == labels) | (labels == -100)\n",
    "        ).item()  # Verify that labels only differ when -100 token is present\n",
    "\n",
    "        # ---- Forward & Backward Pass ----\n",
    "        outputs = model(\n",
    "            input_ids=input_ids, images=images, attention_mask=attention_mask, labels=labels\n",
    "        )\n",
    "        loss = outputs.loss / GRAD_ACCUM_STEPS\n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # ---- Logging ----\n",
    "        if step % LOGGING_STEPS == 0:\n",
    "            wandb.log({\"train_loss\": loss.item() * GRAD_ACCUM_STEPS})\n",
    "\n",
    "        # ---- Validation & Checkpointing ----\n",
    "        if (step + 1) % STEPS_UNTIL_CHECKPOINT == 0:\n",
    "            print(f\"Arrived at checkpoint {step + 1}.\")\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            for _ in range(NUM_BATCHES_TO_EVALUATE):\n",
    "                try:\n",
    "                    eval_batch = next(eval_batch_iterator)\n",
    "                except StopIteration:\n",
    "                    eval_batch_iterator = iter(dataloader_eval)\n",
    "                    eval_batch = next(eval_batch_iterator)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    val_outputs = model(**eval_batch)\n",
    "                    val_loss = val_outputs.loss.item()\n",
    "                    val_losses.append(val_loss)\n",
    "\n",
    "            avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "            wandb.log({\"val_loss\": avg_val_loss})\n",
    "\n",
    "            # ---- Checkpoint Saving ----\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                checkpoint_path = os.path.join(\n",
    "                    CHECKPOINT_DIR, f\"best_model_epoch{epoch}_step{step}.pth\"\n",
    "                )\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(f\"ðŸ”¥ Saved best model at {checkpoint_path}\")\n",
    "\n",
    "            model.train()  # Resume training\n",
    "\n",
    "wandb.finish()\n",
    "# torch.save(model.state_dict(), \"llava_lora_final.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mimic_cxr_df from cache\n",
      "Loading balanced_binary_qa_mimic_cxr_df from cache\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=18460230, study_id=53631792, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p18/p18460230/s53631792/369dc5bd-70bd89d0-2d90fa80-f319ec1d-fb2802aa.jpg', disease=<ChexpertFinding.PLEURAL_EFFUSION: 'Pleural Effusion'>, label=<ChexpertLabel.POSITIVE: 1.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pleural Effusion?  ASSISTANT:\"]\n",
      "Label: tensor([1.])\n",
      "File_idx 0, ASSISTANT:  Yes, the patient has pleural effusion.\n",
      "File_idx 0, ASSISTANT (after confidence request):  {\"confidence\": 9}\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=13263843, study_id=52138943, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p13/p13263843/s52138943/de739d0b-2345495b-255f0e3b-00ccbf4c-ab4d3400.jpg', disease=<ChexpertFinding.PLEURAL_EFFUSION: 'Pleural Effusion'>, label=<ChexpertLabel.POSITIVE: 1.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pleural Effusion?  ASSISTANT:\"]\n",
      "Label: tensor([1.])\n",
      "File_idx 1, ASSISTANT:  Yes, the patient has pleural effusion.\n",
      "File_idx 1, ASSISTANT (after confidence request):  {\"confidence\": 9}\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=16050730, study_id=57637607, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p16/p16050730/s57637607/adb48138-344feb7e-14e31d10-2639c54e-0b5a95d7.jpg', disease=<ChexpertFinding.PNEUMOTHORAX: 'Pneumothorax'>, label=<ChexpertLabel.NEGATIVE: 0.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pneumothorax?  ASSISTANT:\"]\n",
      "Label: tensor([0.])\n",
      "File_idx 2, ASSISTANT:  No, the patient does not have pneumothorax.\n",
      "File_idx 2, ASSISTANT (after confidence request):  {\"confidence\": 9}\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=11569093, study_id=57204814, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p11/p11569093/s57204814/0d2a50a2-3711662a-d7838521-4dc58d09-3732a3ad.jpg', disease=<ChexpertFinding.SUPPORT_DEVICES: 'Support Devices'>, label=<ChexpertLabel.POSITIVE: 1.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Support Devices?  ASSISTANT:\"]\n",
      "Label: tensor([1.])\n",
      "File_idx 3, ASSISTANT:  Yes, the patient has support devices.\n",
      "File_idx 3, ASSISTANT (after confidence request):  {\"confidence\": 8}\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=15192710, study_id=58836461, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p15/p15192710/s58836461/dc93422b-fd5ec685-19eb4eba-fb31f8d0-b60d8b47.jpg', disease=<ChexpertFinding.PNEUMOTHORAX: 'Pneumothorax'>, label=<ChexpertLabel.NEGATIVE: 0.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pneumothorax?  ASSISTANT:\"]\n",
      "Label: tensor([0.])\n",
      "File_idx 4, ASSISTANT:  No, the image shows no pneumothorax.\n",
      "File_idx 4, ASSISTANT (after confidence request):  {\"confidence\": 9}\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=16622813, study_id=50921860, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p16/p16622813/s50921860/066a59e3-316782a3-2d4238bc-d5354678-1ec6dcd9.jpg', disease=<ChexpertFinding.CARDIOMEGALY: 'Cardiomegaly'>, label=<ChexpertLabel.NEGATIVE: 0.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Cardiomegaly?  ASSISTANT:\"]\n",
      "Label: tensor([0.])\n",
      "File_idx 5, ASSISTANT:  No, the patient does not have cardiomegaly.\n",
      "File_idx 5, ASSISTANT (after confidence request):  {\"confidence\": 9}\n"
     ]
    }
   ],
   "source": [
    "from RewardingVisualDoubt import inference\n",
    "\n",
    "tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "padding_tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "padding_tokenizer.padding_side = \"left\"\n",
    "padding_tokenizer.pad_token_id = padding_tokenizer.bos_token_id\n",
    "dataset_test = dataset.get_binary_qa_prompted_mimic_cxr_llava_model_input_dataset(\n",
    "    split=dataset.DatasetSplit.TEST,\n",
    "    tokenizer=tokenizer,\n",
    "    prompter=prompter.build_binary_qa_instruction_from_disease_under_study,\n",
    ")\n",
    "dataloader_test = dataset.get_mimic_cxr_llava_model_input_dataloader(\n",
    "    dataset=dataset_test, batch_size=1, padding_tokenizer=padding_tokenizer, num_workers=8\n",
    ")\n",
    "\n",
    "for idx, batch in enumerate(dataloader_test):\n",
    "    batch = t.cast(dataset.MimicCxrLlavaModelInputBatchDict, batch)\n",
    "    batch_llava_model_input_dict = batch[\"batch_llava_model_input_dict\"]\n",
    "    batch_llava_model_input_dict = dataset.move_llava_model_input_dict_to_device(\n",
    "        batch_llava_model_input_dict, torch.device(shared.torch_devices.cuda.value)\n",
    "    )\n",
    "    input_ids, images = (\n",
    "        batch_llava_model_input_dict[\"text_prompt_input_ids\"],\n",
    "        batch_llava_model_input_dict[\"images\"],\n",
    "    )\n",
    "    stopping_criteria = KeywordsStoppingCriteria([STOP_STR], tokenizer, input_ids)\n",
    "    pred = inference.generate_radialog_answer_for_binary_qa_for_single_study(\n",
    "        model, tokenizer, input_ids, images, stopping_criteria\n",
    "    )\n",
    "    confidence_request_prompt = (\n",
    "        batch[\"batch_prompts\"][0]\n",
    "        + \" \"\n",
    "        + pred\n",
    "        + \" \"\n",
    "        + prompter.build_post_generation_user_confidence_request()  # POST_GENERATION_CONFIDENCE_REQUEST_8\n",
    "    )\n",
    "    confidence_request_input_ids = torch.unsqueeze(\n",
    "        torch.IntTensor(tokenizer(confidence_request_prompt)[\"input_ids\"]), 0\n",
    "    ).to(device)\n",
    "    stopping_criteria = KeywordsStoppingCriteria(\n",
    "        [STOP_STR], tokenizer, confidence_request_input_ids\n",
    "    )\n",
    "    pred_with_confidence = inference.generate_radialog_answer_for_binary_qa_for_single_study(\n",
    "        model, tokenizer, confidence_request_input_ids, images, stopping_criteria\n",
    "    )\n",
    "    print(f\"\\n Metadata: {batch['batch_mimic_cxr_datapoint_metadata']}\")\n",
    "    print(f\"Prompt: {batch['batch_prompts']}\")\n",
    "    print(f\"Label:\", batch[\"batch_labels\"])\n",
    "    print(f\"File_idx {idx}, ASSISTANT: \", pred)\n",
    "    print(f\"File_idx {idx}, ASSISTANT (after confidence request): \", pred_with_confidence)\n",
    "    if idx == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archived Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_prompt = \"Hello, how are you?\"\n",
    "labels = \"Say, lost in London?\"\n",
    "input_ids = tokenizer(my_prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "labels = tokenizer(labels, return_tensors=\"pt\").input_ids.to(device)\n",
    "resp = model(input_ids=input_ids, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 319,815,680 || all params: 3,866,461,096 || trainable%: 8.271534927142067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='203358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    20/203358 05:33 < 1047:38:42, 0.05 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[15], line 73\u001b[0m\n",
      "\u001b[1;32m     65\u001b[0m trainer \u001b[38;5;241m=\u001b[39m MyTrainer(\n",
      "\u001b[1;32m     66\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n",
      "\u001b[1;32m     67\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n",
      "\u001b[1;32m     68\u001b[0m     my_train_dataloader\u001b[38;5;241m=\u001b[39mdataloader_train,\n",
      "\u001b[1;32m     69\u001b[0m     my_eval_dataloader\u001b[38;5;241m=\u001b[39mdataloader_eval,\n",
      "\u001b[1;32m     70\u001b[0m )\n",
      "\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# ---- Train ----\u001b[39;00m\n",
      "\u001b[0;32m---> 73\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# ---- Save final model ----\u001b[39;00m\n",
      "\u001b[1;32m     76\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(DEFAULT_OUTPUT_DIR)\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/llava_hf/lib/python3.10/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n",
      "\u001b[1;32m   1536\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n",
      "\u001b[1;32m   1537\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n",
      "\u001b[1;32m   1538\u001b[0m )\n",
      "\u001b[0;32m-> 1539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1544\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/llava_hf/lib/python3.10/site-packages/transformers/trainer.py:1809\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "\u001b[1;32m   1806\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "\u001b[1;32m   1808\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n",
      "\u001b[0;32m-> 1809\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n",
      "\u001b[1;32m   1812\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n",
      "\u001b[1;32m   1813\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n",
      "\u001b[1;32m   1814\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n",
      "\u001b[1;32m   1815\u001b[0m ):\n",
      "\u001b[1;32m   1816\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n",
      "\u001b[1;32m   1817\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/llava_hf/lib/python3.10/site-packages/transformers/trainer.py:2665\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n",
      "\u001b[1;32m   2663\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[1;32m   2664\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 2665\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   2667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/llava_hf/lib/python3.10/site-packages/accelerate/accelerator.py:1853\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1851\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;32m   1852\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1853\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/llava_hf/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n",
      "\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n",
      "\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n",
      "\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n",
      "\u001b[1;32m    486\u001b[0m     )\n",
      "\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n",
      "\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/llava_hf/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n",
      "\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n",
      "\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n",
      "\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n",
      "\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n",
      "\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# ---- Config ----\n",
    "DEFAULT_OUTPUT_DIR = path.Path(\"output\")\n",
    "BATCH_SIZE = 8\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "LR = 5e-5\n",
    "NUM_EPOCHS = 3\n",
    "LOGGING_STEPS = 1\n",
    "\n",
    "# ---- Init wandb ----\n",
    "wandb.init(project=\"radialog-confidence-score-sft\")\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# ---- Training args ----\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=DEFAULT_OUTPUT_DIR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=0.01,\n",
    "    # fp16=True,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "\n",
    "collate_with_tokenizer = partial(\n",
    "    dataset.prompted_mimic_cxr_llava_model_input_collate_fn_for_sft_simplified,\n",
    "    padding_tokenizer=padding_tokenizer,\n",
    ")\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "class MyTrainer(Trainer):\n",
    "    def __init__(self, *args, my_train_dataloader=None, my_eval_dataloader=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.my_train_dataloader = my_train_dataloader\n",
    "        self.my_eval_dataloader = my_eval_dataloader\n",
    "\n",
    "    def get_train_dataloader(self):\n",
    "        if self.my_train_dataloader is not None:\n",
    "            return self.my_train_dataloader\n",
    "        return super().get_train_dataloader()\n",
    "\n",
    "    def get_eval_dataloader(self, eval_dataset=None):\n",
    "        if self.my_eval_dataloader is not None:\n",
    "            return self.my_eval_dataloader\n",
    "        return super().get_eval_dataloader(eval_dataset)\n",
    "\n",
    "\n",
    "# ---- Trainer ----\n",
    "trainer = MyTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    my_train_dataloader=dataloader_train,\n",
    "    my_eval_dataloader=dataloader_eval,\n",
    ")\n",
    "\n",
    "# ---- Train ----\n",
    "trainer.train()\n",
    "\n",
    "# ---- Save final model ----\n",
    "trainer.save_model(DEFAULT_OUTPUT_DIR)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.5176, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resp.loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava_hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
