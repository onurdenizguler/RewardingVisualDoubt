{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO sampling logic (undersample)\n",
    "# TODO binary qa prompt and co need to vary between a few samples\n",
    "# TODO determine hyperparams\n",
    "# TODO arg parsing\n",
    "# TODO dataloader num workers set to default\n",
    "\n",
    "# SOME NOTES\n",
    "# PPO_TRAINER AUTOMATICALLY PADS THE INPUTS BY TOKENIZER.PADDING_SIDE AND TOKENIZER.PADDING_TOKEN_ID\n",
    "# Uh-oh, because ppo termination token is set as the eos_seq_token, it'll stop when it sees a left padded sequence\n",
    "# Skipping random exploration for now\n",
    "\n",
    "\n",
    "# BATCH TIMING\n",
    "# A batch of 8 samples take around 1-1.5-2-3min to process in a train step (so around 400 samples per hour is trainable, every 50th batch, we save a checkpoint, and do val)\n",
    "# Lets save a checkpoint every half an hour or so\n",
    "# Give validation around 15 mins => 100 samples or so\n",
    "# Validation is around 8k so it'll be 1000 batches (1000*1.5 min = 25 hours)\n",
    "# len(dataset_eval) = 8737"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e3c336876e4aebbfeebb5105a32e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 69 files:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% Set script for interactive development and import modules\n",
    "from RewardingVisualDoubt import infrastructure\n",
    "\n",
    "infrastructure.make_ipython_reactive_to_changing_codebase()\n",
    "infrastructure.supress_known_warnings()\n",
    "\n",
    "import pathlib as path\n",
    "import typing as t\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import accelerate\n",
    "import dataclasses\n",
    "\n",
    "# from LLAVA_Biovil.llava.mm_utils import KeywordsStoppingCriteria\n",
    "from trl import PPOConfig, PPOTrainer\n",
    "\n",
    "from RewardingVisualDoubt import dataset, prompter, shared, vllm, train, response, reward\n",
    "\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"da3cb086bbc110c16cbc5ba4c284a19b0b461710\"\n",
    "\n",
    "from LLAVA_Biovil.llava.mm_utils import KeywordsStoppingCriteria\n",
    "\n",
    "STOP_STR = prompter.Seperator.END_OF_SEQUENCE_SEPERATOR.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model in non-trainable mode...\n",
      "Model base:  liuhaotian/llava-v1.5-7b\n",
      "Loading LLaVA from base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a913607d8d4905a58d340888fd2d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541a8d8dd4f841ee9d9f850cc2dbd926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading additional LLaVA weights...\n",
      "Using downloaded and verified file: /tmp/biovil_t_image_model_proj_size_128.pt\n",
      "Loaded additional vision tower weights...\n",
      "Adding pretrained RaDialog LoRA adapters and value head to the model...\n",
      "Loading the datasets and the dataloaders...\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "######################################## 0. Define the environment ########################################\n",
    "\n",
    "DEFAULT_BATCH_SIZE = 8\n",
    "DEFAULT_OUTPUT_DIR = path.Path(\"output\")\n",
    "\n",
    "device_str = (\n",
    "    shared.torch_devices.cuda.value if torch.cuda.is_available() else shared.torch_devices.cpu.value\n",
    ")\n",
    "device = torch.device(device_str)\n",
    "\n",
    "######################################## 1. Load the model and tokenizer ########################################\n",
    "\n",
    "model = vllm.load_pretrained_llava_model_for_ppo_training(device_str=device_str)\n",
    "# model_ref = vllm.load_pretrained_llava_model_for_ppo_training(device_str=device_str)\n",
    "\n",
    "tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "padding_tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "padding_tokenizer.padding_side = \"left\"  # Why? Because: A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "\n",
    "\n",
    "######################################## 2. Load the datasets and the dataloaders ########################################\n",
    "\n",
    "print(\"Loading the datasets and the dataloaders...\")\n",
    "dataset_train = dataset.get_binary_qa_prompted_mimic_cxr_llava_model_input_dataset(\n",
    "    split=dataset.DatasetSplit.TRAIN,\n",
    "    tokenizer=tokenizer,\n",
    "    prompter=prompter.build_binary_qa_instruction_from_disease_under_study,\n",
    ")\n",
    "dataset_eval = dataset.get_binary_qa_prompted_mimic_cxr_llava_model_input_dataset(\n",
    "    split=dataset.DatasetSplit.VALIDATION,\n",
    "    tokenizer=tokenizer,\n",
    "    prompter=prompter.build_binary_qa_instruction_from_disease_under_study,\n",
    ")\n",
    "\n",
    "padding_tokenizer.pad_token = padding_tokenizer.bos_token  # TODO how about this?\n",
    "\n",
    "dataloader_train = dataset.get_mimic_cxr_llava_model_input_dataloader(\n",
    "    dataset=dataset_train,\n",
    "    batch_size=DEFAULT_BATCH_SIZE,\n",
    "    padding_tokenizer=padding_tokenizer,\n",
    "    num_workers=8,  # Let Torch decide.\n",
    ")\n",
    "\n",
    "dataloader_eval = dataset.get_mimic_cxr_llava_model_input_dataloader(\n",
    "    dataset=dataset_eval,\n",
    "    batch_size=2 * DEFAULT_BATCH_SIZE,\n",
    "    padding_tokenizer=padding_tokenizer,\n",
    "    num_workers=8,  # Let Torch decide.\n",
    ")\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")  # Adds higher directory to python modules path.\n",
    "from workflows import radialog_binary_qa_ppo_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## 3. Define the PPO and generation configurations ########################################\n",
    "epochs = 1\n",
    "lr = 5e-6\n",
    "log_with = \"foo\"\n",
    "out_dir = \"output\"\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    learning_rate=lr,\n",
    "    task_name=\"gpt\",\n",
    "    batch_size=DEFAULT_BATCH_SIZE,\n",
    "    mini_batch_size=int(DEFAULT_BATCH_SIZE / 4),\n",
    "    log_with=\"wandb\",\n",
    "    project_kwargs=dataclasses.asdict(\n",
    "        accelerate.utils.ProjectConfiguration(\n",
    "            project_dir=\"radialog_binary_qa_ppo_training\", logging_dir=\"logs\"\n",
    "        )\n",
    "    ),\n",
    "    remove_unused_columns=False,\n",
    "    # optimize_device_cache=True,\n",
    "    init_kl_coef=0.05,\n",
    ")\n",
    "\n",
    "generation_kwargs_ppo = {\n",
    "    \"min_length\": -1,  # don't ignore the EOS token (see above)\n",
    "    \"top_k\": 0.0,  # no top-k sampling\n",
    "    \"top_p\": 1.0,  # no nucleus sampling\n",
    "    \"temperature\": 0.5,  # DONT BE CREATIVE\n",
    "    \"do_sample\": True,  # yes, we want to sample\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,  # most decoder models don't have a padding token - use EOS token instead (for this tokenizer it was already set to eos_token_id)\n",
    "    \"max_new_tokens\": 50,  # let's not be chatty, we need a few tokens to generate confidence but also not limit the response too much\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,  # (instead of ppo_terminators list)\n",
    "}\n",
    "\n",
    "ppo_trainer = t.cast(\n",
    "    PPOTrainer,\n",
    "    PPOTrainer(\n",
    "        model=model,\n",
    "        config=ppo_config,\n",
    "        tokenizer=tokenizer,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# not sure if needed but just to be safe for now\n",
    "tokenizer.padding_side = \"left\"\n",
    "model.config.padding_side = \"left\"\n",
    "model.config.tokenizer_padding_side = \"left\"\n",
    "# model.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_epoch = []\n",
    "iterator_train = iter(dataloader_train)\n",
    "batch = next(iterator_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(i)\n",
    "    batch = next(iterator_train)\n",
    "    rewards, batch_report = radialog_binary_qa_ppo_training.radialog_binary_qa_ppo_training_step(\n",
    "        model,\n",
    "        device,\n",
    "        tokenizer,\n",
    "        generation_kwargs_ppo,\n",
    "        ppo_trainer,\n",
    "        batch,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(-15., device='cuda:0'),\n",
       " tensor(-15., device='cuda:0'),\n",
       " tensor(-15., device='cuda:0'),\n",
       " tensor(-15., device='cuda:0'),\n",
       " tensor(-15., device='cuda:0'),\n",
       " tensor(-15., device='cuda:0'),\n",
       " tensor(-15., device='cuda:0'),\n",
       " tensor(-15., device='cuda:0')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER:  image . You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pneumonia?  ASSISTANT: Yes, the patient has: Pneumonia USER: Now evaluate your own response. How confident are you in your answer? Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. A value close to 0 means you think there is a high probability that the answer is wrong. The closer the value is to 10, the higher you think is the probability that the answer is correct.  ASSISTANT: When asked how confident I am about a response, I consistently provide it in a JSON object, adhering to my policy.The confidence JSON follows this structure: {'confidence': int}.Here's my confidence JSON about my last response:  \",\n",
       "  \"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER:  image . You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pneumothorax?  ASSISTANT: No, the patient does not have: USER: Now evaluate your own response. How confident are you in your answer? Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. A value close to 0 means you think there is a high probability that the answer is wrong. The closer the value is to 10, the higher you think is the probability that the answer is correct.  ASSISTANT: When asked how confident I am about a response, I consistently provide it in a JSON object, adhering to my policy.The confidence JSON follows this structure: {'confidence': int}.Here's my confidence JSON about my last response:  \",\n",
       "  \"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER:  image . You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pleural Effusion?  ASSISTANT: Yes, the patient has pleural effusion. USER: Now evaluate your own response. How confident are you in your answer? Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. A value close to 0 means you think there is a high probability that the answer is wrong. The closer the value is to 10, the higher you think is the probability that the answer is correct.  ASSISTANT: When asked how confident I am about a response, I consistently provide it in a JSON object, adhering to my policy.The confidence JSON follows this structure: {'confidence': int}.Here's my confidence JSON about my last response:  \",\n",
       "  \"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER:  image . You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Support Devices?  ASSISTANT: No USER: Now evaluate your own response. How confident are you in your answer? Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. A value close to 0 means you think there is a high probability that the answer is wrong. The closer the value is to 10, the higher you think is the probability that the answer is correct.  ASSISTANT: When asked how confident I am about a response, I consistently provide it in a JSON object, adhering to my policy.The confidence JSON follows this structure: {'confidence': int}.Here's my confidence JSON about my last response:  \",\n",
       "  \"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER:  image . You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Atelectasis?  ASSISTANT: Yes, the patient has atelectasis. USER: Now evaluate your own response. How confident are you in your answer? Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. A value close to 0 means you think there is a high probability that the answer is wrong. The closer the value is to 10, the higher you think is the probability that the answer is correct.  ASSISTANT: When asked how confident I am about a response, I consistently provide it in a JSON object, adhering to my policy.The confidence JSON follows this structure: {'confidence': int}.Here's my confidence JSON about my last response:  \",\n",
       "  \"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER:  image . You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Cardiomegaly?  ASSISTANT: No. USER: Now evaluate your own response. How confident are you in your answer? Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. A value close to 0 means you think there is a high probability that the answer is wrong. The closer the value is to 10, the higher you think is the probability that the answer is correct.  ASSISTANT: When asked how confident I am about a response, I consistently provide it in a JSON object, adhering to my policy.The confidence JSON follows this structure: {'confidence': int}.Here's my confidence JSON about my last response:  \",\n",
       "  \"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER:  image . You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Lung Opacity?  ASSISTANT: Yes, the patient has: Yes USER: Now evaluate your own response. How confident are you in your answer? Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. A value close to 0 means you think there is a high probability that the answer is wrong. The closer the value is to 10, the higher you think is the probability that the answer is correct.  ASSISTANT: When asked how confident I am about a response, I consistently provide it in a JSON object, adhering to my policy.The confidence JSON follows this structure: {'confidence': int}.Here's my confidence JSON about my last response:  \",\n",
       "  \"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER:  image . You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pleural Effusion?  ASSISTANT: Yes, the patient has pleural effusion. USER: Now evaluate your own response. How confident are you in your answer? Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. A value close to 0 means you think there is a high probability that the answer is wrong. The closer the value is to 10, the higher you think is the probability that the answer is correct.  ASSISTANT: When asked how confident I am about a response, I consistently provide it in a JSON object, adhering to my policy.The confidence JSON follows this structure: {'confidence': int}.Here's my confidence JSON about my last response:  \"],\n",
       " 'response': [None, None, None, None, None, None, None, None],\n",
       " 'generated_answers_texts': ['Yes, the patient has: Pneumonia',\n",
       "  'No, the patient does not have:',\n",
       "  'Yes, the patient has pleural effusion.',\n",
       "  'No',\n",
       "  'Yes, the patient has atelectasis.',\n",
       "  'No.',\n",
       "  'Yes, the patient has: Yes',\n",
       "  'Yes, the patient has pleural effusion.'],\n",
       " 'generated_confidences_texts': ['1', '1', '4', '9', '10', '1', '8', '10']}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "POST_GENERATION_CONFIDENCE_REQUEST_1 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer? \"\n",
    "    \"Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. \"\n",
    "    \"A value close to 0 means you think there is a high probability that the answer is wrong. \"\n",
    "    \"The closer the value is to 10, the higher you think is the probability that the answer is correct. \"\n",
    "    \"The output should have the format 'Confidence: <confidence>' and nothing else. \"\n",
    ")\n",
    "\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_2 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer? \"\n",
    "    \"Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. \"\n",
    "    \"The output should have the format 'My Confidence: <confidence>'. \"\n",
    "    \"ASSISTANT: Here is my self evaluation of my confidence. My Confidence (out of 10):\"\n",
    ")\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_3 = \"USER: Irrelevant from my question, what is your name?\"\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_4 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer? \"\n",
    "    \"Provide a confidence between 0, 1, 2, 3, 4, 5 of how sure you are the answer is correct. \"\n",
    "    \"The output should have the format 'My Confidence: <confidence>' My Reasoning for my Self Confidence Evaluation: <reasoning>. \"\n",
    "    \"ASSISTANT: Here is my self evaluation of my confidence. My Confidence (out of 5):\"\n",
    ")\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_5 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer? \"\n",
    "    \"Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,  of how sure you are the answer is correct. \"\n",
    "    \"The output should have the format 'My Confidence: <confidence>' My Very Short Reasoning for my Self Confidence Evaluation: <very_short_reasoning>. \"\n",
    "    \"ASSISTANT: Here is my self evaluation of my confidence. My Confidence (out of 10):\"\n",
    ")\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_6 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer and what is your explanation regarding your confidence?\"\n",
    "    \"ASSISTANT: When asked how confident I am about a response, I consistently provide it in a JSON object, adhering to my policy.\"\n",
    "    \"My confidence is a value between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, where 0 means I am not confident at all and 10 means I am very confident.\"\n",
    "    \"The confidence JSON follows this structure: {'confidence': int, 'explanation': str}.\"\n",
    "    \"Here's my confidence JSON about my last response: \"\n",
    ")\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_7 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer?\"\n",
    "    \"ASSISTANT: When asked how confident I am about a response, I consistently provide it in a JSON object, adhering to my policy.\"\n",
    "    \"My confidence is a value between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, where 0 means I am not confident at all and 10 means I am very confident.\"\n",
    "    \"The confidence JSON follows this structure: {'confidence': int}.\"\n",
    "    \"Here's my confidence JSON about my last response: \"\n",
    ")\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_8 = (\n",
    "    \"</s> USER: Now evaluate your own response. How confident are you in your answer?\"\n",
    "    \"Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. \"\n",
    "    \"A value close to 0 means you think there is a high probability that the answer is wrong. \"\n",
    "    \"The closer the value is to 10, the higher you think is the probability that the answer is correct. \"\n",
    "    \"ASSISTANT: When asked how confident I am about a response, I consistently provide it in a JSON object, adhering to my policy.\"\n",
    "    \"The confidence JSON follows this structure: {'confidence': int}.\"\n",
    "    \"Here's my confidence JSON about my last response: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=18460230, study_id=53631792, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p18/p18460230/s53631792/369dc5bd-70bd89d0-2d90fa80-f319ec1d-fb2802aa.jpg', disease=<ChexpertFinding.PLEURAL_EFFUSION: 'Pleural Effusion'>, label=<ChexpertLabel.POSITIVE: 1.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pleural Effusion?  ASSISTANT:\"]\n",
      "Label: tensor([1.])\n",
      "File_idx 0, ASSISTANT:  Yes, the image shows pleural effusion.\n",
      "File_idx 0, ASSISTANT (after confidence request):  {\"confidence\": 9}\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=13263843, study_id=52138943, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p13/p13263843/s52138943/de739d0b-2345495b-255f0e3b-00ccbf4c-ab4d3400.jpg', disease=<ChexpertFinding.PLEURAL_EFFUSION: 'Pleural Effusion'>, label=<ChexpertLabel.POSITIVE: 1.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pleural Effusion?  ASSISTANT:\"]\n",
      "Label: tensor([1.])\n",
      "File_idx 1, ASSISTANT:  Yes, there is evidence of that in the image.\n",
      "File_idx 1, ASSISTANT (after confidence request):  {\"confidence\": 8}\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=16050730, study_id=57637607, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p16/p16050730/s57637607/adb48138-344feb7e-14e31d10-2639c54e-0b5a95d7.jpg', disease=<ChexpertFinding.PNEUMOTHORAX: 'Pneumothorax'>, label=<ChexpertLabel.NEGATIVE: 0.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pneumothorax?  ASSISTANT:\"]\n",
      "Label: tensor([0.])\n",
      "File_idx 2, ASSISTANT:  No, the image shows no Pneumothorax.\n",
      "File_idx 2, ASSISTANT (after confidence request):  {\"confidence\": 9}\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=11569093, study_id=57204814, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p11/p11569093/s57204814/0d2a50a2-3711662a-d7838521-4dc58d09-3732a3ad.jpg', disease=<ChexpertFinding.SUPPORT_DEVICES: 'Support Devices'>, label=<ChexpertLabel.POSITIVE: 1.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Support Devices?  ASSISTANT:\"]\n",
      "Label: tensor([1.])\n",
      "File_idx 3, ASSISTANT:  Yes, there is evidence of that in the image.\n",
      "File_idx 3, ASSISTANT (after confidence request):  {\"confidence\": 8}\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=15192710, study_id=58836461, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p15/p15192710/s58836461/dc93422b-fd5ec685-19eb4eba-fb31f8d0-b60d8b47.jpg', disease=<ChexpertFinding.PNEUMOTHORAX: 'Pneumothorax'>, label=<ChexpertLabel.NEGATIVE: 0.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pneumothorax?  ASSISTANT:\"]\n",
      "Label: tensor([0.])\n",
      "File_idx 4, ASSISTANT:  No, there is no evidence of that in the image.\n",
      "File_idx 4, ASSISTANT (after confidence request):  {\"confidence\": 9}\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=16622813, study_id=50921860, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p16/p16622813/s50921860/066a59e3-316782a3-2d4238bc-d5354678-1ec6dcd9.jpg', disease=<ChexpertFinding.CARDIOMEGALY: 'Cardiomegaly'>, label=<ChexpertLabel.NEGATIVE: 0.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Cardiomegaly?  ASSISTANT:\"]\n",
      "Label: tensor([0.])\n",
      "File_idx 5, ASSISTANT:  No, there is no evidence of that in the image.\n",
      "File_idx 5, ASSISTANT (after confidence request):  {\"confidence\": 9}\n"
     ]
    }
   ],
   "source": [
    "STOP_STR = prompter.Seperator.END_OF_SEQUENCE_SEPERATOR.value\n",
    "from LLAVA_Biovil.llava.mm_utils import KeywordsStoppingCriteria\n",
    "from RewardingVisualDoubt import inference\n",
    "\n",
    "padding_tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "padding_tokenizer.padding_side = \"left\"\n",
    "padding_tokenizer.pad_token_id = padding_tokenizer.bos_token_id\n",
    "dataset_test = dataset.get_binary_qa_prompted_mimic_cxr_llava_model_input_dataset(\n",
    "    split=dataset.DatasetSplit.TEST,\n",
    "    tokenizer=tokenizer,\n",
    "    prompter=prompter.build_binary_qa_instruction_from_disease_under_study,\n",
    ")\n",
    "dataloader_test = dataset.get_mimic_cxr_llava_model_input_dataloader(\n",
    "    dataset=dataset_test, batch_size=1, padding_tokenizer=padding_tokenizer, num_workers=8\n",
    ")\n",
    "\n",
    "for idx, batch in enumerate(dataloader_test):\n",
    "    batch = t.cast(dataset.MimicCxrLlavaModelInputBatchDict, batch)\n",
    "    batch_llava_model_input_dict = batch[\"batch_llava_model_input_dict\"]\n",
    "    batch_llava_model_input_dict = dataset.move_llava_model_input_dict_to_device(\n",
    "        batch_llava_model_input_dict, torch.device(shared.torch_devices.cuda.value)\n",
    "    )\n",
    "    input_ids, images = (\n",
    "        batch_llava_model_input_dict[\"text_prompt_input_ids\"],\n",
    "        batch_llava_model_input_dict[\"images\"],\n",
    "    )\n",
    "    stopping_criteria = KeywordsStoppingCriteria([STOP_STR], tokenizer, input_ids)\n",
    "    pred = inference.generate_radialog_answer_for_binary_qa_for_single_study(\n",
    "        model, tokenizer, input_ids, images, stopping_criteria\n",
    "    )\n",
    "    confidence_request_prompt = (\n",
    "        batch[\"batch_prompts\"][0]\n",
    "        + \" \"\n",
    "        + pred\n",
    "        + \" \"\n",
    "        + prompter.build_post_generation_user_confidence_request()  # POST_GENERATION_CONFIDENCE_REQUEST_8\n",
    "    )\n",
    "    confidence_request_input_ids = torch.unsqueeze(\n",
    "        torch.IntTensor(tokenizer(confidence_request_prompt)[\"input_ids\"]), 0\n",
    "    ).to(device)\n",
    "    stopping_criteria = KeywordsStoppingCriteria(\n",
    "        [STOP_STR], tokenizer, confidence_request_input_ids\n",
    "    )\n",
    "    pred_with_confidence = inference.generate_radialog_answer_for_binary_qa_for_single_study(\n",
    "        model, tokenizer, confidence_request_input_ids, images, stopping_criteria\n",
    "    )\n",
    "    print(f\"\\n Metadata: {batch['batch_mimic_cxr_datapoint_metadata']}\")\n",
    "    print(f\"Prompt: {batch['batch_prompts']}\")\n",
    "    print(f\"Label:\", batch[\"batch_labels\"])\n",
    "    print(f\"File_idx {idx}, ASSISTANT: \", pred)\n",
    "    print(f\"File_idx {idx}, ASSISTANT (after confidence request): \", pred_with_confidence)\n",
    "    if idx == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER:  image . You are to act as a radiologist and answer a single question. After you respond, please provide your self evaluation of your confidence. Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. A value close to 0 means you think there is a high probability that the answer is wrong. Your confidence is to be reported in a JSON dictionary of the following format: {'confidence': int}. Is the following disease visible in the given X-ray image: Cardiomegaly, and how confident are you?  ASSISTANT: No.</s>\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################## TEST TO SEE IF TEMPERATURE AND TOP_P PARAMS HELP WITH USER CONFIDENCE REQUEST WITHOUT ASSISTANT CONFIRMATION ########################################\n",
    "\n",
    "\n",
    "from LLAVA_Biovil.llava.mm_utils import tokenizer_image_token\n",
    "\n",
    "iterator_train = iter(dataloader_train)\n",
    "batch = next(iterator_train)\n",
    "\n",
    "batch_llava_model_input_dict = batch[\"batch_llava_model_input_dict\"]\n",
    "batch_llava_model_input_dict = dataset.move_llava_model_input_dict_to_device(\n",
    "    batch_llava_model_input_dict, device\n",
    ")\n",
    "_, images = (\n",
    "    batch_llava_model_input_dict[\"text_prompt_input_ids\"],\n",
    "    batch_llava_model_input_dict[\"images\"],\n",
    ")\n",
    "\n",
    "my_prompt = prompter.build_binary_qa_instruction_from_disease_under_study_with_confidence_request(\n",
    "    \"Cardiomegaly\"\n",
    ")\n",
    "tokenized_prompt = tokenizer_image_token(my_prompt, tokenizer, return_tensors=\"pt\").to(device)\n",
    "\n",
    "stopping_criteria = KeywordsStoppingCriteria([STOP_STR], tokenizer, tokenized_prompt.unsqueeze(0))\n",
    "\n",
    "prompt_and_generated_answers_ids = model.generate(\n",
    "    input_ids=tokenized_prompt.unsqueeze(0),\n",
    "    images=images[0].unsqueeze(0),\n",
    "    # attention_mask=attention_mask,\n",
    "    do_sample=True,\n",
    "    use_cache=True,\n",
    "    temperature=1.8,\n",
    "    top_p=0.7,\n",
    "    max_new_tokens=300,  # TODO maybe move to the kwargs\n",
    "    stopping_criteria=[stopping_criteria],  # TODO understand better\n",
    "    pad_token_id=tokenizer.pad_token_id,  # used in tokenizing after the generation, # TODO maybe move to the kwargs\n",
    "    # **generation_kwargs_prediction,  # TODO check which args to pass.\n",
    ")\n",
    "\n",
    "tokenizer.decode(train.replace_image_token_with_another_token(prompt_and_generated_answers_ids)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iterator_train)\n",
    "\n",
    "batch = t.cast(dataset.MimicCxrLlavaModelInputBatchDict, batch)\n",
    "batch_llava_model_input_dict = batch[\"batch_llava_model_input_dict\"]\n",
    "batch_llava_model_input_dict = dataset.move_llava_model_input_dict_to_device(\n",
    "    batch_llava_model_input_dict, device\n",
    ")\n",
    "input_ids, images = (\n",
    "    batch_llava_model_input_dict[\"text_prompt_input_ids\"],\n",
    "    batch_llava_model_input_dict[\"images\"],\n",
    ")\n",
    "attention_mask = batch[\"batch_attention_mask\"].to(device)  # TODO handle elsewhere\n",
    "labels = batch[\"batch_labels\"].to(device)  # TODO handle elsewhere\n",
    "\n",
    "\n",
    "model.eval()\n",
    "stopping_criteria = KeywordsStoppingCriteria([STOP_STR], tokenizer, input_ids)\n",
    "\n",
    "\n",
    "t3 = time.time()\n",
    "prompt_and_generated_answers_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    images=images,\n",
    "    attention_mask=attention_mask,\n",
    "    do_sample=False,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=32,  # Limiting, YES, but binary q&a answers are not very long!\n",
    "    stopping_criteria=[stopping_criteria],\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "t4 = time.time()\n",
    "prompt_and_generated_answers_ids = train.remove_trailing_padding_from_prediction(\n",
    "    prompt_and_generated_answers_ids, tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "# Append confidence request to the generated answers\n",
    "prompt_and_generated_answers_with_confidence_requests_ids = []\n",
    "for item in prompt_and_generated_answers_ids:\n",
    "    confidence_request_input_ids = (\n",
    "        tokenizer(prompter.build_post_generation_user_confidence_request(), return_tensors=\"pt\")\n",
    "        .input_ids.to(device)\n",
    "        .squeeze(0)\n",
    "    )[\n",
    "        1:\n",
    "    ]  # drop start of sequence token\n",
    "    prompt_and_generated_answers_with_confidence_requests_ids.append(\n",
    "        torch.cat((item, confidence_request_input_ids), 0)\n",
    "    )\n",
    "model.train()\n",
    "\n",
    "t5 = time.time()\n",
    "generated_confidences_ids = ppo_trainer.generate(\n",
    "    prompt_and_generated_answers_with_confidence_requests_ids,  # ppo_trainer.generate() method admits list of tensors, not a batch tensor unfortunately\n",
    "    images=images,\n",
    "    return_prompt=False,\n",
    "    **generation_kwargs_ppo,\n",
    ")\n",
    "t6 = time.time()\n",
    "\n",
    "\n",
    "complete_conversation_ids = [\n",
    "    torch.cat((p, c), 0)\n",
    "    for p, c in zip(\n",
    "        prompt_and_generated_answers_with_confidence_requests_ids,\n",
    "        generated_confidences_ids,\n",
    "    )\n",
    "]\n",
    "generated_answer_only_ids = [\n",
    "    prompt_and_generated_answers_ids[i][len(input_ids[i]) :] for i in range(len(input_ids))\n",
    "]\n",
    "\n",
    "# Remove the unindex image token from the prompt\n",
    "prompt_and_generated_answers_with_confidence_requests_ids = (\n",
    "    train.replace_image_token_with_another_token_for_list_of_tensors(\n",
    "        prompt_and_generated_answers_with_confidence_requests_ids\n",
    "    )\n",
    ")\n",
    "generated_answers_texts = tokenizer.batch_decode(\n",
    "    generated_answer_only_ids,\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "generated_confidences_texts = tokenizer.batch_decode(\n",
    "    generated_confidences_ids,\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "generated_answer_labels = response.parse_binary_labels(generated_answers_texts)\n",
    "generated_confidence_values = response.parse_confidences(generated_confidences_texts)\n",
    "\n",
    "rewards = [\n",
    "    reward.generated_answer_and_confidence_to_reward(\n",
    "        generated_answer_label, generated_confidence_value, ground_truth_label\n",
    "    )\n",
    "    for generated_answer_label, generated_confidence_value, ground_truth_label in zip(\n",
    "        generated_answer_labels, generated_confidence_values, labels.bool().tolist()\n",
    "    )\n",
    "]\n",
    "\n",
    "report = {}\n",
    "report[\"generated_answer_labels\"] = generated_answer_labels\n",
    "\n",
    "rewards_epoch += rewards\n",
    "rewards = [torch.tensor(r).to(device) for r in rewards]\n",
    "\n",
    "t7 = time.time()\n",
    "stats = ppo_trainer.step(\n",
    "    prompt_and_generated_answers_with_confidence_requests_ids, generated_answer_only_ids, rewards\n",
    ")\n",
    "t8 = time.time()\n",
    "\n",
    "# ppo_trainer.log_stats(stats, batch, rewards, columns_to_log=[\"query\", \"response\", \"answer\"])\n",
    "\n",
    "# print(f\"Finished epoch {epoch}. Average reward: {avg_reward}\")\n",
    "# ppo_trainer.save_pretrained(os.path.join(out_dir, \"model_finetuned\"))\n",
    "\n",
    "# TODO: For random exploration\n",
    "# chance_to_change_confidence -= reduce_per_step\n",
    "# chance_to_change_confidence = max(0, chance_to_change_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.95582970558741\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(rewards_epoch) // batch_size):\n",
    "    print(sum(rewards_epoch[i * batch_size : (i + 1) * batch_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 4, 6, 7, 5, 7, 5, 6]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generated_confidence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, False, False, True, True, True, True, False]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generated_answer_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yes, the patient has consolidation.',\n",
       " 'No, there is no evidence of that in the image.',\n",
       " 'No, there is no evidence of that in the image.',\n",
       " 'Yes, the patient has enlarged cardiomediastinum.',\n",
       " 'Yes, there is evidence of that in the image.',\n",
       " 'Yes, the patient has cardiomegaly.',\n",
       " 'Yes, the patient has support devices.',\n",
       " 'No, there is no evidence of that in the image.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generated_answers_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n{\"confidence\": 8}',\n",
       " '9',\n",
       " '\\n{\"confidence\": 4}',\n",
       " '\\n{\"confidence\": 5}',\n",
       " '\\n{\"confidence\": 9}',\n",
       " '\\n{\"confidence\": 6}',\n",
       " '\\n{\"confidence\": 8}',\n",
       " '\\n{\"confidence\": 6}']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generated_confidences_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yes, the image shows atelectasis.',\n",
       " 'Yes, the patient has pleural effusion.',\n",
       " 'Yes, there is evidence of that in the image.',\n",
       " 'Yes, the image shows pleural effusion.',\n",
       " 'No, there is no evidence of that in the image.',\n",
       " 'No, there is no evidence of that in the image.',\n",
       " 'No, there is no evidence of that in the image.',\n",
       " 'Yes, the image shows pleural effusion.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answers_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n{\"confidence\": 8}',\n",
       " '\\n{\"confidence\": 7}',\n",
       " '\\n{\"confidence\": 9}',\n",
       " '\\n{\"confidence\": 9}',\n",
       " '\\n{\"confidence\": 3}',\n",
       " '\\n{\"confidence\": 3}',\n",
       " \"\\n\\n{'confidence': 3}\",\n",
       " '\\n{\"confidence\": 4}']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confidences_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9359.999895095825 time it took to ppo step\n",
      "64616.09601974487 time it took to generate confidences\n",
      "1864.7639751434326 time it took to generate answers\n",
      "1835.4952335357666 time it took to get batch\n",
      "total time it took 77 seconds\n"
     ]
    }
   ],
   "source": [
    "print((t8 - t7) * 1000, \"time it took to ppo step\")\n",
    "print((t6 - t5) * 1000, \"time it took to generate confidences\")\n",
    "print((t4 - t3) * 1000, \"time it took to generate answers\")\n",
    "print((t2 - t1) * 1000, \"time it took to get batch\")\n",
    "\n",
    "print(\"total time it took\", int((t8 - t1)), \"seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava_hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
