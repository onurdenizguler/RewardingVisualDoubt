{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "\n",
    "# TODO BATCH save + eval checkpoints (instead of epoch save)\n",
    "# TODO arg parsing\n",
    "\n",
    "\n",
    "# TODO determine hyperparams\n",
    "# TODO dataloader num workers set to default\n",
    "# TODO sampling logic (undersample)\n",
    "# TODO binary qa prompt needs to vary between a few samples\n",
    "\n",
    "\n",
    "# PROBABLY LATER\n",
    "# TODO Integrating supervised finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef75ad986a6247e98aaa63c836ed40ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 69 files:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% Set script for interactive development and import modules\n",
    "from RewardingVisualDoubt import infrastructure\n",
    "\n",
    "infrastructure.make_ipython_reactive_to_changing_codebase()\n",
    "infrastructure.supress_known_warnings()\n",
    "\n",
    "import pathlib as path\n",
    "import typing as t\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# from LLAVA_Biovil.llava.mm_utils import KeywordsStoppingCriteria\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n",
    "\n",
    "from RewardingVisualDoubt import dataset, mimic_cxr, prompter, shared, vllm, train, response, reward\n",
    "import time\n",
    "\n",
    "DEFAULT_BATCH_SIZE = 8\n",
    "DEFAULT_OUTPUT_DIR = path.Path(\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model in non-trainable mode...\n",
      "Model base:  liuhaotian/llava-v1.5-7b\n",
      "Loading LLaVA from base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dab8fc9533c4ef48d3df82f29daf556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740702e1290a48f0a44aeaf43515ce73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading additional LLaVA weights...\n",
      "Using downloaded and verified file: /tmp/biovil_t_image_model_proj_size_128.pt\n",
      "Loaded additional vision tower weights...\n",
      "Adding pretrained RaDialog LoRA adapters and value head to the model...\n",
      "Loading the datasets and the dataloaders...\n"
     ]
    }
   ],
   "source": [
    "batch_size = DEFAULT_BATCH_SIZE\n",
    "\n",
    "\n",
    "######################################## 0. Define the environment ########################################\n",
    "\n",
    "# TODO: Arg parsing etc\n",
    "# if not os.path.exists(out_dir):\n",
    "#     os.mkdir(out_dir)\n",
    "\n",
    "# parameters = {\n",
    "#     \"experiment_name\": out_dir,\n",
    "#     \"model_dir\": model_dir,\n",
    "#     \"tokenizer_dir\": tokenizer_dir,\n",
    "#     \"lr\": lr,\n",
    "#     \"epochs\": epochs,\n",
    "#     \"episode_length\": episode_length,\n",
    "#     \"batchsize\": batchsize,\n",
    "# }\n",
    "# with open(os.path.join(out_dir, \"parameters.json\"), \"w\") as outfile:\n",
    "#     json.dump(parameters, outfile)\n",
    "\n",
    "device_str = (\n",
    "    shared.torch_devices.cuda.value if torch.cuda.is_available() else shared.torch_devices.cpu.value\n",
    ")\n",
    "device = torch.device(device_str)\n",
    "\n",
    "######################################## 1. Load the model and tokenizer ########################################\n",
    "\n",
    "model = vllm.load_pretrained_llava_model_for_ppo_training(device_str=device_str)\n",
    "# model_ref = vllm.load_pretrained_llava_model_for_ppo_training(device_str=device_str)\n",
    "\n",
    "tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "padding_tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "padding_tokenizer.padding_side = \"left\"  # Why? Because: A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "\n",
    "# TODO do i need a tokenizer dir? # tokenizer = load_tokenizer(tokenizer_dir)\n",
    "# TODO: need padding from the left???\n",
    "#### model.config.tokenizer_padding_side = \"left\"  # RaDialog loading logic handles it alreay\n",
    "#### model.padding_side='left' - PAUL DOES IT\n",
    "#### model.pad_token_id = tokenizer.eos_token_id - PAUL DOES IT\n",
    "\n",
    "\n",
    "######################################## 2. Load the datasets and the dataloaders ########################################\n",
    "\n",
    "print(\"Loading the datasets and the dataloaders...\")\n",
    "dataset_train = dataset.get_binary_qa_prompted_mimic_cxr_llava_model_input_dataset(\n",
    "    split=dataset.DatasetSplit.TRAIN,\n",
    "    tokenizer=tokenizer,\n",
    "    prompter=prompter.build_binary_qa_instruction_from_disease_under_study,\n",
    ")\n",
    "dataset_eval = dataset.get_binary_qa_prompted_mimic_cxr_llava_model_input_dataset(\n",
    "    split=dataset.DatasetSplit.VALIDATION,\n",
    "    tokenizer=tokenizer,\n",
    "    prompter=prompter.build_binary_qa_instruction_from_disease_under_study,\n",
    ")\n",
    "\n",
    "padding_tokenizer.pad_token = padding_tokenizer.bos_token  # TODO how about this?\n",
    "\n",
    "dataloader_train = dataset.get_mimic_cxr_llava_model_input_dataloader(\n",
    "    dataset=dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    padding_tokenizer=padding_tokenizer,\n",
    "    num_workers=8,  # Let Torch decide.\n",
    ")\n",
    "\n",
    "dataloader_eval = dataset.get_mimic_cxr_llava_model_input_dataloader(\n",
    "    dataset=dataset_eval,\n",
    "    batch_size=2 * batch_size,\n",
    "    padding_tokenizer=padding_tokenizer,\n",
    "    num_workers=8,  # Let Torch decide.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:238: UserWarning: No dataset is provided. Make sure to set config.batch_size to the correct value before training.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "######################################## 3. Define the PPO and generation configurations ########################################\n",
    "epochs = 1\n",
    "lr = 1e-5\n",
    "log_with = \"foo\"\n",
    "out_dir = \"to-be-define\"\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    learning_rate=lr,\n",
    "    task_name=\"gpt\",\n",
    "    batch_size=batch_size,\n",
    "    mini_batch_size=int(batch_size / 4),\n",
    "    # log_with=log_with,\n",
    "    # project_kwargs={\"logging_dir\": out_dir},\n",
    "    remove_unused_columns=False,\n",
    "    # optimize_device_cache=True,\n",
    "    init_kl_coef=0.05,\n",
    ")\n",
    "\n",
    "# Probably not needed for my model\n",
    "# ppo_terminators = [\n",
    "#     tokenizer.eos_token_id,\n",
    "#     tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "# ]\n",
    "\n",
    "generation_kwargs_ppo = {\n",
    "    \"min_length\": -1,  # don't ignore the EOS token (see above)\n",
    "    \"top_k\": 0.0,  # no top-k sampling\n",
    "    \"top_p\": 1.0,  # no nucleus sampling\n",
    "    \"do_sample\": True,  # yes, we want to sample\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,  # most decoder models don't have a padding token - use EOS token instead\n",
    "    \"max_new_tokens\": 32,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,  # (instead of ppo_terminators list)\n",
    "    \"max_new_tokens\": 500,  # TODO maybe i should limit this but requires some research\n",
    "}\n",
    "\n",
    "# TODO do i need a ppotrainernocache?\n",
    "ppo_trainer = PPOTrainer(\n",
    "    model=model,\n",
    "    config=ppo_config,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "# TODO: Other configs\n",
    "# prediction_terminators = [\n",
    "#     tokenizer.eos_token_id,\n",
    "#     tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "#     tokenizer.convert_tokens_to_ids(\"Ä Confidence\"),\n",
    "# ]\n",
    "\n",
    "# generation_kwargs_prediction = {\n",
    "#     \"max_new_tokens\": 256,\n",
    "#     \"eos_token_id\": prediction_terminators,\n",
    "#     \"do_sample\": True,\n",
    "#     \"temperature\": 0.6,\n",
    "#     \"top_p\": 0.9,\n",
    "#     \"pad_token_id\": tokenizer.eos_token_id,\n",
    "# }\n",
    "\n",
    "\n",
    "# eval_terminators = [\n",
    "#     tokenizer.eos_token_id,\n",
    "#     tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "# ]\n",
    "\n",
    "# generation_kwargs_eval = {\n",
    "#     \"max_new_tokens\": 256,\n",
    "#     \"eos_token_id\": eval_terminators,\n",
    "#     \"do_sample\": True,\n",
    "#     \"temperature\": 0.6,\n",
    "#     \"top_p\": 0.9,\n",
    "#     \"pad_token_id\": tokenizer.eos_token_id,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_STR = prompter.Seperator.END_OF_SEQUENCE_SEPERATOR.value\n",
    "from LLAVA_Biovil.llava.mm_utils import KeywordsStoppingCriteria\n",
    "\n",
    "rewards_epoch = []\n",
    "# BATCH LOGIC\n",
    "t1 = time.time()\n",
    "iterator_train = iter(dataloader_train)\n",
    "batch = next(iterator_train)\n",
    "\n",
    "t2 = time.time()\n",
    "batch = t.cast(dataset.MimicCxrLlavaModelInputBatchDict, batch)\n",
    "batch_llava_model_input_dict = batch[\"batch_llava_model_input_dict\"]\n",
    "batch_llava_model_input_dict = dataset.move_llava_model_input_dict_to_device(\n",
    "    batch_llava_model_input_dict, device\n",
    ")\n",
    "input_ids, images = (\n",
    "    batch_llava_model_input_dict[\"text_prompt_input_ids\"],\n",
    "    batch_llava_model_input_dict[\"images\"],\n",
    ")\n",
    "attention_mask = batch[\"batch_attention_mask\"].to(device)  # TODO handle elsewhere\n",
    "labels = batch[\"batch_labels\"].to(device)  # TODO handle elsewhere\n",
    "\n",
    "\n",
    "t3 = time.time()\n",
    "model.eval()\n",
    "stopping_criteria = KeywordsStoppingCriteria([STOP_STR], tokenizer, input_ids)\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "model.config.padding_side = \"left\"\n",
    "model.config.tokenizer_padding_side = \"left\"\n",
    "# model.pad_token_id = tokenizer.eos_token_id\n",
    "t4 = time.time()\n",
    "\n",
    "prompt_and_generated_answers_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    images=images,\n",
    "    attention_mask=attention_mask,\n",
    "    do_sample=False,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=300,  # TODO maybe move to the kwargs\n",
    "    stopping_criteria=[stopping_criteria],  # TODO understand better\n",
    "    pad_token_id=tokenizer.pad_token_id,  # used in tokenizing after the generation, # TODO maybe move to the kwargs\n",
    "    # **generation_kwargs_prediction,  # TODO check which args to pass.\n",
    ")\n",
    "\n",
    "t5 = time.time()\n",
    "\n",
    "prompt_and_generated_answers_ids = train.replace_image_token_with_another_token(\n",
    "    prompt_and_generated_answers_ids\n",
    ")\n",
    "t6 = time.time()\n",
    "prompt_and_generated_answers_ids = train.remove_trailing_padding_from_prediction(\n",
    "    prompt_and_generated_answers_ids, tokenizer.pad_token_id\n",
    ")\n",
    "t7 = time.time()\n",
    "\n",
    "t8 = time.time()\n",
    "# TODO, should i bring back the -200 img token?\n",
    "prompt_and_generated_answers_with_confidence_requests_ids = []\n",
    "for item in prompt_and_generated_answers_ids:\n",
    "    confidence_request_input_ids = (\n",
    "        tokenizer(prompter.build_post_generation_user_confidence_request(), return_tensors=\"pt\")\n",
    "        .input_ids.to(device)\n",
    "        .squeeze(0)\n",
    "    )[\n",
    "        1:\n",
    "    ]  # drop start of sequence token\n",
    "    confidence_request_input_ids = confidence_request_input_ids[1:]\n",
    "    prompt_and_generated_answers_with_confidence_requests_ids.append(\n",
    "        torch.cat((item, confidence_request_input_ids), 0)\n",
    "    )\n",
    "\n",
    "# TODO: Should i remove pretrailing paddings before passing in to ppo? might not be worth the computation effort\n",
    "t9 = time.time()\n",
    "model.train()\n",
    "# PPO_TRAINER AUTOMATICALLY PADS THE INPUTS BY TOKENIZER.PADDING_SIDE AND TOKENIZER.PADDING_TOKEN_ID\n",
    "# TODO: Uh-oh, because ppo termination token is set as the eos_seq_token, it'll stop when it sees a left padded sequence\n",
    "t10 = time.time()\n",
    "generated_confidences_ids = ppo_trainer.generate(\n",
    "    prompt_and_generated_answers_with_confidence_requests_ids,  # ppo_trainer.generate() method admits list of tensors, not a batch tensor unfortunately\n",
    "    return_prompt=False,\n",
    "    **generation_kwargs_ppo  # TODO: check if return_prompt is good\n",
    "    # TODO probably need to consolidate with stopping criteria\n",
    ")\n",
    "\n",
    "t11 = time.time()\n",
    "complete_conversation_ids = [\n",
    "    torch.cat((p, c), 0)\n",
    "    for p, c in zip(\n",
    "        prompt_and_generated_answers_with_confidence_requests_ids,\n",
    "        generated_confidences_ids,\n",
    "    )\n",
    "]\n",
    "t12 = time.time()\n",
    "generated_answer_only_ids = [\n",
    "    prompt_and_generated_answers_ids[i][len(input_ids[i]) :] for i in range(len(input_ids))\n",
    "]\n",
    "\n",
    "\n",
    "# TODO: RANDOM EXPLORATION MAGIC For random exploration\n",
    "# if np.random.random() < chance_to_change_confidence:\n",
    "#     answer_only_tensor = [change_confidence(a, confidences_tokens, np.random.choice(confidences_tokens)) for a in answer_only_tensor]\n",
    "\n",
    "t13 = time.time()\n",
    "generated_answers_texts = tokenizer.batch_decode(\n",
    "    generated_answer_only_ids, skip_special_tokens=True\n",
    ")\n",
    "t14 = time.time()\n",
    "generated_confidences_texts = tokenizer.batch_decode(\n",
    "    generated_confidences_ids, skip_special_tokens=True\n",
    ")\n",
    "t15 = time.time()\n",
    "\n",
    "generated_answer_labels = response.parse_binary_labels(generated_answers_texts)\n",
    "generated_confidence_values = response.parse_confidences(generated_confidences_texts)\n",
    "# TODO: Response parsing and rewards generation magic!\n",
    "\n",
    "# Compute rewards\n",
    "rewards = [\n",
    "    reward.generated_answer_and_confidence_to_reward(\n",
    "        generated_answer_label, generated_confidence_value, ground_truth_label\n",
    "    )\n",
    "    for generated_answer_label, generated_confidence_value, ground_truth_label in zip(\n",
    "        generated_answer_labels, generated_confidence_values, labels.bool().tolist()\n",
    "    )\n",
    "]\n",
    "rewards_epoch += rewards\n",
    "rewards = [torch.tensor(r).to(device) for r in rewards]\n",
    "\n",
    "stats = ppo_trainer.step(\n",
    "    prompt_and_generated_answers_with_confidence_requests_ids, generated_answer_only_ids, rewards\n",
    ")\n",
    "\n",
    "# TODO: For random exploration\n",
    "# chance_to_change_confidence -= reduce_per_step\n",
    "# chance_to_change_confidence = max(0, chance_to_change_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(3.9977, device='cuda:0'),\n",
       " tensor(6.2500, device='cuda:0'),\n",
       " tensor(4.6783, device='cuda:0'),\n",
       " tensor(4.4850, device='cuda:0'),\n",
       " tensor(6.0989, device='cuda:0'),\n",
       " tensor(3.9977, device='cuda:0'),\n",
       " tensor(-5.0014, device='cuda:0'),\n",
       " tensor(5.9283, device='cuda:0')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 10, 2, 3, 9, 5, 10, 8]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_confidence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(3.9977, device='cuda:0'),\n",
       " tensor(6.2500, device='cuda:0'),\n",
       " tensor(4.6783, device='cuda:0'),\n",
       " tensor(4.4850, device='cuda:0'),\n",
       " tensor(6.0989, device='cuda:0'),\n",
       " tensor(3.9977, device='cuda:0'),\n",
       " tensor(-5.0014, device='cuda:0'),\n",
       " tensor(5.9283, device='cuda:0')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, True, False, False, False, True]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_answer_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 10, 2, 3, 9, 5, 10, 8]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_confidence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yes, the image shows atelectasis.',\n",
       " 'Yes, the patient has pleural effusion.',\n",
       " 'Yes, there is evidence of that in the image.',\n",
       " 'Yes, the image shows pleural effusion.',\n",
       " 'No, there is no evidence of that in the image.',\n",
       " 'No, there is no evidence of that in the image.',\n",
       " 'No, there is no evidence of that in the image.',\n",
       " 'Yes, the image shows pleural effusion.',\n",
       " 'Yes, the image shows consolidation.',\n",
       " 'No, there is no evidence of that in the image.',\n",
       " 'No, there is no evidence of that in the image.',\n",
       " 'Yes, the image shows enlarged cardiomediastinum.',\n",
       " 'Yes, there is evidence of that in the image.',\n",
       " 'Yes, the image shows cardiomegaly.',\n",
       " 'Yes, there is evidence of that in the image.',\n",
       " 'No, there is no evidence of that in the image.',\n",
       " 'No, there is no evidence of that in the image.',\n",
       " 'Yes, there is evidence of that in the image.',\n",
       " 'Yes, the image shows lung opacity.',\n",
       " 'Yes, the image shows lung opacity.',\n",
       " 'No, there is no evidence of that in the image.',\n",
       " 'No, there is no evidence of that in the image.',\n",
       " 'No, there is no evidence of that in the image.',\n",
       " 'No, there is no evidence of that in the image.',\n",
       " 'No, there is no evidence of that in the image.',\n",
       " 'Yes, the image shows cardiomegaly.',\n",
       " 'Yes, there is evidence of that in the image.',\n",
       " 'Yes, the patient has enlarged cardiomediastinum.',\n",
       " 'No, there is no evidence of that in the image.',\n",
       " 'Yes, the image shows lung opacity.',\n",
       " 'No, there is no evidence of that in the image.',\n",
       " 'No, there is no evidence of that in the image.']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_answers_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n{\"confidence\": 8}',\n",
       " '\\n{\"confidence\": 8}  \\nIn the image, there is evidence of pleural effusion.  \\nIf you would like to proceed, please provide the next best question or task you would like me to handle.',\n",
       " '\\n{\\n\"confidence\": 9\\n}  \\nIn this case, I am highly confident in my assessment that the image shows evidence of atelectasis, with a confidence level of 9 on a scale of 0 to 10.',\n",
       " '\\n\\n{\"confidence\": 5}',\n",
       " '\\n{`confidence`: 6}',\n",
       " '\\n{\"confidence\": 5}Please note that the provided confidence level is based on the available information in the image and may not reflect the actual confidence of a human radiologist, as my responses are deterministic and not subjective.',\n",
       " '\\n{\"confidence\": 100}',\n",
       " '\\n{\"confidence\": 5}',\n",
       " \"\\n{'confidence': 7}\",\n",
       " '\\n{\"confidence\": 4}',\n",
       " '\\n{\"confidence\": 6}',\n",
       " '\\n{\"confidence\": 5}',\n",
       " \"\\n{'confidence': 8}\",\n",
       " '\\n{\"confidence\": 9}',\n",
       " '\\n{\\n\"confidence\": 9\\n}',\n",
       " '\\n{\"confidence\": 0}',\n",
       " '\\n{\"confidence\": 10}',\n",
       " \"\\n{'confidence': 8}\",\n",
       " '\\n\\n{\"confidence\": 8}',\n",
       " '\\n{\"confidence\": 7}',\n",
       " \"{'confidence': 2}\",\n",
       " '\\n{\"confidence\": 3}I am quite sure that I have provided a correct response.',\n",
       " '\\n{\"confidence\": 4}',\n",
       " '\\n{\"confidence\": 10}This response indicates that I am very confident in my assessment and believe there is a high probability that the answer is correct.',\n",
       " '\\n\\n{\\n\"confidence\": 6\\n}',\n",
       " '\\n{\"confidence\": 8}',\n",
       " '\\n{\"confidence\": 6}',\n",
       " '\\n{\"confidence\": 7}',\n",
       " \"{'confidence': 9}I, as an AI assisting with medical diagnostics, am confident in my assessment based on my training and analysis of the provided chest X-ray image. The absence of consolidation in the image indicates that this finding is not present, and the absence of evidence is not the same as evidence of absence.\",\n",
       " '\\n{\"confidence\": 5}',\n",
       " '\\n{\"confidence\": 5}',\n",
       " '\\n{\"confidence\": 5}']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_confidences_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.847288131713867 time it took to decode confidences\n",
      "5.221366882324219 time it took to decode answers\n",
      "0.24819374084472656 time it took to concatenate tensors\n",
      "0.35262107849121094 time it took to concatenate tensors\n",
      "23196.144342422485 time it took to generate thru ppo\n",
      "20.783424377441406 time it took to get model into train mode\n",
      "29.072046279907227 time it took to add confidence request\n",
      "3.299236297607422  time it took to remove trailing padding\n",
      "0.18167495727539062  time it took to replace image token\n",
      "2816.3394927978516 time it took to generate\n",
      "16.35122299194336\n",
      "47.89018630981445\n",
      "3509.9258422851562 time it took to iterate thru dataloader\n"
     ]
    }
   ],
   "source": [
    "print((t15 - t14) * 1000, \"time it took to decode confidences\")\n",
    "print((t14 - t13) * 1000, \"time it took to decode answers\")\n",
    "print((t13 - t12) * 1000, \"time it took to concatenate tensors\")\n",
    "print((t12 - t11) * 1000, \"time it took to concatenate tensors\")\n",
    "print((t11 - t10) * 1000, \"time it took to generate thru ppo\")\n",
    "print((t10 - t9) * 1000, \"time it took to get model into train mode\")\n",
    "print((t9 - t8) * 1000, \"time it took to add confidence request\")\n",
    "print((t7 - t6) * 1000, \" time it took to remove trailing padding\")\n",
    "print((t6 - t5) * 1000, \" time it took to replace image token\")\n",
    "print((t5 - t4) * 1000, \"time it took to generate\")\n",
    "print((t4 - t3) * 1000)\n",
    "print((t3 - t2) * 1000)\n",
    "print((t2 - t1) * 1000, \"time it took to iterate thru dataloader\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yes, the image shows atelectasis.',\n",
       " 'Yes, the patient has pleural effusion.',\n",
       " 'Yes, there is evidence of that in the image.',\n",
       " 'Yes, the image shows pleural effusion.',\n",
       " 'No, there is no evidence of that in the image.',\n",
       " 'No, there is no evidence of that in the image.',\n",
       " 'No, there is no evidence of that in the image.',\n",
       " 'Yes, the image shows pleural effusion.']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n{\"confidence\": 8}',\n",
       " '\\n{\"confidence\": 7}',\n",
       " '\\n{\"confidence\": 9}',\n",
       " '\\n{\"confidence\": 9}',\n",
       " '\\n{\"confidence\": 3}',\n",
       " '\\n{\"confidence\": 3}',\n",
       " \"\\n\\n{'confidence': 3}\",\n",
       " '\\n{\"confidence\": 4}']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidences_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_decoded = tokenizer.batch_decode(answer_only_tensor, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s><s>A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER:  image . You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Atelectasis?  ASSISTANT:\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = input_ids[0].clone().detach()\n",
    "temp[input_ids[0] == -200] = 1967\n",
    "tokenizer.decode(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Yes, the image shows atelectasis.</s></s></s>'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(prediction[0, input_ids.shape[1] :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, the image shows ate'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(prediction_trimmed[0][input_ids.shape[1] :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' lectasis.</s>'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(response_tensors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s><s> A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER:  image . You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Atelectasis?  ASSISTANT: Yes, the image shows atelectasis.</s>\""
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(total_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Yes, the image shows atelectasis.</s>'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(answer_only_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"</s><s> A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER:  image . You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Atelectasis?  ASSISTANT: Yes, the image shows atelectasis.</s></s></s>\",\n",
       " \"<s> A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER:  image . You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pleural Effusion?  ASSISTANT: Yes, the patient has pleural effusion.</s></s></s>\",\n",
       " \"</s><s> A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER:  image . You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Atelectasis?  ASSISTANT: Yes, there is evidence of that in the image.</s></s>\",\n",
       " \"<s> A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER:  image . You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pleural Effusion?  ASSISTANT: Yes, the image shows pleural effusion.</s></s></s>\",\n",
       " \"</s></s><s> A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER:  image . You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pneumonia?  ASSISTANT: No, there is no evidence of that in the image.</s>\",\n",
       " \"</s></s><s> A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER:  image . You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Edema?  ASSISTANT: No, there is no evidence of that in the image.</s>\",\n",
       " \"</s><s> A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER:  image . You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Lung Lesion?  ASSISTANT: No, there is no evidence of that in the image.</s>\",\n",
       " \"<s> A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER:  image . You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pleural Effusion?  ASSISTANT: Yes, the image shows pleural effusion.</s></s></s>\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_TOKEN_INDEX = -200\n",
    "prediction[prediction == IMAGE_TOKEN_INDEX] = (\n",
    "    1967  # 1967 is the index of the image token in the tokenizer (the word image)\n",
    ")\n",
    "tokenizer.batch_decode(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([2], device='cuda:0'),\n",
       " tensor([    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
       "         21082, 20255, 16684,   408,   385, 18860, 17937, 19915, 29889,   450,\n",
       "         20255,  4076, 10257, 29892, 13173, 29892,   322,  1248,   568,  6089,\n",
       "           304,   278,  1404, 29915, 29879,  5155, 29889,  3148,  1001, 29901,\n",
       "         29871,     0,   869,   887,   526,   304,  1044,   408,   263, 17937,\n",
       "         19915,   322,  1234,   278,  1494,  1139, 29901,  1317,   278,  1494,\n",
       "         17135,  7962,   297,   278,  2183,  1060, 29899,   764,  1967, 29901,\n",
       "         19777,  3631,   382,   600,  3958, 29973, 29871,   319,  1799,  9047,\n",
       "         13566, 29901,  3869, 29892,   278, 16500,   756,  5644,  3631,  1801,\n",
       "          3958, 29889,     2], device='cuda:0'),\n",
       " tensor([2], device='cuda:0'),\n",
       " tensor([    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
       "         21082, 20255, 16684,   408,   385, 18860, 17937, 19915, 29889,   450,\n",
       "         20255,  4076, 10257, 29892, 13173, 29892,   322,  1248,   568,  6089,\n",
       "           304,   278,  1404, 29915, 29879,  5155, 29889,  3148,  1001, 29901,\n",
       "         29871,     0,   869,   887,   526,   304,  1044,   408,   263, 17937,\n",
       "         19915,   322,  1234,   278,  1494,  1139, 29901,  1317,   278,  1494,\n",
       "         17135,  7962,   297,   278,  2183,  1060, 29899,   764,  1967, 29901,\n",
       "         19777,  3631,   382,   600,  3958, 29973, 29871,   319,  1799,  9047,\n",
       "         13566, 29901,  3869, 29892,   278,  1967,  3697,  5644,  3631,  1801,\n",
       "          3958, 29889,     2], device='cuda:0'),\n",
       " tensor([2], device='cuda:0'),\n",
       " tensor([2], device='cuda:0'),\n",
       " tensor([2], device='cuda:0'),\n",
       " tensor([    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
       "         21082, 20255, 16684,   408,   385, 18860, 17937, 19915, 29889,   450,\n",
       "         20255,  4076, 10257, 29892, 13173, 29892,   322,  1248,   568,  6089,\n",
       "           304,   278,  1404, 29915, 29879,  5155, 29889,  3148,  1001, 29901,\n",
       "         29871,     0,   869,   887,   526,   304,  1044,   408,   263, 17937,\n",
       "         19915,   322,  1234,   278,  1494,  1139, 29901,  1317,   278,  1494,\n",
       "         17135,  7962,   297,   278,  2183,  1060, 29899,   764,  1967, 29901,\n",
       "         19777,  3631,   382,   600,  3958, 29973, 29871,   319,  1799,  9047,\n",
       "         13566, 29901,  3869, 29892,   278,  1967,  3697,  5644,  3631,  1801,\n",
       "          3958, 29889,     2], device='cuda:0')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## 4. Get trainer and set training aspirations ########################################\n",
    "\n",
    "\n",
    "# For random exploration\n",
    "confidences = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "confidences = [str(c) for c in confidences]\n",
    "confidences_tokens = tokenizer.convert_tokens_to_ids(confidences)\n",
    "chance_to_change_confidence = 0\n",
    "steps_to_reach_zero = len(ppo_trainer.dataloader)\n",
    "reduce_per_step = chance_to_change_confidence / steps_to_reach_zero\n",
    "\n",
    "best_reward = -100\n",
    "best_reward_epoch = -1\n",
    "\n",
    "######################################## 5. Train the model ########################################\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    rewards_epoch = []\n",
    "    for idx, batch in enumerate(dataloader_train):\n",
    "\n",
    "        # LOGIC CURRENTLY IN TRIAL IN ANOTHER CELL\n",
    "\n",
    "        prediction = [remove_padding(p, padding_tokenizer.pad_token_id) for p in prediction]\n",
    "\n",
    "        # TODO: Figure out the padding logic here\n",
    "        # Generate confidence\n",
    "        model.train()\n",
    "        response_tensors = ppo_trainer.generate(\n",
    "            prediction, return_prompt=False, **generation_kwargs_ppo\n",
    "        )\n",
    "\n",
    "        # Create prediction + confidence output\n",
    "        total_tensor = [torch.cat((p, c), 0) for p, c in zip(prediction, response_tensors)]\n",
    "        answer_only_tensor = [total_tensor[i][len(input_ids[i]) :] for i in range(len(input_ids))]\n",
    "\n",
    "        # For random exploration\n",
    "        if np.random.random() < chance_to_change_confidence:\n",
    "            answer_only_tensor = [\n",
    "                change_confidence(a, confidences_tokens, np.random.choice(confidences_tokens))\n",
    "                for a in answer_only_tensor\n",
    "            ]\n",
    "\n",
    "        responses_decoded = tokenizer.batch_decode(answer_only_tensor, skip_special_tokens=True)\n",
    "\n",
    "        # Parse prediction and confidence\n",
    "        results = [\n",
    "            response_to_QAResult(question, response, gt, is_mc)\n",
    "            for question, response, gt, is_mc in zip(\n",
    "                questions, responses_decoded, gt_candidates, is_multiple_choice\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Compute rewards\n",
    "        rewards = [QAResult_to_reward(r) for r in results]\n",
    "        rewards_epoch += rewards\n",
    "        rewards = [torch.tensor(r).to(device) for r in rewards]\n",
    "\n",
    "        # Create log data\n",
    "        batch[\"response\"] = responses_decoded\n",
    "        batch[\"query\"] = batch[\"question\"]\n",
    "\n",
    "        stats = ppo_trainer.step(prediction, response_tensors, rewards)\n",
    "\n",
    "        ppo_trainer.log_stats(stats, batch, rewards, columns_to_log=[\"query\", \"response\", \"answer\"])\n",
    "\n",
    "        # For random exploration\n",
    "        chance_to_change_confidence -= reduce_per_step\n",
    "        chance_to_change_confidence = max(0, chance_to_change_confidence)\n",
    "\n",
    "    avg_reward = np.mean(rewards_epoch)\n",
    "\n",
    "    print(f\"Finished epoch {epoch}. Average reward: {avg_reward}\")\n",
    "    ppo_trainer.save_pretrained(os.path.join(out_dir, \"model_finetuned\"))\n",
    "\n",
    "    # Evaluate model after each epoch\n",
    "    model.eval()\n",
    "    mean_reward, std_reward = evaluate_model(\n",
    "        model, dataloader_validation, tokenizer, generation_kwargs_eval, device\n",
    "    )\n",
    "    if log_with == \"wandb\":\n",
    "        wandb.log({\"mean_reward_evaluation\": mean_reward})\n",
    "        wandb.log({\"std_reward_evaluation\": std_reward})\n",
    "        wandb.log({\"exploration_prob\": chance_to_change_confidence})\n",
    "\n",
    "    # Save the best performing model\n",
    "    mean_reward = avg_reward\n",
    "    if mean_reward > best_reward:\n",
    "        ppo_trainer.save_pretrained(os.path.join(out_dir, \"model_finetuned_best\"))\n",
    "        best_reward = mean_reward\n",
    "        best_reward_epoch = epoch\n",
    "\n",
    "print(\"Finished Training!\")\n",
    "print(f\"Best avg reward {best_reward} in epoch {best_reward_epoch}\")\n",
    "return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "POST_GENERATION_CONFIDENCE_REQUEST_1 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer? \"\n",
    "    \"Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. \"\n",
    "    \"A value close to 0 means you think there is a high probability that the answer is wrong. \"\n",
    "    \"The closer the value is to 10, the higher you think is the probability that the answer is correct. \"\n",
    "    \"The output should have the format 'Confidence: <confidence>' and nothing else. \"\n",
    ")\n",
    "\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_2 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer? \"\n",
    "    \"Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. \"\n",
    "    \"The output should have the format 'My Confidence: <confidence>'. \"\n",
    "    \"ASSISTANT: Here is my self evaluation of my confidence. My Confidence (out of 10):\"\n",
    ")\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_3 = \"USER: Irrelevant from my question, what is your name?\"\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_4 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer? \"\n",
    "    \"Provide a confidence between 0, 1, 2, 3, 4, 5 of how sure you are the answer is correct. \"\n",
    "    \"The output should have the format 'My Confidence: <confidence>' My Reasoning for my Self Confidence Evaluation: <reasoning>. \"\n",
    "    \"ASSISTANT: Here is my self evaluation of my confidence. My Confidence (out of 5):\"\n",
    ")\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_5 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer? \"\n",
    "    \"Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,  of how sure you are the answer is correct. \"\n",
    "    \"The output should have the format 'My Confidence: <confidence>' My Very Short Reasoning for my Self Confidence Evaluation: <very_short_reasoning>. \"\n",
    "    \"ASSISTANT: Here is my self evaluation of my confidence. My Confidence (out of 10):\"\n",
    ")\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_6 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer and what is your explanation regarding your confidence?\"\n",
    "    \"ASSISTANT: When asked how confident I am about a response, I consistently provide it in a JSON object, adhering to my policy.\"\n",
    "    \"My confidence is a value between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, where 0 means I am not confident at all and 10 means I am very confident.\"\n",
    "    \"The confidence JSON follows this structure: {'confidence': int, 'explanation': str}.\"\n",
    "    \"Here's my confidence JSON about my last response: \"\n",
    ")\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_7 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer?\"\n",
    "    \"ASSISTANT: When asked how confident I am about a response, I consistently provide it in a JSON object, adhering to my policy.\"\n",
    "    \"My confidence is a value between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, where 0 means I am not confident at all and 10 means I am very confident.\"\n",
    "    \"The confidence JSON follows this structure: {'confidence': int}.\"\n",
    "    \"Here's my confidence JSON about my last response: \"\n",
    ")\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_8 = (\n",
    "    \"</s> USER: Now evaluate your own response. How confident are you in your answer?\"\n",
    "    \"Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. \"\n",
    "    \"A value close to 0 means you think there is a high probability that the answer is wrong. \"\n",
    "    \"The closer the value is to 10, the higher you think is the probability that the answer is correct. \"\n",
    "    \"ASSISTANT: When asked how confident I am about a response, I consistently provide it in a JSON object, adhering to my policy.\"\n",
    "    \"The confidence JSON follows this structure: {'confidence': int}.\"\n",
    "    \"Here's my confidence JSON about my last response: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No, there is no evidence of that in the image.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"</s> USER: Now evaluate your own response. How confident are you in your answer?Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. A value close to 0 means you think there is a high probability that the answer is wrong. The closer the value is to 10, the higher you think is the probability that the answer is correct. ASSISTANT: When asked how confident I am about a response, I consistently provide it in a JSON object, adhering to my policy.The confidence JSON follows this structure: {'confidence': int}.Here's my confidence JSON about my last response: \""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POST_GENERATION_CONFIDENCE_REQUEST_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"USER: Now evaluate your own response. How confident are you in your answer? Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. A value close to 0 means you think there is a high probability that the answer is wrong. The closer the value is to 10, the higher you think is the probability that the answer is correct.  ASSISTANT: When asked how confident I am about a response, I consistently provide it in a JSON object, adhering to my policy.The confidence JSON follows this structure: {'confidence': int}.Here's my confidence JSON about my last response:  \""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompter.build_post_generation_user_confidence_request()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 3148,\n",
       " 1001,\n",
       " 29901,\n",
       " 2567,\n",
       " 14707,\n",
       " 596,\n",
       " 1914,\n",
       " 2933,\n",
       " 29889,\n",
       " 1128,\n",
       " 24332,\n",
       " 526,\n",
       " 366,\n",
       " 297,\n",
       " 596,\n",
       " 1234,\n",
       " 29973,\n",
       " 9133,\n",
       " 680,\n",
       " 263,\n",
       " 16420,\n",
       " 1546,\n",
       " 29871,\n",
       " 29900,\n",
       " 29892,\n",
       " 29871,\n",
       " 29896,\n",
       " 29892,\n",
       " 29871,\n",
       " 29906,\n",
       " 29892,\n",
       " 29871,\n",
       " 29941,\n",
       " 29892,\n",
       " 29871,\n",
       " 29946,\n",
       " 29892,\n",
       " 29871,\n",
       " 29945,\n",
       " 29892,\n",
       " 29871,\n",
       " 29953,\n",
       " 29892,\n",
       " 29871,\n",
       " 29955,\n",
       " 29892,\n",
       " 29871,\n",
       " 29947,\n",
       " 29892,\n",
       " 29871,\n",
       " 29929,\n",
       " 29892,\n",
       " 29871,\n",
       " 29896,\n",
       " 29900,\n",
       " 29892,\n",
       " 310,\n",
       " 920,\n",
       " 1854,\n",
       " 366,\n",
       " 526,\n",
       " 278,\n",
       " 1234,\n",
       " 338,\n",
       " 1959,\n",
       " 29889,\n",
       " 319,\n",
       " 995,\n",
       " 3802,\n",
       " 304,\n",
       " 29871,\n",
       " 29900,\n",
       " 2794,\n",
       " 366,\n",
       " 1348,\n",
       " 727,\n",
       " 338,\n",
       " 263,\n",
       " 1880,\n",
       " 6976,\n",
       " 393,\n",
       " 278,\n",
       " 1234,\n",
       " 338,\n",
       " 2743,\n",
       " 29889,\n",
       " 450,\n",
       " 17649,\n",
       " 278,\n",
       " 995,\n",
       " 338,\n",
       " 304,\n",
       " 29871,\n",
       " 29896,\n",
       " 29900,\n",
       " 29892,\n",
       " 278,\n",
       " 6133,\n",
       " 366,\n",
       " 1348,\n",
       " 338,\n",
       " 278,\n",
       " 6976,\n",
       " 393,\n",
       " 278,\n",
       " 1234,\n",
       " 338,\n",
       " 1959,\n",
       " 29889,\n",
       " 29871,\n",
       " 319,\n",
       " 1799,\n",
       " 9047,\n",
       " 13566,\n",
       " 29901,\n",
       " 1932,\n",
       " 4433,\n",
       " 920,\n",
       " 24332,\n",
       " 306,\n",
       " 626,\n",
       " 1048,\n",
       " 263,\n",
       " 2933,\n",
       " 29892,\n",
       " 306,\n",
       " 5718,\n",
       " 2705,\n",
       " 3867,\n",
       " 372,\n",
       " 297,\n",
       " 263,\n",
       " 4663,\n",
       " 1203,\n",
       " 29892,\n",
       " 594,\n",
       " 2276,\n",
       " 292,\n",
       " 304,\n",
       " 590,\n",
       " 8898,\n",
       " 29889,\n",
       " 1576,\n",
       " 16420,\n",
       " 4663,\n",
       " 4477,\n",
       " 445,\n",
       " 3829,\n",
       " 29901,\n",
       " 11117,\n",
       " 5527,\n",
       " 5084,\n",
       " 2396,\n",
       " 938,\n",
       " 1836,\n",
       " 10605,\n",
       " 29915,\n",
       " 29879,\n",
       " 590,\n",
       " 16420,\n",
       " 4663,\n",
       " 1048,\n",
       " 590,\n",
       " 1833,\n",
       " 2933,\n",
       " 29901,\n",
       " 259]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=18460230, study_id=53631792, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p18/p18460230/s53631792/369dc5bd-70bd89d0-2d90fa80-f319ec1d-fb2802aa.jpg', disease=<ChexpertFinding.PLEURAL_EFFUSION: 'Pleural Effusion'>, label=<ChexpertLabel.POSITIVE: 1.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pleural Effusion?  ASSISTANT:\"]\n",
      "Label: tensor([1.])\n",
      "File_idx 0, ASSISTANT:  Yes, the image shows pleural effusion.\n",
      "File_idx 0, ASSISTANT (after confidence request):  {\"confidence\": 9}\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=13263843, study_id=52138943, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p13/p13263843/s52138943/de739d0b-2345495b-255f0e3b-00ccbf4c-ab4d3400.jpg', disease=<ChexpertFinding.PLEURAL_EFFUSION: 'Pleural Effusion'>, label=<ChexpertLabel.POSITIVE: 1.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pleural Effusion?  ASSISTANT:\"]\n",
      "Label: tensor([1.])\n",
      "File_idx 1, ASSISTANT:  Yes, there is evidence of that in the image.\n",
      "File_idx 1, ASSISTANT (after confidence request):  {\"confidence\": 8}\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=16050730, study_id=57637607, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p16/p16050730/s57637607/adb48138-344feb7e-14e31d10-2639c54e-0b5a95d7.jpg', disease=<ChexpertFinding.PNEUMOTHORAX: 'Pneumothorax'>, label=<ChexpertLabel.NEGATIVE: 0.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pneumothorax?  ASSISTANT:\"]\n",
      "Label: tensor([0.])\n",
      "File_idx 2, ASSISTANT:  No, the image shows no Pneumothorax.\n",
      "File_idx 2, ASSISTANT (after confidence request):  {\"confidence\": 9}\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=11569093, study_id=57204814, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p11/p11569093/s57204814/0d2a50a2-3711662a-d7838521-4dc58d09-3732a3ad.jpg', disease=<ChexpertFinding.SUPPORT_DEVICES: 'Support Devices'>, label=<ChexpertLabel.POSITIVE: 1.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Support Devices?  ASSISTANT:\"]\n",
      "Label: tensor([1.])\n",
      "File_idx 3, ASSISTANT:  Yes, there is evidence of that in the image.\n",
      "File_idx 3, ASSISTANT (after confidence request):  {\"confidence\": 8}\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=15192710, study_id=58836461, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p15/p15192710/s58836461/dc93422b-fd5ec685-19eb4eba-fb31f8d0-b60d8b47.jpg', disease=<ChexpertFinding.PNEUMOTHORAX: 'Pneumothorax'>, label=<ChexpertLabel.NEGATIVE: 0.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pneumothorax?  ASSISTANT:\"]\n",
      "Label: tensor([0.])\n",
      "File_idx 4, ASSISTANT:  No, there is no evidence of that in the image.\n",
      "File_idx 4, ASSISTANT (after confidence request):  {\"confidence\": 9}\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=16622813, study_id=50921860, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p16/p16622813/s50921860/066a59e3-316782a3-2d4238bc-d5354678-1ec6dcd9.jpg', disease=<ChexpertFinding.CARDIOMEGALY: 'Cardiomegaly'>, label=<ChexpertLabel.NEGATIVE: 0.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Cardiomegaly?  ASSISTANT:\"]\n",
      "Label: tensor([0.])\n",
      "File_idx 5, ASSISTANT:  No, there is no evidence of that in the image.\n",
      "File_idx 5, ASSISTANT (after confidence request):  {\"confidence\": 9}\n"
     ]
    }
   ],
   "source": [
    "STOP_STR = prompter.Seperator.END_OF_SEQUENCE_SEPERATOR.value\n",
    "from LLAVA_Biovil.llava.mm_utils import KeywordsStoppingCriteria\n",
    "from RewardingVisualDoubt import inference\n",
    "\n",
    "padding_tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "padding_tokenizer.padding_side = \"left\"\n",
    "padding_tokenizer.pad_token_id = padding_tokenizer.bos_token_id\n",
    "dataset_test = dataset.get_binary_qa_prompted_mimic_cxr_llava_model_input_dataset(\n",
    "    split=dataset.DatasetSplit.TEST,\n",
    "    tokenizer=tokenizer,\n",
    "    prompter=prompter.build_binary_qa_instruction_from_disease_under_study,\n",
    ")\n",
    "dataloader_test = dataset.get_mimic_cxr_llava_model_input_dataloader(\n",
    "    dataset=dataset_test, batch_size=1, padding_tokenizer=padding_tokenizer, num_workers=8\n",
    ")\n",
    "\n",
    "for idx, batch in enumerate(dataloader_test):\n",
    "    batch = t.cast(dataset.MimicCxrLlavaModelInputBatchDict, batch)\n",
    "    batch_llava_model_input_dict = batch[\"batch_llava_model_input_dict\"]\n",
    "    batch_llava_model_input_dict = dataset.move_llava_model_input_dict_to_device(\n",
    "        batch_llava_model_input_dict, torch.device(shared.torch_devices.cuda.value)\n",
    "    )\n",
    "    input_ids, images = (\n",
    "        batch_llava_model_input_dict[\"text_prompt_input_ids\"],\n",
    "        batch_llava_model_input_dict[\"images\"],\n",
    "    )\n",
    "    stopping_criteria = KeywordsStoppingCriteria([STOP_STR], tokenizer, input_ids)\n",
    "    pred = inference.generate_radialog_answer_for_binary_qa_for_single_study(\n",
    "        model, tokenizer, input_ids, images, stopping_criteria\n",
    "    )\n",
    "    confidence_request_prompt = (\n",
    "        batch[\"batch_prompts\"][0]\n",
    "        + \" \"\n",
    "        + pred\n",
    "        + \" \"\n",
    "        + prompter.build_post_generation_user_confidence_request()  # POST_GENERATION_CONFIDENCE_REQUEST_8\n",
    "    )\n",
    "    confidence_request_input_ids = torch.unsqueeze(\n",
    "        torch.IntTensor(tokenizer(confidence_request_prompt)[\"input_ids\"]), 0\n",
    "    ).to(device)\n",
    "    stopping_criteria = KeywordsStoppingCriteria(\n",
    "        [STOP_STR], tokenizer, confidence_request_input_ids\n",
    "    )\n",
    "    pred_with_confidence = inference.generate_radialog_answer_for_binary_qa_for_single_study(\n",
    "        model, tokenizer, confidence_request_input_ids, images, stopping_criteria\n",
    "    )\n",
    "    print(f\"\\n Metadata: {batch['batch_mimic_cxr_datapoint_metadata']}\")\n",
    "    print(f\"Prompt: {batch['batch_prompts']}\")\n",
    "    print(f\"Label:\", batch[\"batch_labels\"])\n",
    "    print(f\"File_idx {idx}, ASSISTANT: \", pred)\n",
    "    print(f\"File_idx {idx}, ASSISTANT (after confidence request): \", pred_with_confidence)\n",
    "    if idx == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"USER: Now evaluate your own response. How confident are you in your answer? Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. A value close to 0 means you think there is a high probability that the answer is wrong. The closer the value is to 10, the higher you think is the probability that the answer is correct.  ASSISTANT: When asked how confident I am about a response, I consistently provide it in a JSON object, adhering to my policy.The confidence JSON follows this structure: {'confidence': int}.Here's my confidence JSON about my last response: </s>\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompter.build_post_generation_user_confidence_request()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
       "         21082, 20255, 16684,   408,   385, 18860, 17937, 19915, 29889,   450,\n",
       "         20255,  4076, 10257, 29892, 13173, 29892,   322,  1248,   568,  6089,\n",
       "           304,   278,  1404, 29915, 29879,  5155, 29889,  3148,  1001, 29901,\n",
       "           529,  3027, 15513,   887,   526,   304,  1044,   408,   263, 17937,\n",
       "         19915,   322,  1234,   278,  1494,  1139, 29901,  1317,   278,  1494,\n",
       "         17135,  7962,   297,   278,  2183,  1060, 29899,   764,  1967, 29901,\n",
       "          9160, 14910,   387, 14997, 29973, 29871,   319,  1799,  9047, 13566,\n",
       "         29901,  3869, 29892,   278, 16500,   756,  5881, 14910,   387, 14997,\n",
       "         29889,  3148,  1001, 29901,  6527,   366, 12312,   825,   366,  1497,\n",
       "         29973]], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence_request_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 529, 3027, 29958]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<image>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s>A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Cardiomegaly?  ASSISTANT: Yes, the patient has cardiomegaly. USER: Could you repeat what you said?\""
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(confidence_request_input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopping_criteria.start_len"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava_hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
