{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# TODO Reward function\n",
    "# TODO Response format + response handling\n",
    "# TODO Integrating supervised finetuning\n",
    "# TODO minibatch save + eval checkpoints\n",
    "# TODO arg parsing\n",
    "\n",
    "\n",
    "# TODO determine hyperparams\n",
    "# TODO dataloader num workers set to default\n",
    "# TODO sampling logic (undersample)\n",
    "# TODO binary qa prompt needs to vary between a few samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37cc47d635214999a8c49accbcd46c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 69 files:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% Set script for interactive development and import modules\n",
    "from RewardingVisualDoubt import infrastructure\n",
    "\n",
    "infrastructure.make_ipython_reactive_to_changing_codebase()\n",
    "infrastructure.supress_known_warnings()\n",
    "\n",
    "import pathlib as path\n",
    "import typing as t\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# from LLAVA_Biovil.llava.mm_utils import KeywordsStoppingCriteria\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n",
    "\n",
    "from RewardingVisualDoubt import dataset, mimic_cxr, prompter, shared, vllm, train\n",
    "\n",
    "DEFAULT_BATCH_SIZE = 8\n",
    "DEFAULT_OUTPUT_DIR = path.Path(\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model in non-trainable mode...\n",
      "Model base:  liuhaotian/llava-v1.5-7b\n",
      "Loading LLaVA from base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dbc7aa36e0d40eeadf70d614df93ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3efbf0425c9045328c5d81c764e2d25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading additional LLaVA weights...\n",
      "Using downloaded and verified file: /tmp/biovil_t_image_model_proj_size_128.pt\n",
      "Loaded additional vision tower weights...\n",
      "Adding pretrained RaDialog LoRA adapters and value head to the model...\n"
     ]
    }
   ],
   "source": [
    "batch_size = DEFAULT_BATCH_SIZE\n",
    "\n",
    "\n",
    "######################################## 0. Define the environment ########################################\n",
    "\n",
    "# TODO: Arg parsing etc\n",
    "# if not os.path.exists(out_dir):\n",
    "#     os.mkdir(out_dir)\n",
    "\n",
    "# parameters = {\n",
    "#     \"experiment_name\": out_dir,\n",
    "#     \"model_dir\": model_dir,\n",
    "#     \"tokenizer_dir\": tokenizer_dir,\n",
    "#     \"lr\": lr,\n",
    "#     \"epochs\": epochs,\n",
    "#     \"episode_length\": episode_length,\n",
    "#     \"batchsize\": batchsize,\n",
    "# }\n",
    "# with open(os.path.join(out_dir, \"parameters.json\"), \"w\") as outfile:\n",
    "#     json.dump(parameters, outfile)\n",
    "\n",
    "device_str = (\n",
    "    shared.torch_devices.cuda.value if torch.cuda.is_available() else shared.torch_devices.cpu.value\n",
    ")\n",
    "device = torch.device(device_str)\n",
    "\n",
    "######################################## 1. Load the model and tokenizer ########################################\n",
    "\n",
    "model = vllm.load_pretrained_llava_model_for_ppo_training(device_str=device_str)\n",
    "# model_ref = vllm.load_pretrained_llava_model_for_ppo_training(device_str=device_str)\n",
    "\n",
    "tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "padding_tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "padding_tokenizer.padding_side = \"left\"  # Why? Because: A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "\n",
    "# TODO do i need a tokenizer dir? # tokenizer = load_tokenizer(tokenizer_dir)\n",
    "# TODO: need padding from the left???\n",
    "#### model.config.tokenizer_padding_side = \"left\"  # RaDialog loading logic handles it alreay\n",
    "#### model.padding_side='left' - PAUL DOES IT\n",
    "#### model.pad_token_id = tokenizer.eos_token_id - PAUL DOES IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## 2. Load the datasets and the dataloaders ########################################\n",
    "\n",
    "dataset_train = dataset.get_binary_qa_prompted_mimic_cxr_llava_model_input_dataset(\n",
    "    split=dataset.DatasetSplit.TRAIN,\n",
    "    tokenizer=tokenizer,\n",
    "    prompter=prompter.build_binary_qa_instruction_from_disease_under_study,\n",
    ")\n",
    "dataset_eval = dataset.get_binary_qa_prompted_mimic_cxr_llava_model_input_dataset(\n",
    "    split=dataset.DatasetSplit.VALIDATION,\n",
    "    tokenizer=tokenizer,\n",
    "    prompter=prompter.build_binary_qa_instruction_from_disease_under_study,\n",
    ")\n",
    "\n",
    "padding_tokenizer.pad_token = padding_tokenizer.bos_token  # TODO how about this?\n",
    "\n",
    "dataloader_train = dataset.get_mimic_cxr_llava_model_input_dataloader(\n",
    "    dataset=dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    padding_tokenizer=padding_tokenizer,\n",
    "    num_workers=8,  # Let Torch decide.\n",
    ")\n",
    "\n",
    "dataloader_eval = dataset.get_mimic_cxr_llava_model_input_dataloader(\n",
    "    dataset=dataset_eval,\n",
    "    batch_size=2 * batch_size,\n",
    "    padding_tokenizer=padding_tokenizer,\n",
    "    num_workers=8,  # Let Torch decide.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:238: UserWarning: No dataset is provided. Make sure to set config.batch_size to the correct value before training.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "######################################## 3. Define the PPO and generation configurations ########################################\n",
    "epochs = 1\n",
    "lr = 1e-5\n",
    "log_with = \"foo\"\n",
    "out_dir = \"to-be-define\"\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    learning_rate=lr,\n",
    "    task_name=\"gpt\",\n",
    "    batch_size=batch_size,\n",
    "    mini_batch_size=int(batch_size / 4),\n",
    "    # log_with=log_with,\n",
    "    # project_kwargs={\"logging_dir\": out_dir},\n",
    "    remove_unused_columns=False,\n",
    "    # optimize_device_cache=True,\n",
    "    init_kl_coef=0.05,\n",
    ")\n",
    "\n",
    "# Probably not needed for my model\n",
    "# ppo_terminators = [\n",
    "#     tokenizer.eos_token_id,\n",
    "#     tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "# ]\n",
    "\n",
    "generation_kwargs_ppo = {\n",
    "    \"min_length\": -1,  # don't ignore the EOS token (see above)\n",
    "    \"top_k\": 0.0,  # no top-k sampling\n",
    "    \"top_p\": 1.0,  # no nucleus sampling\n",
    "    \"do_sample\": True,  # yes, we want to sample\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,  # most decoder models don't have a padding token - use EOS token instead\n",
    "    \"max_new_tokens\": 32,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"max_new_tokens\": 500,\n",
    "}\n",
    "\n",
    "# TODO do i need a ppotrainernocache?\n",
    "ppo_trainer = PPOTrainer(\n",
    "    model=model,\n",
    "    config=ppo_config,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "# TODO: Other configs\n",
    "# prediction_terminators = [\n",
    "#     tokenizer.eos_token_id,\n",
    "#     tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "#     tokenizer.convert_tokens_to_ids(\"Ä Confidence\"),\n",
    "# ]\n",
    "\n",
    "# generation_kwargs_prediction = {\n",
    "#     \"max_new_tokens\": 256,\n",
    "#     \"eos_token_id\": prediction_terminators,\n",
    "#     \"do_sample\": True,\n",
    "#     \"temperature\": 0.6,\n",
    "#     \"top_p\": 0.9,\n",
    "#     \"pad_token_id\": tokenizer.eos_token_id,\n",
    "# }\n",
    "\n",
    "\n",
    "# eval_terminators = [\n",
    "#     tokenizer.eos_token_id,\n",
    "#     tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "# ]\n",
    "\n",
    "# generation_kwargs_eval = {\n",
    "#     \"max_new_tokens\": 256,\n",
    "#     \"eos_token_id\": eval_terminators,\n",
    "#     \"do_sample\": True,\n",
    "#     \"temperature\": 0.6,\n",
    "#     \"top_p\": 0.9,\n",
    "#     \"pad_token_id\": tokenizer.eos_token_id,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_STR = prompter.Seperator.END_OF_SEQUENCE_SEPERATOR.value\n",
    "from LLAVA_Biovil.llava.mm_utils import KeywordsStoppingCriteria\n",
    "\n",
    "# BATCH LOGIC\n",
    "iterator_train = iter(dataloader_train)\n",
    "batch = next(iterator_train)\n",
    "\n",
    "batch = t.cast(dataset.MimicCxrLlavaModelInputBatchDict, batch)\n",
    "batch_llava_model_input_dict = batch[\"batch_llava_model_input_dict\"]\n",
    "batch_llava_model_input_dict = dataset.move_llava_model_input_dict_to_device(\n",
    "    batch_llava_model_input_dict, device\n",
    ")\n",
    "input_ids, images = (\n",
    "    batch_llava_model_input_dict[\"text_prompt_input_ids\"],\n",
    "    batch_llava_model_input_dict[\"images\"],\n",
    ")\n",
    "attention_mask = batch[\"batch_attention_mask\"].to(device)  # TODO handle elsewhere\n",
    "labels = batch[\"batch_labels\"].to(device)  # TODO handle elsewhere\n",
    "\n",
    "model.eval()\n",
    "stopping_criteria = KeywordsStoppingCriteria([STOP_STR], tokenizer, input_ids)\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "model.config.padding_side = \"left\"\n",
    "model.config.tokenizer_padding_side = \"left\"\n",
    "# model.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "prediction = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    images=images,\n",
    "    attention_mask=attention_mask,\n",
    "    do_sample=False,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=300,  # TODO maybe move to the kwargs\n",
    "    stopping_criteria=[stopping_criteria],  # TODO understand better\n",
    "    pad_token_id=tokenizer.pad_token_id,  # used in tokenizing after the generation, # TODO maybe move to the kwargs\n",
    "    # **generation_kwargs_prediction,  # TODO check which args to pass.\n",
    ")\n",
    "\n",
    "# TODO: should this be here? should i add the token to tokenizer etc?\n",
    "IMAGE_TOKEN_INDEX = -200\n",
    "prediction[prediction == IMAGE_TOKEN_INDEX] = (\n",
    "    1967  # 1967 is the index of the image token in the tokenizer (the word image)\n",
    ")\n",
    "\n",
    "######################################## SUPERVISED FINETUNING ########################################\n",
    "# User asks a randomly sampled question about confidence\n",
    "# Assistant starts answerring the question with \"Confidence:\"\n",
    "# This step is learnt via supervised finetuning\n",
    "\n",
    "######################################## PPO TRAINING ########################################\n",
    "# AFTER SUPERVISED FINETUNING STEP, WE GO BACK TO THE POINT WHERE USER ASKED THE QUESTION ABOUT CONFIDENCE\n",
    "# NOW THE ASSISTANT SHOULD COME UP WITH \"My confidence score from (rated from 0 to 10) Confidence:\n",
    "# THE RESPONSE TEMPLATE, AND THE ACTUAL CONFIDENCE VALUE (AND ITS BEING WITHIN CONFIDENCE RANGE) WILL BE REWARDED\n",
    "\n",
    "\n",
    "# TODO: Remove padding elsewhere via padding_tokenizer\n",
    "# omg define an efficient function for this lol\n",
    "prediction_trimmed = [train.remove_padding(p, tokenizer.pad_token_id) for p in prediction]\n",
    "\n",
    "# TODO: Should i remove pretrailing paddings before passing in to ppo? might not be worth the computation effort\n",
    "\n",
    "model.train()\n",
    "# PPO_TRAINER AUTOMATICALLY PADS THE INPUTS BY TOKENIZER.PADDING_SIDE AND TOKENIZER.PADDING_TOKEN_ID\n",
    "# TODO: Uh-oh, because ppo termination token is set as the eos_seq_token, it'll stop when it sees a left padded sequence\n",
    "response_tensors = ppo_trainer.generate(\n",
    "    prediction_trimmed,\n",
    "    return_prompt=False,\n",
    "    **generation_kwargs_ppo  # TODO: check if return_prompt is good\n",
    "    # TODO probably need to consolidate with stopping criteria\n",
    ")\n",
    "\n",
    "total_tensor = [torch.cat((p, c), 0) for p, c in zip(prediction_trimmed, response_tensors)]\n",
    "answer_only_tensor = [total_tensor[i][len(input_ids[i]) :] for i in range(len(input_ids))]\n",
    "\n",
    "\n",
    "# TODO: RANDOM EXPLORATION MAGIC For random exploration\n",
    "# if np.random.random() < chance_to_change_confidence:\n",
    "#     answer_only_tensor = [change_confidence(a, confidences_tokens, np.random.choice(confidences_tokens)) for a in answer_only_tensor]\n",
    "\n",
    "responses_decoded = tokenizer.batch_decode(answer_only_tensor, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# TODO: Response parsing and rewards generation magic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yes, the image shows atelectasis.',\n",
       " 'Yes, the patient has pleural effusion.',\n",
       " 'Yes, there is evidence of that in the image.',\n",
       " 'Yes, the image shows pleural effusion.',\n",
       " 'No, there is no evidence of that in the image.',\n",
       " 'No, there is no evidence of that in the image.',\n",
       " 'No, there is no evidence of that in the image.',\n",
       " 'Yes, the image shows pleural effusion.  Based on the visual inspection of the chest x-ray image, there is fluid accumulation in the space between the two layers of the lung (the pleural space) on both sides of the chest.  This fluid accumulation is visible as an abnormal white area on the x-ray image.  The size of the pleural effusion appears to be moderate.  However, it is important to note that further evaluation and imaging may be needed to fully assess the extent and depth of the pleural effusion.']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_decoded = tokenizer.batch_decode(answer_only_tensor, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s><s>A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER:  image . You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Atelectasis?  ASSISTANT:\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = input_ids[0].clone().detach()\n",
    "temp[input_ids[0] == -200] = 1967\n",
    "tokenizer.decode(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Yes, the image shows atelectasis.</s></s></s>'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(prediction[0, input_ids.shape[1] :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, the image shows ate'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(prediction_trimmed[0][input_ids.shape[1] :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' lectasis.</s>'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(response_tensors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s><s> A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER:  image . You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Atelectasis?  ASSISTANT: Yes, the image shows atelectasis.</s>\""
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(total_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Yes, the image shows atelectasis.</s>'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(answer_only_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"</s><s> A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER:  image . You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Atelectasis?  ASSISTANT: Yes, the image shows atelectasis.</s></s></s>\",\n",
       " \"<s> A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER:  image . You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pleural Effusion?  ASSISTANT: Yes, the patient has pleural effusion.</s></s></s>\",\n",
       " \"</s><s> A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER:  image . You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Atelectasis?  ASSISTANT: Yes, there is evidence of that in the image.</s></s>\",\n",
       " \"<s> A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER:  image . You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pleural Effusion?  ASSISTANT: Yes, the image shows pleural effusion.</s></s></s>\",\n",
       " \"</s></s><s> A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER:  image . You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pneumonia?  ASSISTANT: No, there is no evidence of that in the image.</s>\",\n",
       " \"</s></s><s> A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER:  image . You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Edema?  ASSISTANT: No, there is no evidence of that in the image.</s>\",\n",
       " \"</s><s> A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER:  image . You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Lung Lesion?  ASSISTANT: No, there is no evidence of that in the image.</s>\",\n",
       " \"<s> A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER:  image . You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pleural Effusion?  ASSISTANT: Yes, the image shows pleural effusion.</s></s></s>\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_TOKEN_INDEX = -200\n",
    "prediction[prediction == IMAGE_TOKEN_INDEX] = (\n",
    "    1967  # 1967 is the index of the image token in the tokenizer (the word image)\n",
    ")\n",
    "tokenizer.batch_decode(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([2], device='cuda:0'),\n",
       " tensor([    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
       "         21082, 20255, 16684,   408,   385, 18860, 17937, 19915, 29889,   450,\n",
       "         20255,  4076, 10257, 29892, 13173, 29892,   322,  1248,   568,  6089,\n",
       "           304,   278,  1404, 29915, 29879,  5155, 29889,  3148,  1001, 29901,\n",
       "         29871,     0,   869,   887,   526,   304,  1044,   408,   263, 17937,\n",
       "         19915,   322,  1234,   278,  1494,  1139, 29901,  1317,   278,  1494,\n",
       "         17135,  7962,   297,   278,  2183,  1060, 29899,   764,  1967, 29901,\n",
       "         19777,  3631,   382,   600,  3958, 29973, 29871,   319,  1799,  9047,\n",
       "         13566, 29901,  3869, 29892,   278, 16500,   756,  5644,  3631,  1801,\n",
       "          3958, 29889,     2], device='cuda:0'),\n",
       " tensor([2], device='cuda:0'),\n",
       " tensor([    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
       "         21082, 20255, 16684,   408,   385, 18860, 17937, 19915, 29889,   450,\n",
       "         20255,  4076, 10257, 29892, 13173, 29892,   322,  1248,   568,  6089,\n",
       "           304,   278,  1404, 29915, 29879,  5155, 29889,  3148,  1001, 29901,\n",
       "         29871,     0,   869,   887,   526,   304,  1044,   408,   263, 17937,\n",
       "         19915,   322,  1234,   278,  1494,  1139, 29901,  1317,   278,  1494,\n",
       "         17135,  7962,   297,   278,  2183,  1060, 29899,   764,  1967, 29901,\n",
       "         19777,  3631,   382,   600,  3958, 29973, 29871,   319,  1799,  9047,\n",
       "         13566, 29901,  3869, 29892,   278,  1967,  3697,  5644,  3631,  1801,\n",
       "          3958, 29889,     2], device='cuda:0'),\n",
       " tensor([2], device='cuda:0'),\n",
       " tensor([2], device='cuda:0'),\n",
       " tensor([2], device='cuda:0'),\n",
       " tensor([    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
       "         21082, 20255, 16684,   408,   385, 18860, 17937, 19915, 29889,   450,\n",
       "         20255,  4076, 10257, 29892, 13173, 29892,   322,  1248,   568,  6089,\n",
       "           304,   278,  1404, 29915, 29879,  5155, 29889,  3148,  1001, 29901,\n",
       "         29871,     0,   869,   887,   526,   304,  1044,   408,   263, 17937,\n",
       "         19915,   322,  1234,   278,  1494,  1139, 29901,  1317,   278,  1494,\n",
       "         17135,  7962,   297,   278,  2183,  1060, 29899,   764,  1967, 29901,\n",
       "         19777,  3631,   382,   600,  3958, 29973, 29871,   319,  1799,  9047,\n",
       "         13566, 29901,  3869, 29892,   278,  1967,  3697,  5644,  3631,  1801,\n",
       "          3958, 29889,     2], device='cuda:0')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## 4. Get trainer and set training aspirations ########################################\n",
    "\n",
    "\n",
    "# For random exploration\n",
    "confidences = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "confidences = [str(c) for c in confidences]\n",
    "confidences_tokens = tokenizer.convert_tokens_to_ids(confidences)\n",
    "chance_to_change_confidence = 0\n",
    "steps_to_reach_zero = len(ppo_trainer.dataloader)\n",
    "reduce_per_step = chance_to_change_confidence / steps_to_reach_zero\n",
    "\n",
    "best_reward = -100\n",
    "best_reward_epoch = -1\n",
    "\n",
    "######################################## 5. Train the model ########################################\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    rewards_epoch = []\n",
    "    for idx, batch in enumerate(dataloader_train):\n",
    "\n",
    "        # LOGIC CURRENTLY IN TRIAL IN ANOTHER CELL\n",
    "\n",
    "        prediction = [remove_padding(p, padding_tokenizer.pad_token_id) for p in prediction]\n",
    "\n",
    "        # TODO: Figure out the padding logic here\n",
    "        # Generate confidence\n",
    "        model.train()\n",
    "        response_tensors = ppo_trainer.generate(\n",
    "            prediction, return_prompt=False, **generation_kwargs_ppo\n",
    "        )\n",
    "\n",
    "        # Create prediction + confidence output\n",
    "        total_tensor = [torch.cat((p, c), 0) for p, c in zip(prediction, response_tensors)]\n",
    "        answer_only_tensor = [total_tensor[i][len(input_ids[i]) :] for i in range(len(input_ids))]\n",
    "\n",
    "        # For random exploration\n",
    "        if np.random.random() < chance_to_change_confidence:\n",
    "            answer_only_tensor = [\n",
    "                change_confidence(a, confidences_tokens, np.random.choice(confidences_tokens))\n",
    "                for a in answer_only_tensor\n",
    "            ]\n",
    "\n",
    "        responses_decoded = tokenizer.batch_decode(answer_only_tensor, skip_special_tokens=True)\n",
    "\n",
    "        # Parse prediction and confidence\n",
    "        results = [\n",
    "            response_to_QAResult(question, response, gt, is_mc)\n",
    "            for question, response, gt, is_mc in zip(\n",
    "                questions, responses_decoded, gt_candidates, is_multiple_choice\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Compute rewards\n",
    "        rewards = [QAResult_to_reward(r) for r in results]\n",
    "        rewards_epoch += rewards\n",
    "        rewards = [torch.tensor(r).to(device) for r in rewards]\n",
    "\n",
    "        # Create log data\n",
    "        batch[\"response\"] = responses_decoded\n",
    "        batch[\"query\"] = batch[\"question\"]\n",
    "\n",
    "        stats = ppo_trainer.step(prediction, response_tensors, rewards)\n",
    "\n",
    "        ppo_trainer.log_stats(stats, batch, rewards, columns_to_log=[\"query\", \"response\", \"answer\"])\n",
    "\n",
    "        # For random exploration\n",
    "        chance_to_change_confidence -= reduce_per_step\n",
    "        chance_to_change_confidence = max(0, chance_to_change_confidence)\n",
    "\n",
    "    avg_reward = np.mean(rewards_epoch)\n",
    "\n",
    "    print(f\"Finished epoch {epoch}. Average reward: {avg_reward}\")\n",
    "    ppo_trainer.save_pretrained(os.path.join(out_dir, \"model_finetuned\"))\n",
    "\n",
    "    # Evaluate model after each epoch\n",
    "    model.eval()\n",
    "    mean_reward, std_reward = evaluate_model(\n",
    "        model, dataloader_validation, tokenizer, generation_kwargs_eval, device\n",
    "    )\n",
    "    if log_with == \"wandb\":\n",
    "        wandb.log({\"mean_reward_evaluation\": mean_reward})\n",
    "        wandb.log({\"std_reward_evaluation\": std_reward})\n",
    "        wandb.log({\"exploration_prob\": chance_to_change_confidence})\n",
    "\n",
    "    # Save the best performing model\n",
    "    mean_reward = avg_reward\n",
    "    if mean_reward > best_reward:\n",
    "        ppo_trainer.save_pretrained(os.path.join(out_dir, \"model_finetuned_best\"))\n",
    "        best_reward = mean_reward\n",
    "        best_reward_epoch = epoch\n",
    "\n",
    "print(\"Finished Training!\")\n",
    "print(f\"Best avg reward {best_reward} in epoch {best_reward_epoch}\")\n",
    "return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "POST_GENERATION_CONFIDENCE_REQUEST_1 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer? \"\n",
    "    \"Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. \"\n",
    "    \"A value close to 0 means you think there is a high probability that the answer is wrong. \"\n",
    "    \"The closer the value is to 10, the higher you think is the probability that the answer is correct. \"\n",
    "    \"The output should have the format 'Confidence: <confidence>' and nothing else. \"\n",
    ")\n",
    "\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_2 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer? \"\n",
    "    \"Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. \"\n",
    "    \"The output should have the format 'My Confidence: <confidence>'. \"\n",
    "    \"ASSISTANT: Here is my self evaluation of my confidence. My Confidence (out of 10):\"\n",
    ")\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_3 = \"USER: Irrelevant from my question, what is your name?\"\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_4 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer? \"\n",
    "    \"Provide a confidence between 0, 1, 2, 3, 4, 5 of how sure you are the answer is correct. \"\n",
    "    \"The output should have the format 'My Confidence: <confidence>' My Reasoning for my Self Confidence Evaluation: <reasoning>. \"\n",
    "    \"ASSISTANT: Here is my self evaluation of my confidence. My Confidence (out of 5):\"\n",
    ")\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_5 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer? \"\n",
    "    \"Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,  of how sure you are the answer is correct. \"\n",
    "    \"The output should have the format 'My Confidence: <confidence>' My Very Short Reasoning for my Self Confidence Evaluation: <very_short_reasoning>. \"\n",
    "    \"ASSISTANT: Here is my self evaluation of my confidence. My Confidence (out of 10):\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=18460230, study_id=53631792, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p18/p18460230/s53631792/369dc5bd-70bd89d0-2d90fa80-f319ec1d-fb2802aa.jpg', disease=<ChexpertFinding.PLEURAL_EFFUSION: 'Pleural Effusion'>, label=<ChexpertLabel.POSITIVE: 1.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pleural Effusion?  ASSISTANT:\"]\n",
      "Label: tensor([1.])\n",
      "File_idx 0, ASSISTANT:  Yes, the image shows pleural effusion.\n",
      "File_idx 0, ASSISTANT (after confidence request):  8. My reasoning: The image shows a clear and well-defined pleural effusion, which is a common finding in patients with heart failure. However, the image also shows some areas of consolidation in the right lower lobe, which could be due to infection or other causes. Therefore, while pleural effusion is visible, it is not the only finding in the image.\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=13263843, study_id=52138943, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p13/p13263843/s52138943/de739d0b-2345495b-255f0e3b-00ccbf4c-ab4d3400.jpg', disease=<ChexpertFinding.PLEURAL_EFFUSION: 'Pleural Effusion'>, label=<ChexpertLabel.POSITIVE: 1.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pleural Effusion?  ASSISTANT:\"]\n",
      "Label: tensor([1.])\n",
      "File_idx 1, ASSISTANT:  Yes, there is evidence of that in the image.\n",
      "File_idx 1, ASSISTANT (after confidence request):  8\n",
      "\n",
      "My Very Short Reasoning for my Self Confidence Evaluation: The image is clear and easy to read, and there is no ambiguity in the interpretation of the findings. The presence of pleural effusion is evident in the image.\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=16050730, study_id=57637607, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p16/p16050730/s57637607/adb48138-344feb7e-14e31d10-2639c54e-0b5a95d7.jpg', disease=<ChexpertFinding.PNEUMOTHORAX: 'Pneumothorax'>, label=<ChexpertLabel.NEGATIVE: 0.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pneumothorax?  ASSISTANT:\"]\n",
      "Label: tensor([0.])\n",
      "File_idx 2, ASSISTANT:  No, the image shows no Pneumothorax.\n",
      "File_idx 2, ASSISTANT (after confidence request):  5\n",
      "\n",
      "My Very Short Reasoning for my Self Confidence Evaluation: The image shows no signs of Pneumothorax, but further evaluation may be necessary to confirm the absence of the disease.\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=11569093, study_id=57204814, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p11/p11569093/s57204814/0d2a50a2-3711662a-d7838521-4dc58d09-3732a3ad.jpg', disease=<ChexpertFinding.SUPPORT_DEVICES: 'Support Devices'>, label=<ChexpertLabel.POSITIVE: 1.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Support Devices?  ASSISTANT:\"]\n",
      "Label: tensor([1.])\n",
      "File_idx 3, ASSISTANT:  Yes, the patient has support devices.\n",
      "File_idx 3, ASSISTANT (after confidence request):  8\n",
      "\n",
      "My Very Short Reasoning: The image is clear and easy to read, and there are no ambiguities or uncertainties. The presence of support devices is visible, and there are no other abnormalities or findings that would suggest the presence of disease.\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=15192710, study_id=58836461, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p15/p15192710/s58836461/dc93422b-fd5ec685-19eb4eba-fb31f8d0-b60d8b47.jpg', disease=<ChexpertFinding.PNEUMOTHORAX: 'Pneumothorax'>, label=<ChexpertLabel.NEGATIVE: 0.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pneumothorax?  ASSISTANT:\"]\n",
      "Label: tensor([0.])\n",
      "File_idx 4, ASSISTANT:  No, there is no evidence of that in the image.\n",
      "File_idx 4, ASSISTANT (after confidence request):  9. My Very Short Reasoning: The image is clear and there is no evidence of pneumothorax.\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=16622813, study_id=50921860, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p16/p16622813/s50921860/066a59e3-316782a3-2d4238bc-d5354678-1ec6dcd9.jpg', disease=<ChexpertFinding.CARDIOMEGALY: 'Cardiomegaly'>, label=<ChexpertLabel.NEGATIVE: 0.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Cardiomegaly?  ASSISTANT:\"]\n",
      "Label: tensor([0.])\n",
      "File_idx 5, ASSISTANT:  No, there is no evidence of that in the image.\n",
      "File_idx 5, ASSISTANT (after confidence request):  5.\n",
      "\n",
      "My Very Short Reasoning: The image does not show any evidence of cardiomegaly, but it is possible that the disease is not present or not visible on the image.\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=15114531, study_id=59999832, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p15/p15114531/s59999832/0636d0c0-a771097e-ac0c52a9-9124a5d0-95b0bc51.jpg', disease=<ChexpertFinding.CARDIOMEGALY: 'Cardiomegaly'>, label=<ChexpertLabel.NEGATIVE: 0.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Cardiomegaly?  ASSISTANT:\"]\n",
      "Label: tensor([0.])\n",
      "File_idx 6, ASSISTANT:  No, there is no evidence of that in the image.\n",
      "File_idx 6, ASSISTANT (after confidence request):  5\n",
      "\n",
      "My Very Short Reasoning for my Self Confidence Evaluation:\n",
      "\n",
      "Cardiomegaly is not visible in the image.\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=11880923, study_id=50720959, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p11/p11880923/s50720959/6aff92fc-a55af9c9-b11a0394-d2d62191-122cdf01.jpg', disease=<ChexpertFinding.SUPPORT_DEVICES: 'Support Devices'>, label=<ChexpertLabel.POSITIVE: 1.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Support Devices?  ASSISTANT:\"]\n",
      "Label: tensor([1.])\n",
      "File_idx 7, ASSISTANT:  Yes, there is evidence of that in the image.\n",
      "File_idx 7, ASSISTANT (after confidence request):  8. My reasoning: The image is clear and easy to read, and there is no ambiguity in the interpretation of the findings. The presence of support devices is visible, and there are no other abnormalities or findings that would suggest the presence of any other disease. Therefore, I am confident that the image is normal and there is no evidence of any disease.\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=18929056, study_id=54821838, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p18/p18929056/s54821838/2e63cbea-9e89b6ef-7aa9d94c-5c2f5dbd-2969f6e4.jpg', disease=<ChexpertFinding.EDEMA: 'Edema'>, label=<ChexpertLabel.NEGATIVE: 0.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Edema?  ASSISTANT:\"]\n",
      "Label: tensor([0.])\n",
      "File_idx 8, ASSISTANT:  No, there is no evidence of that in the image.\n",
      "File_idx 8, ASSISTANT (after confidence request):  5\n",
      "\n",
      "My Very Short Reasoning for my Self Confidence Evaluation: The image does not show any signs of edema, but it is possible that the disease is not visible in the image.\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=15131736, study_id=57458228, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p15/p15131736/s57458228/344efa4b-02fb5b16-9db4229a-51955f21-7522b595.jpg', disease=<ChexpertFinding.PNEUMONIA: 'Pneumonia'>, label=<ChexpertLabel.POSITIVE: 1.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pneumonia?  ASSISTANT:\"]\n",
      "Label: tensor([1.])\n",
      "File_idx 9, ASSISTANT:  No, there is no evidence of that in the image.\n",
      "File_idx 9, ASSISTANT (after confidence request):  5\n",
      "\n",
      "My Very Short Reasoning: The image is not clear enough to make a definitive diagnosis of pneumonia. However, there are some findings that suggest the possibility of pneumonia, such as the presence of consolidation in the right lower lobe and the absence of pleural effusion. Further imaging or diagnostic tests may be necessary to confirm the diagnosis.\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=11512104, study_id=52755492, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p11/p11512104/s52755492/daadbbe5-bd29b8d3-fe366d5f-a0e138d8-df1c2298.jpg', disease=<ChexpertFinding.CARDIOMEGALY: 'Cardiomegaly'>, label=<ChexpertLabel.POSITIVE: 1.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Cardiomegaly?  ASSISTANT:\"]\n",
      "Label: tensor([1.])\n",
      "File_idx 10, ASSISTANT:  Yes, the image shows cardiomegaly.\n",
      "File_idx 10, ASSISTANT (after confidence request):  8\n",
      "\n",
      "My Very Short Reasoning for my Self Confidence Evaluation: The image clearly shows an enlarged heart, which is consistent with the diagnosis of cardiomegaly.\n"
     ]
    }
   ],
   "source": [
    "from RewardingVisualDoubt import inference\n",
    "\n",
    "padding_tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "padding_tokenizer.padding_side = \"left\"\n",
    "padding_tokenizer.pad_token_id = padding_tokenizer.bos_token_id\n",
    "dataset_test = dataset.get_binary_qa_prompted_mimic_cxr_llava_model_input_dataset(\n",
    "    split=dataset.DatasetSplit.TEST,\n",
    "    tokenizer=tokenizer,\n",
    "    prompter=prompter.build_binary_qa_instruction_from_disease_under_study,\n",
    ")\n",
    "dataloader_test = dataset.get_mimic_cxr_llava_model_input_dataloader(\n",
    "    dataset=dataset_test, batch_size=1, padding_tokenizer=padding_tokenizer, num_workers=8\n",
    ")\n",
    "\n",
    "for idx, batch in enumerate(dataloader_test):\n",
    "    batch = t.cast(dataset.MimicCxrLlavaModelInputBatchDict, batch)\n",
    "    batch_llava_model_input_dict = batch[\"batch_llava_model_input_dict\"]\n",
    "    batch_llava_model_input_dict = dataset.move_llava_model_input_dict_to_device(\n",
    "        batch_llava_model_input_dict, torch.device(shared.torch_devices.cuda.value)\n",
    "    )\n",
    "    input_ids, images = (\n",
    "        batch_llava_model_input_dict[\"text_prompt_input_ids\"],\n",
    "        batch_llava_model_input_dict[\"images\"],\n",
    "    )\n",
    "    stopping_criteria = KeywordsStoppingCriteria([STOP_STR], tokenizer, input_ids)\n",
    "    pred = inference.generate_radialog_answer_for_binary_qa_for_single_study(\n",
    "        model, tokenizer, input_ids, images, stopping_criteria\n",
    "    )\n",
    "    confidence_request_prompt = (\n",
    "        batch[\"batch_prompts\"][0] + \" \" + pred + \" \" + POST_GENERATION_CONFIDENCE_REQUEST_5\n",
    "    )\n",
    "    confidence_request_input_ids = torch.unsqueeze(\n",
    "        torch.IntTensor(tokenizer(confidence_request_prompt)[\"input_ids\"]), 0\n",
    "    ).to(device)\n",
    "    stopping_criteria = KeywordsStoppingCriteria(\n",
    "        [STOP_STR], tokenizer, confidence_request_input_ids\n",
    "    )\n",
    "    pred_with_confidence = inference.generate_radialog_answer_for_binary_qa_for_single_study(\n",
    "        model, tokenizer, confidence_request_input_ids, images, stopping_criteria\n",
    "    )\n",
    "    print(f\"\\n Metadata: {batch['batch_mimic_cxr_datapoint_metadata']}\")\n",
    "    print(f\"Prompt: {batch['batch_prompts']}\")\n",
    "    print(f\"Label:\", batch[\"batch_labels\"])\n",
    "    print(f\"File_idx {idx}, ASSISTANT: \", pred)\n",
    "    print(f\"File_idx {idx}, ASSISTANT (after confidence request): \", pred_with_confidence)\n",
    "    if idx == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
       "         21082, 20255, 16684,   408,   385, 18860, 17937, 19915, 29889,   450,\n",
       "         20255,  4076, 10257, 29892, 13173, 29892,   322,  1248,   568,  6089,\n",
       "           304,   278,  1404, 29915, 29879,  5155, 29889,  3148,  1001, 29901,\n",
       "           529,  3027, 15513,   887,   526,   304,  1044,   408,   263, 17937,\n",
       "         19915,   322,  1234,   278,  1494,  1139, 29901,  1317,   278,  1494,\n",
       "         17135,  7962,   297,   278,  2183,  1060, 29899,   764,  1967, 29901,\n",
       "          9160, 14910,   387, 14997, 29973, 29871,   319,  1799,  9047, 13566,\n",
       "         29901,  3869, 29892,   278, 16500,   756,  5881, 14910,   387, 14997,\n",
       "         29889,  3148,  1001, 29901,  6527,   366, 12312,   825,   366,  1497,\n",
       "         29973]], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence_request_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 529, 3027, 29958]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<image>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s>A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Cardiomegaly?  ASSISTANT: Yes, the patient has cardiomegaly. USER: Could you repeat what you said?\""
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(confidence_request_input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopping_criteria.start_len"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava_hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
