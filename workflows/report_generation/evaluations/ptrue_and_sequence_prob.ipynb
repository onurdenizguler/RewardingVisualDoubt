{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23c75f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "162b43d4aba0405792916c5558e0c9f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "141a3b449a35479bb60077cb6f34f1f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 69 files:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from RewardingVisualDoubt import infrastructure\n",
    "\n",
    "infrastructure.make_ipython_reactive_to_changing_codebase()\n",
    "\n",
    "\n",
    "from RewardingVisualDoubt import (\n",
    "    dataset,\n",
    "    green,\n",
    "    evaluation,\n",
    "    training,\n",
    "    vllm,\n",
    "    prompter,\n",
    "    inference,\n",
    "    response,\n",
    "    reward,\n",
    "    shared,\n",
    ")\n",
    "\n",
    "import accelerate\n",
    "import dataclasses\n",
    "import torch\n",
    "import functools\n",
    "import pathlib as path\n",
    "import math\n",
    "import os\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import typing as t\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10e23609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding LoRA adapters to the model for SFT training or inference from Radialog Lora Weights path: /home/guests/deniz_gueler/repos/RewardingVisualDoubt/data/RaDialog_adapter_model.bin\n",
      "Loading LLaVA model with the base LLM and with RaDialog finetuned vision modules...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc28aaee6b114bceaa20df171f27c8a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 69 files:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be loaded at precision: 4bit\n",
      "Loading LLaVA from base liuhaotian/llava-v1.5-7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f4bf7eaa8ae43368a5cb9ff47c9dc24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b0aea59c844597a9592911f5f5d7b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading additional LLaVA weights...\n",
      "Merging model with vision tower weights...\n",
      "Using downloaded and verified file: /tmp/biovil_t_image_model_proj_size_128.pt\n",
      "Adding LoRA adapters to the model...\n",
      "Loading mimic_cxr_df from cache\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 24\n",
    "\n",
    "device, device_str = shared.get_device_and_device_str()\n",
    "model = vllm.shortcut_load_the_original_radialog_model()\n",
    "tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "\n",
    "model.config.padding_side = \"left\"\n",
    "\n",
    "prompter_ = prompter.build_report_generation_instruction_from_findings\n",
    "# dataset_ = dataset.get_in_distribution_report_generation_test_set(tokenizer, prompter_)\n",
    "# dataloader = dataset.get_mimic_cxr_llava_model_input_dataloader(\n",
    "#     dataset_,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     padding_tokenizer=vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "#         for_use_in_padding=True\n",
    "#     ),\n",
    "#     num_workers=8,\n",
    "# )\n",
    "\n",
    "\n",
    "dataset_ = dataset.get_report_generation_prompted_mimic_cxr_llava_model_input_dataset(\n",
    "    split=dataset.DatasetSplit.TEST, tokenizer=tokenizer, prompter=prompter_\n",
    ")\n",
    "datapoint_indexes = list(range(988))\n",
    "dataset_ = t.cast(\n",
    "    dataset.ReportGenerationPromptedMimicCxrLlavaModelInputDataset,\n",
    "    Subset(dataset_, datapoint_indexes),\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset_,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=lambda x: dataset.prompted_mimic_cxr_llava_model_input_collate_fn(\n",
    "        x, vllm.load_pretrained_llava_tokenizer_with_image_support(for_use_in_padding=True)\n",
    "    ),\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "\n",
    "BASE_JSON_DIR = \"/home/guests/deniz_gueler/repos/RewardingVisualDoubt/workflows/report_generation/evaluations/results/test_ood/vanilla_verbalize_ood.json\"\n",
    "records = dataset.read_records_from_json_file(BASE_JSON_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1800c1",
   "metadata": {},
   "source": [
    "# SEQUENCE PROBABILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acc8ab31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/42 [00:00<?, ?it/s]/home/guests/deniz_gueler/miniconda3/envs/llava_hf2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 42/42 [08:44<00:00, 12.49s/it]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(tqdm(dataloader)):\n",
    "    batch = t.cast(dataset.MimicCxrLlavaModelInputBatchDict, batch)\n",
    "    input_ids, images, attention_mask, batch_metadata_list = (\n",
    "        dataset.typical_unpacking_for_report_generation(device, batch)\n",
    "    )\n",
    "    start_idx = i * BATCH_SIZE\n",
    "    end_idx = min((i + 1) * BATCH_SIZE, len(records))\n",
    "    batch_records = records[start_idx:end_idx]\n",
    "    confidence_stripped_reports = training.remove_confidence_part_from_generated_responses(\n",
    "        [record[\"generated_report\"] for record in batch_records]\n",
    "    )\n",
    "    generated_reports_input_ids = tokenizer(confidence_stripped_reports).input_ids\n",
    "\n",
    "    concatenated_sequences = []\n",
    "    generation_lenghts = []\n",
    "    for original_input_ids, generated_input_ids in zip(input_ids, generated_reports_input_ids):\n",
    "        concatenated = torch.cat(\n",
    "            [original_input_ids, torch.tensor(generated_input_ids, dtype=torch.long).to(device)[1:]]\n",
    "        )\n",
    "        sequence_start = (concatenated == 1).nonzero()[0]\n",
    "        concatenated = concatenated[sequence_start:]\n",
    "        concatenated_sequences.append(concatenated)\n",
    "        generation_lenghts.append(len(generated_input_ids) - 1)\n",
    "\n",
    "    final_input_ids, final_attention_masks = dataset.pad_batch_text_sequences(\n",
    "        concatenated_sequences,\n",
    "        padding_tokenizer=vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "            for_use_in_padding=True\n",
    "        ),\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=final_input_ids,\n",
    "            images=images,\n",
    "            attention_mask=final_attention_masks,\n",
    "        )\n",
    "    logits = outputs.logits\n",
    "    collected_mean_probs = []\n",
    "    for logit, gen_len, gen_input_ids in zip(\n",
    "        logits, generation_lenghts, generated_reports_input_ids\n",
    "    ):\n",
    "        logit = logit[-(gen_len + 1) : -1]\n",
    "        token_probs = torch.softmax(logit, dim=-1)\n",
    "        all_rows = torch.arange(token_probs.size(0))\n",
    "        mean_prob = token_probs[all_rows, gen_input_ids[1:]].mean()\n",
    "        collected_mean_probs.append(mean_prob)\n",
    "    for idx, mean_prob in enumerate(collected_mean_probs):\n",
    "        batch_records[idx][\"confidence\"] = round(mean_prob.item() * 10)\n",
    "\n",
    "    json_path = \"sequence_probability_ood.json\"\n",
    "    dataset.append_records_to_json_file(batch_records, json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683eaef3",
   "metadata": {},
   "source": [
    "# P(TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e964482",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [09:06<00:00, 13.02s/it]\n"
     ]
    }
   ],
   "source": [
    "FOLLOWUP_PROMPT = (\n",
    "    \" USER: Was your previous answer correctt? Answer with a single word: Yes or No. ASSISTANT:\"\n",
    ")\n",
    "\n",
    "for i, batch in enumerate(tqdm(dataloader)):\n",
    "    batch = t.cast(dataset.MimicCxrLlavaModelInputBatchDict, batch)\n",
    "    input_ids, images, attention_mask, batch_metadata_list = (\n",
    "        dataset.typical_unpacking_for_report_generation(device, batch)\n",
    "    )\n",
    "    start_idx = i * BATCH_SIZE\n",
    "    end_idx = min((i + 1) * BATCH_SIZE, len(records))\n",
    "    batch_records = records[start_idx:end_idx]\n",
    "    confidence_stripped_reports = training.remove_confidence_part_from_generated_responses(\n",
    "        [record[\"generated_report\"] for record in batch_records]\n",
    "    )\n",
    "    generated_reports_input_ids = tokenizer(confidence_stripped_reports).input_ids\n",
    "\n",
    "    concatenated_sequences = []\n",
    "    for original_input_ids, generated_input_ids in zip(input_ids, generated_reports_input_ids):\n",
    "        concatenated = torch.cat(\n",
    "            [original_input_ids, torch.tensor(generated_input_ids, dtype=torch.long).to(device)[1:]]\n",
    "        )\n",
    "        sequence_start = (concatenated == 1).nonzero()[0]\n",
    "        concatenated = concatenated[sequence_start:]\n",
    "        concatenated = torch.cat(\n",
    "            [\n",
    "                concatenated,\n",
    "                torch.tensor(tokenizer(FOLLOWUP_PROMPT).input_ids, dtype=torch.long).to(device),\n",
    "            ]\n",
    "        )\n",
    "        concatenated_sequences.append(concatenated)\n",
    "\n",
    "    final_input_ids, final_attention_masks = dataset.pad_batch_text_sequences(\n",
    "        concatenated_sequences,\n",
    "        padding_tokenizer=vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "            for_use_in_padding=True\n",
    "        ),\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=final_input_ids,\n",
    "            images=images,\n",
    "            attention_mask=final_attention_masks,\n",
    "        )\n",
    "    logits = outputs.logits\n",
    "    normalized_yes, normalized_no = evaluation.extract_yes_no_probs_from_logits(logits)\n",
    "    for idx, norm_yes in enumerate(normalized_yes):\n",
    "        batch_records[idx][\"confidence\"] = round(norm_yes * 10)\n",
    "    json_path = \"ptrue_ood.json\"\n",
    "    dataset.append_records_to_json_file(batch_records, json_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava_hf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
