{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c75f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from RewardingVisualDoubt import infrastructure\n",
    "\n",
    "infrastructure.make_ipython_reactive_to_changing_codebase()\n",
    "\n",
    "\n",
    "from RewardingVisualDoubt import (\n",
    "    dataset,\n",
    "    green,\n",
    "    evaluation,\n",
    "    training,\n",
    "    vllm,\n",
    "    prompter,\n",
    "    inference,\n",
    "    response,\n",
    "    reward,\n",
    "    shared,\n",
    ")\n",
    "\n",
    "import accelerate\n",
    "import dataclasses\n",
    "import torch\n",
    "import functools\n",
    "import pathlib as path\n",
    "import math\n",
    "import os\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import typing as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e23609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding LoRA adapters to the model for SFT training or inference from Radialog Lora Weights path: /home/guests/deniz_gueler/repos/RewardingVisualDoubt/data/RaDialog_adapter_model.bin\n",
      "Loading LLaVA model with the base LLM and with RaDialog finetuned vision modules...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f62adc60c1b47eabfd5469b7b2a75e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 69 files:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be loaded at precision: 4bit\n",
      "Loading LLaVA from base liuhaotian/llava-v1.5-7b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64614b734c274492bfe8ada326caa413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00079350cc54445799c9e94e55f2cc0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading additional LLaVA weights...\n",
      "Merging model with vision tower weights...\n",
      "Using downloaded and verified file: /tmp/biovil_t_image_model_proj_size_128.pt\n",
      "Adding LoRA adapters to the model...\n",
      "Loading mimic_cxr_df from cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 24\n",
    "\n",
    "device, device_str = shared.get_device_and_device_str()\n",
    "model = vllm.shortcut_load_the_original_radialog_model()\n",
    "tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "\n",
    "model.config.padding_side = \"left\"\n",
    "\n",
    "prompter_ = prompter.build_report_generation_instruction_from_findings\n",
    "dataset_ = dataset.get_in_distribution_report_generation_test_set(tokenizer, prompter_)\n",
    "dataloader = dataset.get_mimic_cxr_llava_model_input_dataloader(\n",
    "    dataset_,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    padding_tokenizer=vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "        for_use_in_padding=True\n",
    "    ),\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "BASE_JSON_DIR = \"/home/guests/deniz_gueler/repos/RewardingVisualDoubt/workflows/report_generation/evaluations/results/testset_in_dist/vanilla_verbalize.json\"\n",
    "records = dataset.read_records_from_json_file(BASE_JSON_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1800c1",
   "metadata": {},
   "source": [
    "# SEQUENCE PROBABILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc8ab31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/46 [00:00<?, ?it/s]/home/guests/deniz_gueler/miniconda3/envs/llava_hf2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 46/46 [09:27<00:00, 12.34s/it]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(tqdm(dataloader)):\n",
    "    batch = t.cast(dataset.MimicCxrLlavaModelInputBatchDict, batch)\n",
    "    input_ids, images, attention_mask, batch_metadata_list = (\n",
    "        dataset.typical_unpacking_for_report_generation(device, batch)\n",
    "    )\n",
    "    start_idx = i * BATCH_SIZE\n",
    "    end_idx = min((i + 1) * BATCH_SIZE, len(records))\n",
    "    batch_records = records[start_idx:end_idx]\n",
    "    confidence_stripped_reports = training.remove_confidence_part_from_generated_responses(\n",
    "        [record[\"generated_report\"] for record in batch_records]\n",
    "    )\n",
    "    generated_reports_input_ids = tokenizer(confidence_stripped_reports).input_ids\n",
    "\n",
    "    concatenated_sequences = []\n",
    "    generation_lenghts = []\n",
    "    for original_input_ids, generated_input_ids in zip(input_ids, generated_reports_input_ids):\n",
    "        concatenated = torch.cat(\n",
    "            [original_input_ids, torch.tensor(generated_input_ids, dtype=torch.long).to(device)[1:]]\n",
    "        )\n",
    "        sequence_start = (concatenated == 1).nonzero()[0]\n",
    "        concatenated = concatenated[sequence_start:]\n",
    "        concatenated_sequences.append(concatenated)\n",
    "        generation_lenghts.append(len(generated_input_ids) - 1)\n",
    "\n",
    "    final_input_ids, final_attention_masks = dataset.pad_batch_text_sequences(\n",
    "        concatenated_sequences,\n",
    "        padding_tokenizer=vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "            for_use_in_padding=True\n",
    "        ),\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=final_input_ids,\n",
    "            images=images,\n",
    "            attention_mask=final_attention_masks,\n",
    "        )\n",
    "    logits = outputs.logits\n",
    "    collected_mean_probs = []\n",
    "    for logit, gen_len, gen_input_ids in zip(\n",
    "        logits, generation_lenghts, generated_reports_input_ids\n",
    "    ):\n",
    "        logit = logit[-(gen_len + 1) : -1]\n",
    "        token_probs = torch.softmax(logit, dim=-1)\n",
    "        all_rows = torch.arange(token_probs.size(0))\n",
    "        mean_prob = token_probs[all_rows, gen_input_ids[1:]].mean()\n",
    "        collected_mean_probs.append(mean_prob)\n",
    "    for idx, mean_prob in enumerate(collected_mean_probs):\n",
    "        batch_records[idx][\"confidence\"] = round(mean_prob.item() * 10)\n",
    "\n",
    "    json_path = \"sequence_probability.json\"\n",
    "    dataset.append_records_to_json_file(batch_records, json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683eaef3",
   "metadata": {},
   "source": [
    "# P(TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "7e964482",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/46 [00:00<?, ?it/s]/home/guests/deniz_gueler/miniconda3/envs/llava_hf2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 46/46 [09:56<00:00, 12.97s/it]\n"
     ]
    }
   ],
   "source": [
    "FOLLOWUP_PROMPT = (\n",
    "    \" USER: Was your previous answer correctt? Answer with a single word: Yes or No. ASSISTANT:\"\n",
    ")\n",
    "\n",
    "for i, batch in enumerate(tqdm(dataloader)):\n",
    "    batch = t.cast(dataset.MimicCxrLlavaModelInputBatchDict, batch)\n",
    "    input_ids, images, attention_mask, batch_metadata_list = (\n",
    "        dataset.typical_unpacking_for_report_generation(device, batch)\n",
    "    )\n",
    "    start_idx = i * BATCH_SIZE\n",
    "    end_idx = min((i + 1) * BATCH_SIZE, len(records))\n",
    "    batch_records = records[start_idx:end_idx]\n",
    "    confidence_stripped_reports = training.remove_confidence_part_from_generated_responses(\n",
    "        [record[\"generated_report\"] for record in batch_records]\n",
    "    )\n",
    "    generated_reports_input_ids = tokenizer(confidence_stripped_reports).input_ids\n",
    "\n",
    "    concatenated_sequences = []\n",
    "    for original_input_ids, generated_input_ids in zip(input_ids, generated_reports_input_ids):\n",
    "        concatenated = torch.cat(\n",
    "            [original_input_ids, torch.tensor(generated_input_ids, dtype=torch.long).to(device)[1:]]\n",
    "        )\n",
    "        sequence_start = (concatenated == 1).nonzero()[0]\n",
    "        concatenated = concatenated[sequence_start:]\n",
    "        concatenated = torch.cat(\n",
    "            [\n",
    "                concatenated,\n",
    "                torch.tensor(tokenizer(FOLLOWUP_PROMPT).input_ids, dtype=torch.long).to(device),\n",
    "            ]\n",
    "        )\n",
    "        concatenated_sequences.append(concatenated)\n",
    "\n",
    "    final_input_ids, final_attention_masks = dataset.pad_batch_text_sequences(\n",
    "        concatenated_sequences,\n",
    "        padding_tokenizer=vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "            for_use_in_padding=True\n",
    "        ),\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=final_input_ids,\n",
    "            images=images,\n",
    "            attention_mask=final_attention_masks,\n",
    "        )\n",
    "    logits = outputs.logits\n",
    "    normalized_yes, normalized_no = evaluation.extract_yes_no_probs_from_logits(logits)\n",
    "    for idx, norm_yes in enumerate(normalized_yes):\n",
    "        batch_records[idx][\"confidence\"] = round(norm_yes * 10)\n",
    "    json_path = \"ptrue.json\"\n",
    "    dataset.append_records_to_json_file(batch_records, json_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava_hf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
