{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9d87694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983caaf00e0b4eceb2f868ea83d6565b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef9a9e74bbc5430c9c9f45d865bd8bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 69 files:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from RewardingVisualDoubt import infrastructure\n",
    "\n",
    "infrastructure.make_ipython_reactive_to_changing_codebase()\n",
    "\n",
    "\n",
    "from RewardingVisualDoubt import (\n",
    "    dataset,\n",
    "    green,\n",
    "    evaluation,\n",
    "    training,\n",
    "    vllm,\n",
    "    prompter,\n",
    "    inference,\n",
    "    response,\n",
    "    reward,\n",
    "    shared,\n",
    ")\n",
    "\n",
    "import accelerate\n",
    "import dataclasses\n",
    "import torch\n",
    "import functools\n",
    "import pathlib as path\n",
    "import math\n",
    "import os\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "import typing as t\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0139cb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# records = dataset.read_records_from_json_file(\n",
    "#     \"/home/guests/deniz_gueler/repos/StudyingVisualDoubt/data/inference/report_generation/difficulty_balanced_training_set/original_radialog_model/original_radialog_model_green_scores.json\"\n",
    "# )\n",
    "\n",
    "records = dataset.read_records_from_json_file(\n",
    "    \"/home/guests/deniz_gueler/repos/StudyingVisualDoubt/data/inference/report_generation/testset_2040_randomly_sampled/base_sft_model/generated_reports_with_confidence_and_green_score.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efc22f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding LoRA adapters to the model for SFT training or inference from Radialog Lora Weights path: /home/guests/deniz_gueler/repos/RewardingVisualDoubt/data/RaDialog_adapter_model.bin\n",
      "Loading LLaVA model with the base LLM and with RaDialog finetuned vision modules...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84ec6100efc44ba598aa495fd83ced15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 69 files:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be loaded at precision: 4bit\n",
      "Loading LLaVA from base liuhaotian/llava-v1.5-7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0edb08030565419eaafe4796e8faaec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab9f221daad475485d0f4872aebdfd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading additional LLaVA weights...\n",
      "Merging model with vision tower weights...\n",
      "Using downloaded and verified file: /tmp/biovil_t_image_model_proj_size_128.pt\n",
      "Adding LoRA adapters to the model...\n",
      "Loading mimic_cxr_df from cache\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 24\n",
    "device, device_str = shared.get_device_and_device_str()\n",
    "model = vllm.shortcut_load_the_original_radialog_model()\n",
    "tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "selected_prompter_fn = prompter.build_report_generation_instruction_from_findings\n",
    "# dataset_ = dataset.get_report_generation_prompted_mimic_cxr_llava_model_input_dataset(\n",
    "#     split=dataset.DatasetSplit.TRAIN, tokenizer=tokenizer, prompter=selected_prompter_fn\n",
    "# )\n",
    "# dataset_ = dataset.get_report_generation_prompted_mimic_cxr_llava_model_input_dataset(\n",
    "#     split=dataset.DatasetSplit.VALIDATION, tokenizer=tokenizer, prompter=selected_prompter_fn\n",
    "# )\n",
    "dataset_ = dataset.get_report_generation_prompted_mimic_cxr_llava_model_input_dataset(\n",
    "    split=dataset.DatasetSplit.TEST, tokenizer=tokenizer, prompter=selected_prompter_fn\n",
    ")\n",
    "# selected_datapoints_json = \"/home/guests/deniz_gueler/repos/RewardingVisualDoubt/workflows/report_generation/selected_datapoints/1476_training_datapoints_balanced_difficulty.json\"\n",
    "# selected_datapoints_json = \"/home/guests/deniz_gueler/repos/RewardingVisualDoubt/workflows/report_generation/selected_datapoints/988_test_datapoints_balanced_difficulty_sampled_from_validation_set_idx505-2120.json\"\n",
    "# with open(selected_datapoints_json, \"r\") as f:\n",
    "#     datapoint_indexes: list[int] = json.load(f)\n",
    "\n",
    "datapoint_indexes = list(range(988))\n",
    "dataset_ = t.cast(\n",
    "    dataset.ReportGenerationPromptedMimicCxrLlavaModelInputDataset,\n",
    "    Subset(dataset_, datapoint_indexes),\n",
    ")\n",
    "model.config.padding_side = \"left\"\n",
    "\n",
    "# dataloader = dataset.get_mimic_cxr_llava_model_input_dataloader(\n",
    "#     dataset_,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     padding_tokenizer=vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "#         for_use_in_padding=True\n",
    "#     ),\n",
    "#     num_workers=8,\n",
    "# )\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset_,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=lambda x: dataset.prompted_mimic_cxr_llava_model_input_collate_fn(\n",
    "        x, vllm.load_pretrained_llava_tokenizer_with_image_support(for_use_in_padding=True)\n",
    "    ),\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    "    persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7b8a71",
   "metadata": {},
   "source": [
    "# Hidden state collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a217ff49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/42 [00:00<?, ?it/s]/home/guests/deniz_gueler/miniconda3/envs/llava_hf2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 42/42 [10:17<00:00, 14.70s/it]\n"
     ]
    }
   ],
   "source": [
    "SELECTED_HIDDEN_LAYER_IDX = 16\n",
    "final_stats = []\n",
    "for i, batch in enumerate(tqdm(dataloader)):\n",
    "    batch = t.cast(dataset.MimicCxrLlavaModelInputBatchDict, batch)\n",
    "    input_ids, images, attention_mask, batch_metadata_list = (\n",
    "        dataset.typical_unpacking_for_report_generation(device, batch)\n",
    "    )\n",
    "    start_idx = i * BATCH_SIZE\n",
    "    end_idx = min((i + 1) * BATCH_SIZE, len(records))\n",
    "    batch_records = records[start_idx:end_idx]\n",
    "    confidence_stripped_reports = training.remove_confidence_part_from_generated_responses(\n",
    "        [record[\"generated_report\"] for record in batch_records]\n",
    "    )\n",
    "    generated_reports_input_ids = tokenizer(confidence_stripped_reports).input_ids\n",
    "\n",
    "    concatenated_sequences = []\n",
    "    generation_lenghts = []\n",
    "    for original_input_ids, generated_input_ids in zip(input_ids, generated_reports_input_ids):\n",
    "        concatenated = torch.cat(\n",
    "            [original_input_ids, torch.tensor(generated_input_ids, dtype=torch.long).to(device)[1:]]\n",
    "        )\n",
    "        sequence_start = (concatenated == 1).nonzero()[0]\n",
    "        concatenated = concatenated[sequence_start:]\n",
    "        concatenated_sequences.append(concatenated)\n",
    "        generation_lenghts.append(len(generated_input_ids) - 1)\n",
    "\n",
    "    final_input_ids, final_attention_masks = dataset.pad_batch_text_sequences(\n",
    "        concatenated_sequences,\n",
    "        padding_tokenizer=vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "            for_use_in_padding=True\n",
    "        ),\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=final_input_ids,\n",
    "            images=images,\n",
    "            attention_mask=final_attention_masks,\n",
    "            return_dict=True,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "\n",
    "    hidden_states = outputs[\"hidden_states\"][SELECTED_HIDDEN_LAYER_IDX]\n",
    "    collected_hidden_states = []\n",
    "    for state in hidden_states:\n",
    "        collected_hidden_states.append(state[-1, :].view(-1).cpu().float().detach().numpy())\n",
    "\n",
    "    stats = [\n",
    "        {\"hidden\": hidden_state, \"target\": float(batch_record[\"green_score\"])}\n",
    "        for hidden_state, batch_record in zip(collected_hidden_states, batch_records)\n",
    "    ]\n",
    "    final_stats.extend(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e96bdeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(final_stats, \"hidden_states_and_green_scores_layer_16_test_ood.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e662f7a",
   "metadata": {},
   "source": [
    "# Training the probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ea16b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class MLP2(nn.Module):\n",
    "    \"\"\"\n",
    "    Multilayer Perceptron.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        return self.layers(x)\n",
    "\n",
    "    def score(self, o, hidden_layer_idx):\n",
    "        return torch.sigmoid(\n",
    "            self.layers(\n",
    "                o.hidden_states[hidden_layer_idx][:, -1, :]\n",
    "                .to(self.layers[0].weight.dtype)\n",
    "                .to(self.layers[0].weight.device)\n",
    "            ).squeeze(1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba30e475",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_PATH = \"hidden_states_and_green_scores_layer_16_training.pt\"\n",
    "\n",
    "# torch.manual_seed(seed)\n",
    "# Path(out_name).parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "data = torch.load(HIDDEN_PATH, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8647398",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_PATH = \"hidden_states_and_green_scores_layer_16_training.pt\"\n",
    "\n",
    "# torch.manual_seed(seed)\n",
    "# Path(out_name).parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "data = torch.load(HIDDEN_PATH, map_location=\"cpu\")\n",
    "dataset = Dataset.from_pandas(pd.DataFrame(data=data))\n",
    "trainloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "# Initialize the MLP\n",
    "mlp = MLP2(hidden_size=len(dataset[0][\"hidden\"]))\n",
    "\n",
    "mlp = mlp.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "44ab7fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Taking training steps... Epoch 1/5: 23it [00:27,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch     0: 0.000\n",
      "accuracy:  0.21988248564506488\n",
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Taking training steps... Epoch 2/5: 23it [00:27,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch     1: 0.000\n",
      "accuracy:  0.17122304113836237\n",
      "Starting epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Taking training steps... Epoch 3/5: 23it [00:26,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch     2: 0.000\n",
      "accuracy:  0.15479655865111638\n",
      "Starting epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Taking training steps... Epoch 4/5: 23it [00:27,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch     3: 0.000\n",
      "accuracy:  0.14891273206700392\n",
      "Starting epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Taking training steps... Epoch 5/5: 23it [00:26,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch     4: 0.000\n",
      "accuracy:  0.14530304872273095\n",
      "Training process has finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "out_name = \"trained_probes/trained_probe_layer16_balanced_difficulty.pt\"\n",
    "# Run the training loop\n",
    "for epoch in range(0, 5):  # 5 epochs at maximum\n",
    "    # Print epoch\n",
    "    print(f\"Starting epoch {epoch}\")\n",
    "\n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "    collected_outputs = []\n",
    "    collected_targets = []\n",
    "\n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in tqdm(\n",
    "        enumerate(trainloader),\n",
    "        desc=f\"Taking training steps... Epoch {epoch+1}/{5}\",\n",
    "    ):\n",
    "        # Get inputs\n",
    "        inputs, targets = (\n",
    "            torch.stack(data[\"hidden\"], dim=1).type(torch.FloatTensor),\n",
    "            data[\"target\"],\n",
    "        )\n",
    "        # Prepare targets\n",
    "        if not isinstance(targets, torch.Tensor):\n",
    "            targets = torch.tensor(targets)\n",
    "        targets = targets.type(torch.FloatTensor).reshape((-1, 1))\n",
    "        collected_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "        outputs = mlp(inputs)\n",
    "        collected_outputs.extend(outputs.cpu().detach())\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "\n",
    "    concat_collected_outputs = torch.concat(collected_outputs).view(-1)\n",
    "    concat_collected_targets = torch.tensor(collected_targets).view(-1)\n",
    "    collected_outputs = []\n",
    "    collected_targets = []\n",
    "\n",
    "    print(\"Loss after epoch %5d: %.3f\" % (epoch, current_loss / len(trainloader)))\n",
    "    print(\n",
    "        \"accuracy: \",\n",
    "        (\n",
    "            torch.abs(\n",
    "                torch.nn.functional.sigmoid(concat_collected_outputs) - concat_collected_targets\n",
    "            )\n",
    "            .sum()\n",
    "            .item()\n",
    "        )\n",
    "        / len(concat_collected_outputs),\n",
    "    )\n",
    "    # print(\"mlp.layers[0].weight.sum(): \", mlp.layers[0].weight.sum())\n",
    "    current_loss = 0.0\n",
    "\n",
    "    # save model\n",
    "    out_epoch_name = f\"{out_name}_epoch{epoch}\"\n",
    "    torch.save(mlp, out_epoch_name)\n",
    "\n",
    "# Process is complete.\n",
    "print(\"Training process has finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab408d4f",
   "metadata": {},
   "source": [
    "# Use the trained probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085b6ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing... (total steps: 16): 16it [00:18,  1.18s/it]\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_PATH_TEST = \"hidden_states_and_green_scores_layer_16_test_ood.pt\"\n",
    "\n",
    "RESULTS_FILE_NAME = \"trained_probe_results_ood.json\"\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class MLP2(nn.Module):\n",
    "    \"\"\"\n",
    "    Multilayer Perceptron.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        return self.layers(x)\n",
    "\n",
    "    def score(self, o, hidden_layer_idx):\n",
    "        return torch.sigmoid(\n",
    "            self.layers(\n",
    "                o.hidden_states[hidden_layer_idx][:, -1, :]\n",
    "                .to(self.layers[0].weight.dtype)\n",
    "                .to(self.layers[0].weight.device)\n",
    "            ).squeeze(1)\n",
    "        )\n",
    "\n",
    "\n",
    "mlp = torch.load(\n",
    "    \"/home/guests/deniz_gueler/repos/RewardingVisualDoubt/workflows/report_generation/evaluations/trained_probes/trained_probe_layer16_balanced_difficulty.pt_epoch3\"\n",
    ")\n",
    "data_test = torch.load(HIDDEN_PATH_TEST, map_location=\"cpu\")\n",
    "dataset_test = Dataset.from_pandas(pd.DataFrame(data=data_test))\n",
    "testloader = DataLoader(dataset_test, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "device = \"cuda\"\n",
    "collected_outputs = []\n",
    "collected_targets = []\n",
    "\n",
    "for i, data_ in tqdm(\n",
    "    enumerate(testloader),\n",
    "    desc=f\"Testing... (total steps: {len(testloader)})\",\n",
    "):\n",
    "\n",
    "    # Get inputs\n",
    "    inputs, targets = (\n",
    "        torch.stack(data_[\"hidden\"], dim=1).type(torch.FloatTensor),\n",
    "        data_[\"target\"],\n",
    "    )\n",
    "    # Prepare targets\n",
    "    if not isinstance(targets, torch.Tensor):\n",
    "        targets = torch.tensor(targets)\n",
    "    targets = targets.type(torch.FloatTensor).reshape((-1, 1))\n",
    "    collected_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    outputs = mlp(inputs)\n",
    "    collected_outputs.extend(outputs.cpu().detach())\n",
    "\n",
    "concat_collected_outputs = torch.concat(collected_outputs).view(-1)\n",
    "concat_collected_targets = torch.tensor(collected_targets).view(-1)\n",
    "\n",
    "\n",
    "for record, current_output, current_target in zip(\n",
    "    records, concat_collected_outputs.tolist(), concat_collected_targets.tolist()\n",
    "):\n",
    "    assert round(current_target, 2) == round(record[\"green_score\"], 2)\n",
    "    record[\"confidence\"] = round(\n",
    "        torch.nn.functional.sigmoid(torch.tensor(current_output)).item() * 10\n",
    "    )\n",
    "    dataset.append_records_to_json_file([record], RESULTS_FILE_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava_hf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
