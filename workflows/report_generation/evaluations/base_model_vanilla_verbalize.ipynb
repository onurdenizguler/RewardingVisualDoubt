{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01d0ddb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85cc095d5ba843cebbc81275eb015d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0be237365434f468fbe874e8b22a4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 69 files:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from RewardingVisualDoubt import infrastructure\n",
    "\n",
    "infrastructure.make_ipython_reactive_to_changing_codebase()\n",
    "\n",
    "\n",
    "from RewardingVisualDoubt import (\n",
    "    dataset,\n",
    "    green,\n",
    "    evaluation,\n",
    "    training,\n",
    "    vllm,\n",
    "    prompter,\n",
    "    inference,\n",
    "    response,\n",
    "    reward,\n",
    "    shared,\n",
    ")\n",
    "\n",
    "import accelerate\n",
    "import dataclasses\n",
    "import torch\n",
    "import functools\n",
    "import pathlib as path\n",
    "import math\n",
    "import os\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import typing as t\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad43c253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding LoRA adapters to the model for SFT training or inference from Radialog Lora Weights path: /home/guests/deniz_gueler/repos/RewardingVisualDoubt/data/RaDialog_adapter_model.bin\n",
      "Loading LLaVA model with the base LLM and with RaDialog finetuned vision modules...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512d5bbcca4c495fbe3a1beebe93dd3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 69 files:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be loaded at precision: 4bit\n",
      "Loading LLaVA from base liuhaotian/llava-v1.5-7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d9c6889faf24ad7838d05d486dfd14d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "947abfb3d87e4fc6b89756f9ae83cf22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading additional LLaVA weights...\n",
      "Merging model with vision tower weights...\n",
      "Using downloaded and verified file: /tmp/biovil_t_image_model_proj_size_128.pt\n",
      "Adding LoRA adapters to the model...\n",
      "Loading mimic_cxr_df from cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 12\n",
    "\n",
    "device, device_str = shared.get_device_and_device_str()\n",
    "model = vllm.shortcut_load_the_original_radialog_model()\n",
    "tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "\n",
    "model.config.padding_side = \"left\"\n",
    "\n",
    "prompter_ = prompter.build_report_generation_instruction_from_findings\n",
    "dataset_ = dataset.get_in_distribution_report_generation_test_set(tokenizer, prompter_)\n",
    "dataloader = dataset.get_mimic_cxr_llava_model_input_dataloader(\n",
    "    dataset_,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    padding_tokenizer=vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "        for_use_in_padding=True\n",
    "    ),\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "\n",
    "# dataset_ = dataset.get_report_generation_prompted_mimic_cxr_llava_model_input_dataset(\n",
    "#     split=dataset.DatasetSplit.TEST, tokenizer=tokenizer, prompter=prompter_\n",
    "# )\n",
    "# datapoint_indexes = list(range(988))\n",
    "# dataset_ = t.cast(\n",
    "#     dataset.ReportGenerationPromptedMimicCxrLlavaModelInputDataset,\n",
    "#     Subset(dataset_, datapoint_indexes),\n",
    "# )\n",
    "# dataloader = DataLoader(\n",
    "#     dataset=dataset_,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     collate_fn=lambda x: dataset.prompted_mimic_cxr_llava_model_input_collate_fn(\n",
    "#         x, vllm.load_pretrained_llava_tokenizer_with_image_support(for_use_in_padding=True)\n",
    "#     ),\n",
    "#     shuffle=False,\n",
    "#     num_workers=8,\n",
    "#     pin_memory=True,\n",
    "#     drop_last=False,\n",
    "#     persistent_workers=True,\n",
    "# )\n",
    "\n",
    "\n",
    "BASE_JSON_DIR = \"/home/guests/deniz_gueler/repos/StudyingVisualDoubt/data/inference/report_generation/testset_difficulty_balanced_from_validation_set_idx_505-2129/base_sft_model/generated_reports_with_confidence_and_green_score.json\"\n",
    "records = dataset.read_records_from_json_file(BASE_JSON_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1586a99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [14:14<00:00,  9.29s/it]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(tqdm(dataloader)):\n",
    "    batch = t.cast(dataset.MimicCxrLlavaModelInputBatchDict, batch)\n",
    "    input_ids, images, stopping_criteria, attention_mask, batch_metadata_list = (\n",
    "        dataset.unpack_report_generation_batch_with_attention_mask_and_metadata(\n",
    "            device, tokenizer, batch\n",
    "        )\n",
    "    )\n",
    "    start_idx = i * BATCH_SIZE\n",
    "    end_idx = min((i + 1) * BATCH_SIZE, len(records))\n",
    "    batch_records = records[start_idx:end_idx]\n",
    "    confidence_stripped_reports = training.remove_confidence_part_from_generated_responses(\n",
    "        [record[\"generated_report\"] for record in batch_records]\n",
    "    )\n",
    "    confidence_stripped_reports_with_post_conf_req = [\n",
    "        report + \" \" + prompter.build_post_generation_user_confidence_request()\n",
    "        for report in confidence_stripped_reports\n",
    "    ]\n",
    "    generated_reports_input_ids = tokenizer(\n",
    "        confidence_stripped_reports_with_post_conf_req\n",
    "    ).input_ids\n",
    "\n",
    "    concatenated_sequences = []\n",
    "    generation_lenghts = []\n",
    "    for original_input_ids, generated_input_ids in zip(input_ids, generated_reports_input_ids):\n",
    "        concatenated = torch.cat(\n",
    "            [original_input_ids, torch.tensor(generated_input_ids, dtype=torch.long).to(device)[1:]]\n",
    "        )\n",
    "        sequence_start = (concatenated == 1).nonzero()[0]\n",
    "        concatenated = concatenated[sequence_start:]\n",
    "        concatenated_sequences.append(concatenated)\n",
    "        generation_lenghts.append(len(generated_input_ids) - 1)\n",
    "\n",
    "    final_input_ids, final_attention_masks = dataset.pad_batch_text_sequences(\n",
    "        concatenated_sequences,\n",
    "        padding_tokenizer=vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "            for_use_in_padding=True\n",
    "        ),\n",
    "    )\n",
    "    final_input_ids = final_input_ids.to(device)\n",
    "    final_attention_masks = final_attention_masks.to(device)\n",
    "    output_ids = model.generate(\n",
    "        input_ids=final_input_ids[:, :-1],\n",
    "        images=images,\n",
    "        attention_mask=final_attention_masks[:, :-1],\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "        max_new_tokens=10,\n",
    "        stopping_criteria=[stopping_criteria],\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "    confidence_texts = []\n",
    "    for input_idx, output in enumerate(output_ids):\n",
    "        confidence_texts.append(\n",
    "            tokenizer.decode(\n",
    "                output[final_input_ids[input_idx].shape[0] :], skip_special_tokens=True\n",
    "            )\n",
    "        )\n",
    "    confidence_values = response.parse_confidences(confidence_texts, granular_confidence=False)\n",
    "\n",
    "    for idx, conf_val in enumerate(confidence_values):\n",
    "        batch_records[idx][\"confidence\"] = conf_val\n",
    "\n",
    "    json_path = \"original_radialog_vanilla_verbalize.json\"\n",
    "    dataset.append_records_to_json_file(batch_records, json_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava_hf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
