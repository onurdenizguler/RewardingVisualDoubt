{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO sampling logic (undersample)\n",
    "# TODO binary qa prompt and co need to vary between a few samples\n",
    "# TODO determine hyperparams\n",
    "# TODO arg parsing\n",
    "# TODO dataloader num workers set to default\n",
    "\n",
    "# SOME NOTES\n",
    "# PPO_TRAINER AUTOMATICALLY PADS THE INPUTS BY TOKENIZER.PADDING_SIDE AND TOKENIZER.PADDING_TOKEN_ID\n",
    "# Uh-oh, because ppo termination token is set as the eos_seq_token, it'll stop when it sees a left padded sequence\n",
    "# Skipping random exploration for now\n",
    "\n",
    "\n",
    "# BATCH TIMING\n",
    "# A batch of 8 samples take around 1-1.5-2-3min to process in a train step (so around 400 samples per hour is trainable, every 50th batch, we save a checkpoint, and do val)\n",
    "# Lets save a checkpoint every half an hour or so\n",
    "# Give validation around 15 mins => 100 samples or so\n",
    "# Validation is around 8k so it'll be 1000 batches (1000*1.5 min = 25 hours)\n",
    "# len(dataset_eval) = 8737"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c08fcb411a54210b0213aa3e789676f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 69 files:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% Set script for interactive development and import modules\n",
    "from RewardingVisualDoubt import infrastructure, training, vllm\n",
    "\n",
    "infrastructure.make_ipython_reactive_to_changing_codebase()\n",
    "infrastructure.supress_known_warnings()\n",
    "\n",
    "import pathlib as path\n",
    "import typing as t\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import accelerate\n",
    "import dataclasses\n",
    "import functools\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from LLAVA_Biovil.llava.mm_utils import KeywordsStoppingCriteria\n",
    "from trl import PPOConfig, PPOTrainer\n",
    "import trl\n",
    "\n",
    "from RewardingVisualDoubt import dataset, prompter, shared, response, reward, vllm\n",
    "from RewardingVisualDoubt import training as training\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"da3cb086bbc110c16cbc5ba4c284a19b0b461710\"\n",
    "\n",
    "from LLAVA_Biovil.llava.mm_utils import KeywordsStoppingCriteria\n",
    "\n",
    "STOP_STR = prompter.Seperator.END_OF_SEQUENCE_SEPERATOR.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding fresh set of LoRA adapters and a fresh value head to the model for PPO training using Llava model loaded from: /home/guests/deniz_gueler/repos/RewardingVisualDoubt/models/radialog_binary_qa_with_confidence_sft_full_merged_model\n",
      "Loading LLaVA model with the base LLM and with RaDialog finetuned vision modules...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8437240fd6034fd4a7d8a3f8ba160a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 69 files:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be loaded at precision: 4bit\n",
      "Loading LLaVA from base /home/guests/deniz_gueler/repos/RewardingVisualDoubt/models/radialog_binary_qa_with_confidence_sft_full_merged_model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c861a3e40ee42d7805e8bb444b196ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading additional LLaVA weights...\n",
      "Merging model with vision tower weights...\n",
      "Using downloaded and verified file: /tmp/biovil_t_image_model_proj_size_128.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The `device_map` argument is not provided. We will override the device_map argument. to set the entire model on the current device. If you want to set the model on multiple devices, please provide a custom `device_map` argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded additional vision tower weights...\n",
      "Adding pretrained RaDialog LoRA adapters (or fresh LoRa adapters) and value head to the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the datasets and the dataloaders...\n",
      "Loading mimic_cxr_df from cache\n",
      "Loading balanced_binary_qa_mimic_cxr_df from cache\n",
      "Loading mimic_cxr_df from cache\n",
      "Loading balanced_binary_qa_mimic_cxr_df from cache\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "############################################ For prototyping only: Input hyperparameters ########################################\n",
    "NUM_EPOCHS = 1\n",
    "DEFAULT_BATCH_SIZE = 8\n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "MINI_BATCH_SIZE = int(DEFAULT_BATCH_SIZE / 2)\n",
    "LEARNING_RATE = 5e-5\n",
    "DEFAULT_OUTPUT_DIR = path.Path(\"output\")\n",
    "\n",
    "batch_size = DEFAULT_BATCH_SIZE\n",
    "num_epochs = NUM_EPOCHS\n",
    "batch_size = DEFAULT_BATCH_SIZE\n",
    "gradient_accumulation_steps = GRADIENT_ACCUMULATION_STEPS\n",
    "mini_batch_size = int(DEFAULT_BATCH_SIZE / 2)\n",
    "learning_rate = LEARNING_RATE\n",
    "out_dir: path.Path = DEFAULT_OUTPUT_DIR\n",
    "\n",
    "######################################## 0. Define the environment ########################################\n",
    "\n",
    "device_str = (\n",
    "    shared.torch_devices.cuda.value if torch.cuda.is_available() else shared.torch_devices.cpu.value\n",
    ")\n",
    "device = torch.device(device_str)\n",
    "\n",
    "######################################## 1. Load the model and tokenizer ########################################\n",
    "\n",
    "model = vllm.load_pretrained_llava_model_for_ppo_training_with_fresh_lora_adapters(\n",
    "    device_str=device_str,\n",
    "    llava_model_path=vllm.RadialogMergedLlavaModelPath.BINARY_QA_WITH_CONFIDENCE_SFT.value,\n",
    "    precision=\"4bit\",\n",
    ")\n",
    "\n",
    "tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "padding_tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "padding_tokenizer.padding_side = \"left\"\n",
    "\n",
    "\n",
    "######################################## 2. Load the datasets and the dataloaders ########################################\n",
    "\n",
    "print(\"Loading the datasets and the dataloaders...\")\n",
    "prompter_ = functools.partial(\n",
    "    prompter.build_binary_qa_prompt_with_response_and_confidence_for_sft, is_for_inference=True\n",
    ")\n",
    "dataset_train = dataset.get_binary_qa_prompted_mimic_cxr_llava_model_input_dataset(\n",
    "    split=dataset.DatasetSplit.TRAIN,\n",
    "    tokenizer=tokenizer,\n",
    "    prompter=prompter_,\n",
    ")\n",
    "dataset_eval = dataset.get_binary_qa_prompted_mimic_cxr_llava_model_input_dataset(\n",
    "    split=dataset.DatasetSplit.VALIDATION,\n",
    "    tokenizer=tokenizer,\n",
    "    prompter=prompter_,\n",
    ")\n",
    "\n",
    "dataloader_train = dataset.get_mimic_cxr_llava_model_input_dataloader(\n",
    "    dataset=dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    padding_tokenizer=padding_tokenizer,\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "dataloader_eval = dataset.get_mimic_cxr_llava_model_input_dataloader(\n",
    "    dataset=dataset_eval,\n",
    "    batch_size=2 * batch_size,\n",
    "    padding_tokenizer=padding_tokenizer,\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "eval_batch_iterator = iter(dataloader_eval)\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")  # Adds higher directory to python modules path.\n",
    "from workflows import radialog_binary_qa_ppo_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: No names found, cannot describe anything.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33monurdenizguler\u001b[0m (\u001b[33monurdenizguler-technical-university-of-munich\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/guests/deniz_gueler/repos/RewardingVisualDoubt/workflows/prototyping/wandb/run-20250501_212234-eb9q15by</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/onurdenizguler-technical-university-of-munich/radialog_binary_qa_ppo_training/runs/eb9q15by' target=\"_blank\">wobbly-dawn-13</a></strong> to <a href='https://wandb.ai/onurdenizguler-technical-university-of-munich/radialog_binary_qa_ppo_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/onurdenizguler-technical-university-of-munich/radialog_binary_qa_ppo_training' target=\"_blank\">https://wandb.ai/onurdenizguler-technical-university-of-munich/radialog_binary_qa_ppo_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/onurdenizguler-technical-university-of-munich/radialog_binary_qa_ppo_training/runs/eb9q15by' target=\"_blank\">https://wandb.ai/onurdenizguler-technical-university-of-munich/radialog_binary_qa_ppo_training/runs/eb9q15by</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:238: UserWarning: No dataset is provided. Make sure to set config.batch_size to the correct value before training.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "######################################## 3. Define the PPO and generation configurations ########################################\n",
    "\n",
    "# Example of batch size 16: 4 epochs over the batch. Each backward batch is of size 8, and each mini batch is of size 4\n",
    "# Gradients get accumulated during 4 + 4 mini batches, and then the model gets updated (the \"backward batch\" is completed)\n",
    "\n",
    "ppo_config = trl.PPOConfig(\n",
    "    learning_rate=learning_rate,\n",
    "    task_name=\"gpt\",\n",
    "    ppo_epochs=1,  # Default value from TRL library is 4 (i.e. will go over the batch 4 times), but since we have a lot of data, we can set it to 1\n",
    "    batch_size=batch_size,\n",
    "    # backward_batch_size=MINI_BATCH_SIZE,  # Default value from TRL library is 1, gets overwritten anyways at __init__ time\n",
    "    mini_batch_size=mini_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    log_with=\"wandb\",\n",
    "    tracker_project_name=\"radialog_binary_qa_ppo_training\",\n",
    "    project_kwargs=dataclasses.asdict(\n",
    "        accelerate.utils.ProjectConfiguration(\n",
    "            project_dir=\"radialog_binary_qa_ppo_training\", logging_dir=\"logs\"\n",
    "        )\n",
    "    ),\n",
    "    remove_unused_columns=False,\n",
    "    # optimize_device_cache=True,\n",
    "    kl_penalty=\"kl\",  # 'kl': model_logp - ref_logp,  'abs': abs(kl),  'mse': mean squared error mse(kl) and 'full': the actual kl for all tokens in the distribution\"\n",
    "    init_kl_coef=0.05,\n",
    ")\n",
    "\n",
    "generation_kwargs_ppo = {\n",
    "    \"min_length\": -1,  # don't ignore the EOS token (see above)\n",
    "    \"top_k\": 0.0,  # no top-k sampling\n",
    "    \"top_p\": 1.0,  # no nucleus sampling\n",
    "    \"temperature\": 1.0,  # DONT BE CREATIVE\n",
    "    \"do_sample\": True,  # yes, we want to sample\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,  # most decoder models don't have a padding token - use EOS token instead (for this tokenizer it was already set to eos_token_id)\n",
    "    \"max_new_tokens\": 50,  # let's not be chatty, we need only a few tokens to generate confidence but also let us not limit the response too much\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,  # (instead of ppo_terminators list)\n",
    "}\n",
    "\n",
    "ppo_trainer = t.cast(\n",
    "    training.MultimodalPPOTrainer,\n",
    "    training.MultimodalPPOTrainer(\n",
    "        model=model,\n",
    "        config=ppo_config,\n",
    "        tokenizer=tokenizer,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# not sure if needed but just to be safe for now\n",
    "tokenizer.padding_side = \"left\"\n",
    "model.config.padding_side = \"left\"\n",
    "model.config.tokenizer_padding_side = \"left\"\n",
    "# model.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_epoch = []\n",
    "accumulating_game_logs: training.GameLogs = {\n",
    "    \"queries\": [],\n",
    "    \"responses\": [],\n",
    "    \"is_answer_correct\": [],\n",
    "    \"scores\": [],\n",
    "    \"confidences\": [],\n",
    "}\n",
    "iterator_train = iter(dataloader_train)\n",
    "# model.pretrained_model.enable_input_require_grads()\n",
    "# custom_game_log_table = wandb.Table(columns=[\"query\", \"response\", \"reward\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 154/360 [58:21<1:17:47, 22.66s/it]"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(360)):\n",
    "    batch = next(iterator_train)\n",
    "    radialog_binary_qa_ppo_training.radialog_binary_qa_ppo_training_step(\n",
    "        model,\n",
    "        device,\n",
    "        tokenizer,\n",
    "        generation_kwargs_ppo,\n",
    "        ppo_trainer,\n",
    "        batch,\n",
    "        reward.reward_teachers_pet_behaviour,\n",
    "        accumulating_game_logs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\n",
    "    \"/home/guests/deniz_gueler/repos/RewardingVisualDoubt/models/teachers_pet_ppo_adapter_1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug the training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### 5.1 Unpack the batch #########\n",
    "batch: dataset.MimicCxrLlavaModelInputBatchDict = batch\n",
    "batch_llava_model_input_dict = batch[\"batch_llava_model_input_dict\"]\n",
    "batch_llava_model_input_dict = dataset.move_llava_model_input_dict_to_device(\n",
    "    batch_llava_model_input_dict, device\n",
    ")\n",
    "input_ids, images = (\n",
    "    batch_llava_model_input_dict[\"text_prompt_input_ids\"],\n",
    "    batch_llava_model_input_dict[\"images\"],\n",
    ")\n",
    "attention_mask = batch[\"batch_attention_mask\"].to(device)\n",
    "labels = t.cast(torch.Tensor, batch[\"batch_labels\"]).to(device)\n",
    "stopping_criteria = KeywordsStoppingCriteria([STOP_STR], tokenizer, input_ids)\n",
    "input_ids_list = training.remove_preciding_padding_from_batch_tensor(input_ids)\n",
    "\n",
    "######### 5.2 Generate the binary q&a answer and remove trailing padding tokens #########\n",
    "model.eval()\n",
    "model.gradient_checkpointing_disable()\n",
    "generated_ids = ppo_trainer.generate(\n",
    "    query_tensor=input_ids_list,  # ppo_trainer.generate() method admits list of tensors, handles padding and batching itself\n",
    "    images=images,\n",
    "    return_prompt=False,\n",
    "    batch_size=input_ids.shape[0],\n",
    "    use_cache=True,  # => not compatible with gradient checkpointing!\n",
    "    stopping_criteria=[stopping_criteria],\n",
    "    **generation_kwargs_ppo,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### 5.3 Parse the responses and compute the scores #########\n",
    "generated_texts = tokenizer.batch_decode(generated_ids)\n",
    "generated_answer_labels = response.parse_binary_labels(generated_texts)\n",
    "generated_confidence_values = response.parse_confidences(generated_texts)\n",
    "\n",
    "scores = [\n",
    "    reward.generated_answer_and_confidence_to_reward(\n",
    "        generated_answer_label, generated_confidence_value, ground_truth_label\n",
    "    )\n",
    "    for generated_answer_label, generated_confidence_value, ground_truth_label in zip(\n",
    "        generated_answer_labels, generated_confidence_values, labels.bool().tolist()\n",
    "    )\n",
    "]\n",
    "\n",
    "scores = t.cast(\n",
    "    list[torch.FloatTensor],\n",
    "    [torch.tensor(s).to(device) for s in scores],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### 5.7 Take a PPO optimization step #########\n",
    "model.train()\n",
    "model.gradient_checkpointing_enable()\n",
    "stats = ppo_trainer.multimodal_step(\n",
    "    queries=t.cast(list[torch.LongTensor], input_ids_list),\n",
    "    responses=t.cast(list[torch.LongTensor], generated_ids),\n",
    "    scores=scores,\n",
    "    images=images,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = ppo_trainer.prepare_model_inputs(\n",
    "    queries=t.cast(list[torch.LongTensor], input_ids_list),\n",
    "    responses=t.cast(list[torch.LongTensor], generated_confidences_ids),\n",
    ")\n",
    "\n",
    "model_inputs[\"images\"] = images  # N\n",
    "model_inputs_names = list(model_inputs.keys())\n",
    "\n",
    "queries = t.cast(list[torch.LongTensor], input_ids_list)\n",
    "responses = t.cast(list[torch.LongTensor], generated_confidences_ids)\n",
    "bs = len(queries)\n",
    "fbs = ppo_trainer.config.mini_batch_size\n",
    "all_logprobs = []\n",
    "all_logits = []\n",
    "all_masks = []\n",
    "all_values = []\n",
    "\n",
    "\n",
    "i = 2\n",
    "input_kwargs = {key: value[i * fbs : (i + 1) * fbs] for key, value in model_inputs.items()}\n",
    "# query_batch = queries[i * fbs : (i + 1) * fbs]\n",
    "# response_batch = responses[i * fbs : (i + 1) * fbs]\n",
    "with torch.no_grad():\n",
    "    logits_mini, _, values_mini = model(**input_kwargs)\n",
    "\n",
    "input_ids_mini = input_kwargs[\"input_ids\"]\n",
    "attention_mask_mini = input_kwargs[\"attention_mask\"]\n",
    "images_mini = input_kwargs[\"images\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    all_logprobs, logits_or_none, values, masks = ppo_trainer.batched_forward_pass(\n",
    "        model,\n",
    "        queries,\n",
    "        responses,\n",
    "        model_inputs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_inds = [0, 1, 2, 3]\n",
    "mini_batch_dict = {\n",
    "    \"logprobs\": all_logprobs[mini_batch_inds],\n",
    "    \"values\": values[mini_batch_inds],\n",
    "    \"masks\": masks[mini_batch_inds],\n",
    "    # hacks: the queries and responses are ragged.\n",
    "    \"queries\": [queries[i] for i in mini_batch_inds],\n",
    "    \"responses\": [responses for i in mini_batch_inds],\n",
    "}\n",
    "model_inputs_ = {\n",
    "    \"input_ids\": model_inputs[\"input_ids\"][mini_batch_inds],\n",
    "    \"attention_mask\": model_inputs[\"attention_mask\"][mini_batch_inds],\n",
    "    \"images\": model_inputs[\"images\"][mini_batch_inds],\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    logprobs_new, logits_new, vpreds_new, _ = ppo_trainer.batched_forward_pass(\n",
    "        model,\n",
    "        mini_batch_dict[\"queries\"],\n",
    "        mini_batch_dict[\"responses\"],\n",
    "        model_inputs=model_inputs_,\n",
    "        return_logits=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archived attempt to account for image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def account_for_image_embeddings_for_single_image_inputs(input_ids, logits, values, attention_mask):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_ids (torch.Tensor): A tensor shaped (batch_size, sequence_length) including exactly 1 image token id (-200 for llava) for each sequence\n",
    "        logits (torch.Tensor): A tensor shaped (batch_size, sequence_length, vocab_size)\n",
    "        values: The values of the model\n",
    "        attention_mask: The attention mask of the model\n",
    "    \"\"\"\n",
    "    # Locate the image index\n",
    "    indexes_of_image_token = (input_ids == training.LLAVA_IMAGE_TOKEN_INDEX).nonzero(as_tuple=True)[\n",
    "        1\n",
    "    ]\n",
    "\n",
    "    # Shift the indexes taking in the account where each sequence begins (bos_token is taken as basis)\n",
    "    indexes_of_bos_token = (input_ids == 1).int().argmax(dim=1)\n",
    "    indexes_of_image_token += indexes_of_bos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries = training.replace_image_token_with_another_token_for_list_of_tensors(input_ids_list)\n",
    "# queries = t.cast(list[torch.LongTensor], queries)\n",
    "model.gradient_checkpointing_enable()\n",
    "stats = ppo_trainer.multimodal_step(\n",
    "    queries=t.cast(list[torch.LongTensor], input_ids_list),\n",
    "    responses=t.cast(list[torch.LongTensor], generated_confidences_ids),\n",
    "    scores=rewards,\n",
    "    images=images,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = model.pretrained_model.base_model.model.model.get_vision_tower()(\n",
    "    images\n",
    ").patch_embeddings\n",
    "image_features = image_features.flatten(2).transpose(1, 2)\n",
    "image_features = model.pretrained_model.base_model.model.model.mm_projector(image_features)\n",
    "image_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels = (\n",
    "    model.pretrained_model.base_model.model.prepare_inputs_labels_for_multimodal(\n",
    "        input_ids=input_ids,\n",
    "        position_ids=None,\n",
    "        attention_mask=attention_mask,\n",
    "        past_key_values=None,\n",
    "        labels=None,\n",
    "        images=images,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POST_GENERATION_CONFIDENCE_REQUEST_1 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer? \"\n",
    "    \"Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. \"\n",
    "    \"A value close to 0 means you think there is a high probability that the answer is wrong. \"\n",
    "    \"The closer the value is to 10, the higher you think is the probability that the answer is correct. \"\n",
    "    \"The output should have the format 'Confidence: <confidence>' and nothing else. \"\n",
    ")\n",
    "\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_2 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer? \"\n",
    "    \"Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. \"\n",
    "    \"The output should have the format 'My Confidence: <confidence>'. \"\n",
    "    \"ASSISTANT: Here is my self evaluation of my confidence. My Confidence (out of 10):\"\n",
    ")\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_3 = \"USER: Irrelevant from my question, what is your name?\"\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_4 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer? \"\n",
    "    \"Provide a confidence between 0, 1, 2, 3, 4, 5 of how sure you are the answer is correct. \"\n",
    "    \"The output should have the format 'My Confidence: <confidence>' My Reasoning for my Self Confidence Evaluation: <reasoning>. \"\n",
    "    \"ASSISTANT: Here is my self evaluation of my confidence. My Confidence (out of 5):\"\n",
    ")\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_5 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer? \"\n",
    "    \"Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,  of how sure you are the answer is correct. \"\n",
    "    \"The output should have the format 'My Confidence: <confidence>' My Very Short Reasoning for my Self Confidence Evaluation: <very_short_reasoning>. \"\n",
    "    \"ASSISTANT: Here is my self evaluation of my confidence. My Confidence (out of 10):\"\n",
    ")\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_6 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer and what is your explanation regarding your confidence?\"\n",
    "    \"ASSISTANT: When asked how confident I am about a response, I consistently provide it in a JSON object, adhering to my policy.\"\n",
    "    \"My confidence is a value between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, where 0 means I am not confident at all and 10 means I am very confident.\"\n",
    "    \"The confidence JSON follows this structure: {'confidence': int, 'explanation': str}.\"\n",
    "    \"Here's my confidence JSON about my last response: \"\n",
    ")\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_7 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer?\"\n",
    "    \"ASSISTANT: When asked how confident I am about a response, I consistently provide it in a JSON object, adhering to my policy.\"\n",
    "    \"My confidence is a value between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, where 0 means I am not confident at all and 10 means I am very confident.\"\n",
    "    \"The confidence JSON follows this structure: {'confidence': int}.\"\n",
    "    \"Here's my confidence JSON about my last response: \"\n",
    ")\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_8 = (\n",
    "    \"</s> USER: Now evaluate your own response. How confident are you in your answer?\"\n",
    "    \"Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. \"\n",
    "    \"A value close to 0 means you think there is a high probability that the answer is wrong. \"\n",
    "    \"The closer the value is to 10, the higher you think is the probability that the answer is correct. \"\n",
    "    \"ASSISTANT: When asked how confident I am about a response, I consistently provide it in a JSON object, adhering to my policy.\"\n",
    "    \"The confidence JSON follows this structure: {'confidence': int}.\"\n",
    "    \"Here's my confidence JSON about my last response: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_STR = prompter.Seperator.END_OF_SEQUENCE_SEPERATOR.value\n",
    "from LLAVA_Biovil.llava.mm_utils import KeywordsStoppingCriteria\n",
    "from RewardingVisualDoubt import inference\n",
    "\n",
    "padding_tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "padding_tokenizer.padding_side = \"left\"\n",
    "padding_tokenizer.pad_token_id = padding_tokenizer.bos_token_id\n",
    "dataset_test = dataset.get_binary_qa_prompted_mimic_cxr_llava_model_input_dataset(\n",
    "    split=dataset.DatasetSplit.TEST,\n",
    "    tokenizer=tokenizer,\n",
    "    prompter=prompter.build_binary_qa_instruction_from_disease_under_study,\n",
    ")\n",
    "dataloader_test = dataset.get_mimic_cxr_llava_model_input_dataloader(\n",
    "    dataset=dataset_test, batch_size=1, padding_tokenizer=padding_tokenizer, num_workers=8\n",
    ")\n",
    "\n",
    "for idx, batch in enumerate(dataloader_test):\n",
    "    batch = t.cast(dataset.MimicCxrLlavaModelInputBatchDict, batch)\n",
    "    batch_llava_model_input_dict = batch[\"batch_llava_model_input_dict\"]\n",
    "    batch_llava_model_input_dict = dataset.move_llava_model_input_dict_to_device(\n",
    "        batch_llava_model_input_dict, torch.device(shared.torch_devices.cuda.value)\n",
    "    )\n",
    "    input_ids, images = (\n",
    "        batch_llava_model_input_dict[\"text_prompt_input_ids\"],\n",
    "        batch_llava_model_input_dict[\"images\"],\n",
    "    )\n",
    "    stopping_criteria = KeywordsStoppingCriteria([STOP_STR], tokenizer, input_ids)\n",
    "    pred = inference.generate_radialog_answer_for_binary_qa_for_single_study(\n",
    "        model, tokenizer, input_ids, images, stopping_criteria\n",
    "    )\n",
    "    confidence_request_prompt = (\n",
    "        batch[\"batch_prompts\"][0]\n",
    "        + \" \"\n",
    "        + pred\n",
    "        + \" \"\n",
    "        + prompter.build_post_generation_user_confidence_request()  # POST_GENERATION_CONFIDENCE_REQUEST_8\n",
    "    )\n",
    "    confidence_request_input_ids = torch.unsqueeze(\n",
    "        torch.IntTensor(tokenizer(confidence_request_prompt)[\"input_ids\"]), 0\n",
    "    ).to(device)\n",
    "    stopping_criteria = KeywordsStoppingCriteria(\n",
    "        [STOP_STR], tokenizer, confidence_request_input_ids\n",
    "    )\n",
    "    pred_with_confidence = inference.generate_radialog_answer_for_binary_qa_for_single_study(\n",
    "        model, tokenizer, confidence_request_input_ids, images, stopping_criteria\n",
    "    )\n",
    "    print(f\"\\n Metadata: {batch['batch_mimic_cxr_datapoint_metadata']}\")\n",
    "    print(f\"Prompt: {batch['batch_prompts']}\")\n",
    "    print(f\"Label:\", batch[\"batch_labels\"])\n",
    "    print(f\"File_idx {idx}, ASSISTANT: \", pred)\n",
    "    print(f\"File_idx {idx}, ASSISTANT (after confidence request): \", pred_with_confidence)\n",
    "    if idx == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## TEST TO SEE IF TEMPERATURE AND TOP_P PARAMS HELP WITH USER CONFIDENCE REQUEST WITHOUT ASSISTANT CONFIRMATION ########################################\n",
    "\n",
    "\n",
    "from LLAVA_Biovil.llava.mm_utils import tokenizer_image_token\n",
    "\n",
    "iterator_train = iter(dataloader_train)\n",
    "batch = next(iterator_train)\n",
    "\n",
    "batch_llava_model_input_dict = batch[\"batch_llava_model_input_dict\"]\n",
    "batch_llava_model_input_dict = dataset.move_llava_model_input_dict_to_device(\n",
    "    batch_llava_model_input_dict, device\n",
    ")\n",
    "_, images = (\n",
    "    batch_llava_model_input_dict[\"text_prompt_input_ids\"],\n",
    "    batch_llava_model_input_dict[\"images\"],\n",
    ")\n",
    "\n",
    "my_prompt = prompter.build_binary_qa_instruction_from_disease_under_study_with_confidence_request(\n",
    "    \"Cardiomegaly\"\n",
    ")\n",
    "tokenized_prompt = tokenizer_image_token(my_prompt, tokenizer, return_tensors=\"pt\").to(device)\n",
    "\n",
    "stopping_criteria = KeywordsStoppingCriteria([STOP_STR], tokenizer, tokenized_prompt.unsqueeze(0))\n",
    "\n",
    "prompt_and_generated_answers_ids = model.generate(\n",
    "    input_ids=tokenized_prompt.unsqueeze(0),\n",
    "    images=images[0].unsqueeze(0),\n",
    "    # attention_mask=attention_mask,\n",
    "    do_sample=True,\n",
    "    use_cache=True,\n",
    "    temperature=1.8,\n",
    "    top_p=0.7,\n",
    "    max_new_tokens=300,  # TODO maybe move to the kwargs\n",
    "    stopping_criteria=[stopping_criteria],  # TODO understand better\n",
    "    pad_token_id=tokenizer.pad_token_id,  # used in tokenizing after the generation, # TODO maybe move to the kwargs\n",
    "    # **generation_kwargs_prediction,  # TODO check which args to pass.\n",
    ")\n",
    "\n",
    "tokenizer.decode(\n",
    "    training.replace_image_token_with_another_token(prompt_and_generated_answers_ids)[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iterator_train)\n",
    "\n",
    "batch = t.cast(dataset.MimicCxrLlavaModelInputBatchDict, batch)\n",
    "batch_llava_model_input_dict = batch[\"batch_llava_model_input_dict\"]\n",
    "batch_llava_model_input_dict = dataset.move_llava_model_input_dict_to_device(\n",
    "    batch_llava_model_input_dict, device\n",
    ")\n",
    "input_ids, images = (\n",
    "    batch_llava_model_input_dict[\"text_prompt_input_ids\"],\n",
    "    batch_llava_model_input_dict[\"images\"],\n",
    ")\n",
    "attention_mask = batch[\"batch_attention_mask\"].to(device)  # TODO handle elsewhere\n",
    "labels = batch[\"batch_labels\"].to(device)  # TODO handle elsewhere\n",
    "\n",
    "\n",
    "model.eval()\n",
    "stopping_criteria = KeywordsStoppingCriteria([STOP_STR], tokenizer, input_ids)\n",
    "\n",
    "\n",
    "t3 = time.time()\n",
    "prompt_and_generated_answers_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    images=images,\n",
    "    attention_mask=attention_mask,\n",
    "    do_sample=False,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=32,  # Limiting, YES, but binary q&a answers are not very long!\n",
    "    stopping_criteria=[stopping_criteria],\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "t4 = time.time()\n",
    "prompt_and_generated_answers_ids = training.remove_trailing_padding_from_prediction(\n",
    "    prompt_and_generated_answers_ids, tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "# Append confidence request to the generated answers\n",
    "prompt_and_generated_answers_with_confidence_requests_ids = []\n",
    "for item in prompt_and_generated_answers_ids:\n",
    "    confidence_request_input_ids = (\n",
    "        tokenizer(prompter.build_post_generation_user_confidence_request(), return_tensors=\"pt\")\n",
    "        .input_ids.to(device)\n",
    "        .squeeze(0)\n",
    "    )[\n",
    "        1:\n",
    "    ]  # drop start of sequence token\n",
    "    prompt_and_generated_answers_with_confidence_requests_ids.append(\n",
    "        torch.cat((item, confidence_request_input_ids), 0)\n",
    "    )\n",
    "model.train()\n",
    "\n",
    "t5 = time.time()\n",
    "generated_confidences_ids = ppo_trainer.generate(\n",
    "    prompt_and_generated_answers_with_confidence_requests_ids,  # ppo_trainer.generate() method admits list of tensors, not a batch tensor unfortunately\n",
    "    images=images,\n",
    "    return_prompt=False,\n",
    "    **generation_kwargs_ppo,\n",
    ")\n",
    "t6 = time.time()\n",
    "\n",
    "\n",
    "complete_conversation_ids = [\n",
    "    torch.cat((p, c), 0)\n",
    "    for p, c in zip(\n",
    "        prompt_and_generated_answers_with_confidence_requests_ids,\n",
    "        generated_confidences_ids,\n",
    "    )\n",
    "]\n",
    "generated_answer_only_ids = [\n",
    "    prompt_and_generated_answers_ids[i][len(input_ids[i]) :] for i in range(len(input_ids))\n",
    "]\n",
    "\n",
    "# Remove the unindex image token from the prompt\n",
    "prompt_and_generated_answers_with_confidence_requests_ids = (\n",
    "    training.replace_image_token_with_another_token_for_list_of_tensors(\n",
    "        prompt_and_generated_answers_with_confidence_requests_ids\n",
    "    )\n",
    ")\n",
    "generated_answers_texts = tokenizer.batch_decode(\n",
    "    generated_answer_only_ids,\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "generated_confidences_texts = tokenizer.batch_decode(\n",
    "    generated_confidences_ids,\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "generated_answer_labels = response.parse_binary_labels(generated_answers_texts)\n",
    "generated_confidence_values = response.parse_confidences(generated_confidences_texts)\n",
    "\n",
    "rewards = [\n",
    "    reward.generated_answer_and_confidence_to_reward(\n",
    "        generated_answer_label, generated_confidence_value, ground_truth_label\n",
    "    )\n",
    "    for generated_answer_label, generated_confidence_value, ground_truth_label in zip(\n",
    "        generated_answer_labels, generated_confidence_values, labels.bool().tolist()\n",
    "    )\n",
    "]\n",
    "\n",
    "report = {}\n",
    "report[\"generated_answer_labels\"] = generated_answer_labels\n",
    "\n",
    "rewards_epoch += rewards\n",
    "rewards = [torch.tensor(r).to(device) for r in rewards]\n",
    "\n",
    "t7 = time.time()\n",
    "stats = ppo_trainer.step(\n",
    "    prompt_and_generated_answers_with_confidence_requests_ids, generated_answer_only_ids, rewards\n",
    ")\n",
    "t8 = time.time()\n",
    "\n",
    "# ppo_trainer.log_stats(stats, batch, rewards, columns_to_log=[\"query\", \"response\", \"answer\"])\n",
    "\n",
    "# print(f\"Finished epoch {epoch}. Average reward: {avg_reward}\")\n",
    "# ppo_trainer.save_pretrained(os.path.join(out_dir, \"model_finetuned\"))\n",
    "\n",
    "# TODO: For random exploration\n",
    "# chance_to_change_confidence -= reduce_per_step\n",
    "# chance_to_change_confidence = max(0, chance_to_change_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_set = \"mini_batch\"\n",
    "input_ids_idx = 1\n",
    "\n",
    "\n",
    "if working_set == \"mini_batch\":\n",
    "    print(\"Working with mini batch\")\n",
    "    input_ids_working = input_ids_mini.clone().detach()\n",
    "    logits_working = logits_mini.clone().detach()\n",
    "elif working_set == \"full_batch\":\n",
    "    print(\"Working with full batch\")\n",
    "    input_ids_working = input_ids.clone().detach()\n",
    "    logits_working = logits.clone().detach()\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"working_set must be one of ['mini_batch', 'full_batch'], but got {working_set}\"\n",
    "    )\n",
    "\n",
    "\n",
    "indexes_of_image_token = (input_ids_working == training.LLAVA_IMAGE_TOKEN_INDEX).nonzero(\n",
    "    as_tuple=True\n",
    ")[1]\n",
    "\n",
    "print(\n",
    "    tokenizer.batch_decode(\n",
    "        torch.argmax(logits_working[input_ids_idx][: indexes_of_image_token[input_ids_idx]], dim=-1)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    tokenizer.batch_decode(\n",
    "        torch.argmax(\n",
    "            logits_working[input_ids_idx][\n",
    "                indexes_of_image_token[input_ids_idx] : indexes_of_image_token[input_ids_idx] + 196\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    tokenizer.batch_decode(\n",
    "        torch.argmax(\n",
    "            logits_working[input_ids_idx][indexes_of_image_token[input_ids_idx] + 196 :], dim=-1\n",
    "        )\n",
    "    )\n",
    ")\n",
    "print(\"\\n \")\n",
    "\n",
    "print(input_ids_working[input_ids_idx])\n",
    "\n",
    "print(\"\\n \\n \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava_hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
