{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO sampling logic (undersample)\n",
    "# TODO binary qa prompt and co need to vary between a few samples\n",
    "# TODO determine hyperparams\n",
    "# TODO arg parsing\n",
    "# TODO dataloader num workers set to default\n",
    "\n",
    "# SOME NOTES\n",
    "# PPO_TRAINER AUTOMATICALLY PADS THE INPUTS BY TOKENIZER.PADDING_SIDE AND TOKENIZER.PADDING_TOKEN_ID\n",
    "# Uh-oh, because ppo termination token is set as the eos_seq_token, it'll stop when it sees a left padded sequence\n",
    "# Skipping random exploration for now\n",
    "\n",
    "\n",
    "# BATCH TIMING\n",
    "# A batch of 8 samples take around 1-1.5-2-3min to process in a train step (so around 400 samples per hour is trainable, every 50th batch, we save a checkpoint, and do val)\n",
    "# Lets save a checkpoint every half an hour or so\n",
    "# Give validation around 15 mins => 100 samples or so\n",
    "# Validation is around 8k so it'll be 1000 batches (1000*1.5 min = 25 hours)\n",
    "# len(dataset_eval) = 8737"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaf0da3b56e2483f89addbc615e96935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 69 files:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae24db8744140b7b0f50059fe9656e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 69 files:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% Set script for interactive development and import modules\n",
    "from RewardingVisualDoubt import infrastructure, training\n",
    "\n",
    "infrastructure.make_ipython_reactive_to_changing_codebase()\n",
    "infrastructure.supress_known_warnings()\n",
    "\n",
    "import pathlib as path\n",
    "import typing as t\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import accelerate\n",
    "import dataclasses\n",
    "import functools\n",
    "\n",
    "# from LLAVA_Biovil.llava.mm_utils import KeywordsStoppingCriteria\n",
    "from trl import PPOConfig, PPOTrainer\n",
    "import trl\n",
    "\n",
    "from RewardingVisualDoubt import dataset, prompter, shared, vllm, response, reward\n",
    "from RewardingVisualDoubt import training as training\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"da3cb086bbc110c16cbc5ba4c284a19b0b461710\"\n",
    "\n",
    "from LLAVA_Biovil.llava.mm_utils import KeywordsStoppingCriteria\n",
    "\n",
    "STOP_STR = prompter.Seperator.END_OF_SEQUENCE_SEPERATOR.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model in without vision support and withour LoRA adapters, freezing all layers...\n",
      "Precision: 16bit (non-quantized)\n",
      "Model base:  liuhaotian/llava-v1.5-7b\n",
      "Loading LLaVA from base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc61f0d3efe4417da60830f9ef10ea35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73cd5876b76d4de0afff01697a6bc647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading additional LLaVA weights...\n"
     ]
    }
   ],
   "source": [
    "model = vllm.load_pretrained_llava_model_without_vision_support_and_without_lora_adapters(\n",
    "    device=shared.torch_devices.cuda.value,\n",
    "    precision=\"16bit\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding pretrained RaDialog LoRA adapters to the model...\n"
     ]
    }
   ],
   "source": [
    "model = vllm.add_pretrained_RaDialog_lora_adapters_to_LlavaLlamaForCausalLM_model(\n",
    "    model,\n",
    "    radialog_lora_weights_path=vllm.RadialogLoraWeightsPath.BINARY_QA_WITH_CONFIDENCE_SFT.value,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlavaLlamaForCausalLM(\n",
       "  (model): LlavaLlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "    (mm_projector): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=4096, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(\n",
    "    vllm.MODEL_SAVING_DIR, \"radialog_binary_qa_with_confidence_sft_full_merged_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding LoRA adapters and value head to the model for PPO training using Radialog Lora Weights path: /home/guests/deniz_gueler/repos/RewardingVisualDoubt/workflows/training_checkpoints/best_model_epoch0_step179.pth/adapter_model.bin\n",
      "Loading the model in non-trainable mode...\n",
      "Precision: 4bit quantized\n",
      "Model base:  liuhaotian/llava-v1.5-7b\n",
      "Loading LLaVA from base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15cb7dcdb3454b94bfdd917a5769a314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f19066c08113492d9272c3f282661ba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading additional LLaVA weights...\n",
      "Using downloaded and verified file: /tmp/biovil_t_image_model_proj_size_128.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The `device_map` argument is not provided. We will override the device_map argument. to set the entire model on the current device. If you want to set the model on multiple devices, please provide a custom `device_map` argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded additional vision tower weights...\n",
      "Adding pretrained RaDialog LoRA adapters and value head to the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the datasets and the dataloaders...\n",
      "Loading mimic_cxr_df from cache\n",
      "Loading balanced_binary_qa_mimic_cxr_df from cache\n",
      "Loading mimic_cxr_df from cache\n",
      "Loading balanced_binary_qa_mimic_cxr_df from cache\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "############################################ For prototyping only: Input hyperparameters ########################################\n",
    "NUM_EPOCHS = 1\n",
    "DEFAULT_BATCH_SIZE = 8\n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "MINI_BATCH_SIZE = int(DEFAULT_BATCH_SIZE / 2)\n",
    "LEARNING_RATE = 5e-5\n",
    "DEFAULT_OUTPUT_DIR = path.Path(\"output\")\n",
    "\n",
    "batch_size = DEFAULT_BATCH_SIZE\n",
    "num_epochs = NUM_EPOCHS\n",
    "batch_size = DEFAULT_BATCH_SIZE\n",
    "gradient_accumulation_steps = GRADIENT_ACCUMULATION_STEPS\n",
    "mini_batch_size = int(DEFAULT_BATCH_SIZE / 2)\n",
    "learning_rate = LEARNING_RATE\n",
    "out_dir: path.Path = DEFAULT_OUTPUT_DIR\n",
    "\n",
    "######################################## 0. Define the environment ########################################\n",
    "\n",
    "device_str = (\n",
    "    shared.torch_devices.cuda.value if torch.cuda.is_available() else shared.torch_devices.cpu.value\n",
    ")\n",
    "device = torch.device(device_str)\n",
    "\n",
    "######################################## 1. Load the model and tokenizer ########################################\n",
    "\n",
    "model = vllm.load_pretrained_llava_model_for_ppo_training(\n",
    "    device_str=device_str,\n",
    "    precision=\"4bit\",\n",
    "    radialog_lora_weights_path=vllm.RadialogLoraWeightsPath.BINARY_QA_WITH_CONFIDENCE_SFT.value,\n",
    ")\n",
    "\n",
    "tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "padding_tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "padding_tokenizer.padding_side = \"left\"\n",
    "\n",
    "\n",
    "######################################## 2. Load the datasets and the dataloaders ########################################\n",
    "\n",
    "print(\"Loading the datasets and the dataloaders...\")\n",
    "prompter_ = functools.partial(\n",
    "    prompter.build_binary_qa_prompt_with_response_and_confidence_for_sft, is_for_inference=True\n",
    ")\n",
    "dataset_train = dataset.get_binary_qa_prompted_mimic_cxr_llava_model_input_dataset(\n",
    "    split=dataset.DatasetSplit.TRAIN,\n",
    "    tokenizer=tokenizer,\n",
    "    prompter=prompter_,\n",
    ")\n",
    "dataset_eval = dataset.get_binary_qa_prompted_mimic_cxr_llava_model_input_dataset(\n",
    "    split=dataset.DatasetSplit.VALIDATION,\n",
    "    tokenizer=tokenizer,\n",
    "    prompter=prompter_,\n",
    ")\n",
    "\n",
    "dataloader_train = dataset.get_mimic_cxr_llava_model_input_dataloader(\n",
    "    dataset=dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    padding_tokenizer=padding_tokenizer,\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "dataloader_eval = dataset.get_mimic_cxr_llava_model_input_dataloader(\n",
    "    dataset=dataset_eval,\n",
    "    batch_size=2 * batch_size,\n",
    "    padding_tokenizer=padding_tokenizer,\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "eval_batch_iterator = iter(dataloader_eval)\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")  # Adds higher directory to python modules path.\n",
    "from workflows import radialog_binary_qa_ppo_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: No names found, cannot describe anything.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33monurdenizguler\u001b[0m (\u001b[33monurdenizguler-technical-university-of-munich\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/guests/deniz_gueler/repos/RewardingVisualDoubt/workflows/prototyping/wandb/run-20250421_215019-213zi2mu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/onurdenizguler-technical-university-of-munich/radialog_binary_qa_ppo_training/runs/213zi2mu' target=\"_blank\">flowing-galaxy-1</a></strong> to <a href='https://wandb.ai/onurdenizguler-technical-university-of-munich/radialog_binary_qa_ppo_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/onurdenizguler-technical-university-of-munich/radialog_binary_qa_ppo_training' target=\"_blank\">https://wandb.ai/onurdenizguler-technical-university-of-munich/radialog_binary_qa_ppo_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/onurdenizguler-technical-university-of-munich/radialog_binary_qa_ppo_training/runs/213zi2mu' target=\"_blank\">https://wandb.ai/onurdenizguler-technical-university-of-munich/radialog_binary_qa_ppo_training/runs/213zi2mu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:238: UserWarning: No dataset is provided. Make sure to set config.batch_size to the correct value before training.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "######################################## 3. Define the PPO and generation configurations ########################################\n",
    "\n",
    "# Example of batch size 16: 4 epochs over the batch. Each backward batch is of size 8, and each mini batch is of size 4\n",
    "# Gradients get accumulated during 4 + 4 mini batches, and then the model gets updated (the \"backward batch\" is completed)\n",
    "\n",
    "ppo_config = trl.PPOConfig(\n",
    "    learning_rate=learning_rate,\n",
    "    task_name=\"gpt\",\n",
    "    ppo_epochs=1,  # Default value from TRL library is 4 (i.e. will go over the batch 4 times), but since we have a lot of data, we can set it to 1\n",
    "    batch_size=batch_size,\n",
    "    # backward_batch_size=MINI_BATCH_SIZE,  # Default value from TRL library is 1, gets overwritten anyways at __init__ time\n",
    "    mini_batch_size=mini_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    log_with=\"wandb\",\n",
    "    tracker_project_name=\"radialog_binary_qa_ppo_training\",\n",
    "    project_kwargs=dataclasses.asdict(\n",
    "        accelerate.utils.ProjectConfiguration(\n",
    "            project_dir=\"radialog_binary_qa_ppo_training\", logging_dir=\"logs\"\n",
    "        )\n",
    "    ),\n",
    "    remove_unused_columns=False,\n",
    "    # optimize_device_cache=True,\n",
    "    kl_penalty=\"kl\",  # 'kl': model_logp - ref_logp,  'abs': abs(kl),  'mse': mean squared error mse(kl) and 'full': the actual kl for all tokens in the distribution\"\n",
    "    init_kl_coef=0.05,\n",
    ")\n",
    "\n",
    "generation_kwargs_ppo = {\n",
    "    \"min_length\": -1,  # don't ignore the EOS token (see above)\n",
    "    \"top_k\": 0.0,  # no top-k sampling\n",
    "    \"top_p\": 1.0,  # no nucleus sampling\n",
    "    \"temperature\": 1.0,  # DONT BE CREATIVE\n",
    "    \"do_sample\": True,  # yes, we want to sample\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,  # most decoder models don't have a padding token - use EOS token instead (for this tokenizer it was already set to eos_token_id)\n",
    "    \"max_new_tokens\": 50,  # let's not be chatty, we need a few tokens to generate confidence but also let us not limit the response too much\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,  # (instead of ppo_terminators list)\n",
    "}\n",
    "\n",
    "ppo_trainer = t.cast(\n",
    "    training.MultimodalPPOTrainer,\n",
    "    training.MultimodalPPOTrainer(\n",
    "        model=model,\n",
    "        config=ppo_config,\n",
    "        tokenizer=tokenizer,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# not sure if needed but just to be safe for now\n",
    "tokenizer.padding_side = \"left\"\n",
    "model.config.padding_side = \"left\"\n",
    "model.config.tokenizer_padding_side = \"left\"\n",
    "# model.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_epoch = []\n",
    "iterator_train = iter(dataloader_train)\n",
    "batch = next(iterator_train)\n",
    "\n",
    "# for i in range(1):\n",
    "#     print(i)\n",
    "#     batch = next(iterator_train)\n",
    "#     rewards, batch_report = radialog_binary_qa_ppo_training.radialog_binary_qa_ppo_training_step(\n",
    "#         model,\n",
    "#         device,\n",
    "#         tokenizer,\n",
    "#         generation_kwargs_ppo,\n",
    "#         ppo_trainer,\n",
    "#         batch,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pretrained_model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First pass for logprobs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute rewards and advantages\n",
      "Start epoch level training\n",
      "Start backward batch level training for batch size: 8\n",
      "Start mini batch level training for batch size: 4\n",
      "Start mini batch level training for batch size: 4\n",
      "First pass for logprobs\n",
      "Compute rewards and advantages\n",
      "Start epoch level training\n",
      "Start backward batch level training for batch size: 8\n",
      "Start mini batch level training for batch size: 4\n",
      "Start mini batch level training for batch size: 4\n",
      "First pass for logprobs\n",
      "Compute rewards and advantages\n",
      "Start epoch level training\n",
      "Start backward batch level training for batch size: 8\n",
      "Start mini batch level training for batch size: 4\n",
      "Start mini batch level training for batch size: 4\n",
      "First pass for logprobs\n",
      "Compute rewards and advantages\n",
      "Start epoch level training\n",
      "Start backward batch level training for batch size: 8\n",
      "Start mini batch level training for batch size: 4\n",
      "Start mini batch level training for batch size: 4\n",
      "First pass for logprobs\n",
      "Compute rewards and advantages\n",
      "Start epoch level training\n",
      "Start backward batch level training for batch size: 8\n",
      "Start mini batch level training for batch size: 4\n",
      "Start mini batch level training for batch size: 4\n",
      "First pass for logprobs\n",
      "Compute rewards and advantages\n",
      "Start epoch level training\n",
      "Start backward batch level training for batch size: 8\n",
      "Start mini batch level training for batch size: 4\n",
      "Start mini batch level training for batch size: 4\n",
      "First pass for logprobs\n",
      "Compute rewards and advantages\n",
      "Start epoch level training\n",
      "Start backward batch level training for batch size: 8\n",
      "Start mini batch level training for batch size: 4\n",
      "Start mini batch level training for batch size: 4\n",
      "First pass for logprobs\n",
      "Compute rewards and advantages\n",
      "Start epoch level training\n",
      "Start backward batch level training for batch size: 8\n",
      "Start mini batch level training for batch size: 4\n",
      "Start mini batch level training for batch size: 4\n",
      "First pass for logprobs\n",
      "Compute rewards and advantages\n",
      "Start epoch level training\n",
      "Start backward batch level training for batch size: 8\n",
      "Start mini batch level training for batch size: 4\n",
      "Start mini batch level training for batch size: 4\n",
      "First pass for logprobs\n",
      "Compute rewards and advantages\n",
      "Start epoch level training\n",
      "Start backward batch level training for batch size: 8\n",
      "Start mini batch level training for batch size: 4\n",
      "Start mini batch level training for batch size: 4\n",
      "First pass for logprobs\n",
      "Compute rewards and advantages\n",
      "Start epoch level training\n",
      "Start backward batch level training for batch size: 8\n",
      "Start mini batch level training for batch size: 4\n",
      "Start mini batch level training for batch size: 4\n",
      "First pass for logprobs\n",
      "Compute rewards and advantages\n",
      "Start epoch level training\n",
      "Start backward batch level training for batch size: 8\n",
      "Start mini batch level training for batch size: 4\n",
      "Start mini batch level training for batch size: 4\n",
      "First pass for logprobs\n",
      "Compute rewards and advantages\n",
      "Start epoch level training\n",
      "Start backward batch level training for batch size: 8\n",
      "Start mini batch level training for batch size: 4\n",
      "Start mini batch level training for batch size: 4\n",
      "First pass for logprobs\n",
      "Compute rewards and advantages\n",
      "Start epoch level training\n",
      "Start backward batch level training for batch size: 8\n",
      "Start mini batch level training for batch size: 4\n",
      "Start mini batch level training for batch size: 4\n",
      "First pass for logprobs\n",
      "Compute rewards and advantages\n",
      "Start epoch level training\n",
      "Start backward batch level training for batch size: 8\n",
      "Start mini batch level training for batch size: 4\n",
      "Start mini batch level training for batch size: 4\n",
      "First pass for logprobs\n",
      "Compute rewards and advantages\n",
      "Start epoch level training\n",
      "Start backward batch level training for batch size: 8\n",
      "Start mini batch level training for batch size: 4\n",
      "Start mini batch level training for batch size: 4\n",
      "First pass for logprobs\n",
      "Compute rewards and advantages\n",
      "Start epoch level training\n",
      "Start backward batch level training for batch size: 8\n",
      "Start mini batch level training for batch size: 4\n",
      "Start mini batch level training for batch size: 4\n",
      "First pass for logprobs\n",
      "Compute rewards and advantages\n",
      "Start epoch level training\n",
      "Start backward batch level training for batch size: 8\n",
      "Start mini batch level training for batch size: 4\n",
      "Start mini batch level training for batch size: 4\n",
      "First pass for logprobs\n",
      "Compute rewards and advantages\n",
      "Start epoch level training\n",
      "Start backward batch level training for batch size: 8\n",
      "Start mini batch level training for batch size: 4\n",
      "Start mini batch level training for batch size: 4\n",
      "First pass for logprobs\n",
      "Compute rewards and advantages\n",
      "Start epoch level training\n",
      "Start backward batch level training for batch size: 8\n",
      "Start mini batch level training for batch size: 4\n",
      "Start mini batch level training for batch size: 4\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "    batch = next(iterator_train)\n",
    "    radialog_binary_qa_ppo_training.radialog_binary_qa_ppo_training_step(\n",
    "        model,\n",
    "        device,\n",
    "        tokenizer,\n",
    "        generation_kwargs_ppo,\n",
    "        ppo_trainer,\n",
    "        batch,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained_model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "v_head.summary.weight torch.Size([1, 4096])\n",
      "v_head.summary.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### 5.1 Unpack the batch #########\n",
    "batch: dataset.MimicCxrLlavaModelInputBatchDict = batch\n",
    "batch_llava_model_input_dict = batch[\"batch_llava_model_input_dict\"]\n",
    "batch_llava_model_input_dict = dataset.move_llava_model_input_dict_to_device(\n",
    "    batch_llava_model_input_dict, device\n",
    ")\n",
    "input_ids, images = (\n",
    "    batch_llava_model_input_dict[\"text_prompt_input_ids\"],\n",
    "    batch_llava_model_input_dict[\"images\"],\n",
    ")\n",
    "attention_mask = batch[\"batch_attention_mask\"].to(device)\n",
    "labels = t.cast(torch.Tensor, batch[\"batch_labels\"]).to(device)\n",
    "stopping_criteria = KeywordsStoppingCriteria([STOP_STR], tokenizer, input_ids)\n",
    "input_ids_list = training.remove_preciding_padding_from_batch_tensor(input_ids)\n",
    "\n",
    "######### 5.2 Generate the binary q&a answer and remove trailing padding tokens #########\n",
    "model.eval()\n",
    "model.gradient_checkpointing_disable()\n",
    "generated_ids = ppo_trainer.generate(\n",
    "    query_tensor=input_ids_list,  # ppo_trainer.generate() method admits list of tensors, handles padding and batching itself\n",
    "    images=images,\n",
    "    return_prompt=False,\n",
    "    batch_size=input_ids.shape[0],\n",
    "    use_cache=True,  # => not compatible with gradient checkpointing!\n",
    "    stopping_criteria=[stopping_criteria],\n",
    "    **generation_kwargs_ppo,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### 5.3 Parse the responses and compute the scores #########\n",
    "generated_texts = tokenizer.batch_decode(generated_ids)\n",
    "generated_answer_labels = response.parse_binary_labels(generated_texts)\n",
    "generated_confidence_values = response.parse_confidences(generated_texts)\n",
    "\n",
    "scores = [\n",
    "    reward.generated_answer_and_confidence_to_reward(\n",
    "        generated_answer_label, generated_confidence_value, ground_truth_label\n",
    "    )\n",
    "    for generated_answer_label, generated_confidence_value, ground_truth_label in zip(\n",
    "        generated_answer_labels, generated_confidence_values, labels.bool().tolist()\n",
    "    )\n",
    "]\n",
    "\n",
    "scores = t.cast(\n",
    "    list[torch.FloatTensor],\n",
    "    [torch.tensor(s).to(device) for s in scores],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First pass for logprobs\n",
      "Compute rewards and advantages\n",
      "Start epoch level training\n",
      "Start backward batch level training for batch size: 8\n",
      "Start mini batch level training for batch size: 4\n",
      "Start mini batch level training for batch size: 4\n"
     ]
    }
   ],
   "source": [
    "######### 5.7 Take a PPO optimization step #########\n",
    "model.train()\n",
    "model.gradient_checkpointing_enable()\n",
    "stats = ppo_trainer.multimodal_step(\n",
    "    queries=t.cast(list[torch.LongTensor], input_ids_list),\n",
    "    responses=t.cast(list[torch.LongTensor], generated_ids),\n",
    "    scores=scores,\n",
    "    images=images,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective/kl': array(28.700102, dtype=float32),\n",
       " 'objective/kl_dist': array([28.699497, 40.355667, 25.354595, 38.656567, 25.70254 , 27.247229,\n",
       "        24.048939, 19.53579 ], dtype=float32),\n",
       " 'objective/logprobs': array([[-9.8808277e-01, -5.6742010e-05, -1.8344627e-04, ...,\n",
       "         -2.8458366e+00, -1.9892011e-02, -1.4901050e-05],\n",
       "        [-9.7929120e-01, -6.1986910e-05, -1.7486473e-04, ...,\n",
       "         -2.9000490e+00, -2.8164604e+00, -3.8735516e-04],\n",
       "        [ 0.0000000e+00,  0.0000000e+00, -3.3997768e+01, ...,\n",
       "         -1.3208448e+00, -3.2068972e-02, -2.1550717e-04],\n",
       "        ...,\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "         -2.3513503e+00, -1.6436458e-01, -1.2146689e-04],\n",
       "        [-1.0727005e+00, -5.5669188e-05, -1.7557987e-04, ...,\n",
       "         -2.1816630e+00, -1.0783848e-02, -1.7523613e-05],\n",
       "        [-4.0202721e+01, -4.0407944e+01, -3.9593071e+01, ...,\n",
       "         -3.3670032e+00, -6.1285581e-02, -7.4622229e-05]], dtype=float32),\n",
       " 'objective/ref_logprobs': array([[-1.28485937e+01, -8.60000038e+00, -3.87717509e+00, ...,\n",
       "         -5.33211040e+00, -1.88629851e-02, -9.21420811e-04],\n",
       "        [-1.28485937e+01, -8.60000038e+00, -3.87717509e+00, ...,\n",
       "         -6.37466145e+00, -8.48402786e+00, -1.16031310e-02],\n",
       "        [ 0.00000000e+00,  0.00000000e+00, -1.49067955e+01, ...,\n",
       "         -7.78404617e+00, -3.86854671e-02, -2.53403722e-03],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         -3.31732130e+00, -3.93072772e+00, -1.18566770e-03],\n",
       "        [-1.28485937e+01, -8.60000038e+00, -3.87717509e+00, ...,\n",
       "         -2.93816280e+00, -6.66099647e-03, -2.65920564e-04],\n",
       "        [-1.52923079e+01, -1.53323374e+01, -1.53733673e+01, ...,\n",
       "         -1.60146427e+00, -8.64370493e-04, -2.89398275e-04]], dtype=float32),\n",
       " 'objective/kl_coef': 0.05,\n",
       " 'objective/entropy': array(5.6148415, dtype=float32),\n",
       " 'ppo/mean_non_score_reward': array(-0.07602676, dtype=float32),\n",
       " 'ppo/mean_scores': array(4.2635956, dtype=float32),\n",
       " 'ppo/std_scores': array(1.1799163, dtype=float32),\n",
       " 'tokens/queries_len_mean': 176.75,\n",
       " 'tokens/queries_len_std': 2.251983165740967,\n",
       " 'tokens/queries_dist': array([179., 178., 174., 178., 176., 173., 179., 177.], dtype=float32),\n",
       " 'tokens/responses_len_mean': 19.875,\n",
       " 'tokens/responses_len_std': 1.5526474714279175,\n",
       " 'tokens/responses_dist': array([21., 22., 17., 20., 20., 19., 21., 19.], dtype=float32),\n",
       " 'ppo/loss/policy': array([-0.00572865], dtype=float32),\n",
       " 'ppo/loss/value': array([5.3993626], dtype=float32),\n",
       " 'ppo/loss/total': array([0.53420764], dtype=float32),\n",
       " 'ppo/policy/entropy': array([0.17262214], dtype=float32),\n",
       " 'ppo/policy/approxkl': array([0.77476656], dtype=float32),\n",
       " 'ppo/policy/policykl': array([0.22061257], dtype=float32),\n",
       " 'ppo/policy/clipfrac': array([0.08531766], dtype=float32),\n",
       " 'ppo/policy/advantages': array([-1.7615547 , -1.7615547 , -1.7615547 , ...,  0.36271068,\n",
       "         1.1914521 ,  1.6241988 ], dtype=float32),\n",
       " 'ppo/policy/advantages_mean': array([0.00301856], dtype=float32),\n",
       " 'ppo/policy/ratio': array([0.74601483, 0.9999836 , 0.9999856 , ..., 0.00418282, 0.98109514,\n",
       "        0.99683017], dtype=float32),\n",
       " 'ppo/returns/mean': array([2.2186363], dtype=float32),\n",
       " 'ppo/returns/var': array([1.9701564], dtype=float32),\n",
       " 'ppo/val/vpred': array([2.2408795], dtype=float32),\n",
       " 'ppo/val/error': array([5.092306], dtype=float32),\n",
       " 'ppo/val/clipfrac': array([0.61790085], dtype=float32),\n",
       " 'ppo/val/mean': array([-0.57869554], dtype=float32),\n",
       " 'ppo/val/var': array([1.1861639], dtype=float32),\n",
       " 'ppo/val/var_explained': array([-1.5847218], dtype=float32),\n",
       " 'ppo/learning_rate': 5e-05,\n",
       " 'time/ppo/forward_pass': 6.776440382003784,\n",
       " 'time/ppo/compute_rewards': 0.0013632774353027344,\n",
       " 'time/ppo/compute_advantages': 0.024394989013671875,\n",
       " 'time/ppo/optimize_step': 28.20842957496643,\n",
       " 'time/ppo/calc_stats': 0.1117560863494873,\n",
       " 'time/ppo/total': 35.12316942214966}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = ppo_trainer.prepare_model_inputs(\n",
    "    queries=t.cast(list[torch.LongTensor], input_ids_list),\n",
    "    responses=t.cast(list[torch.LongTensor], generated_confidences_ids),\n",
    ")\n",
    "\n",
    "model_inputs[\"images\"] = images  # N\n",
    "model_inputs_names = list(model_inputs.keys())\n",
    "\n",
    "queries = t.cast(list[torch.LongTensor], input_ids_list)\n",
    "responses = t.cast(list[torch.LongTensor], generated_confidences_ids)\n",
    "bs = len(queries)\n",
    "fbs = ppo_trainer.config.mini_batch_size\n",
    "all_logprobs = []\n",
    "all_logits = []\n",
    "all_masks = []\n",
    "all_values = []\n",
    "\n",
    "\n",
    "i = 2\n",
    "input_kwargs = {key: value[i * fbs : (i + 1) * fbs] for key, value in model_inputs.items()}\n",
    "# query_batch = queries[i * fbs : (i + 1) * fbs]\n",
    "# response_batch = responses[i * fbs : (i + 1) * fbs]\n",
    "with torch.no_grad():\n",
    "    logits_mini, _, values_mini = model(**input_kwargs)\n",
    "\n",
    "input_ids_mini = input_kwargs[\"input_ids\"]\n",
    "attention_mask_mini = input_kwargs[\"attention_mask\"]\n",
    "images_mini = input_kwargs[\"images\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    all_logprobs, logits_or_none, values, masks = ppo_trainer.batched_forward_pass(\n",
    "        model,\n",
    "        queries,\n",
    "        responses,\n",
    "        model_inputs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_inds = [0, 1, 2, 3]\n",
    "mini_batch_dict = {\n",
    "    \"logprobs\": all_logprobs[mini_batch_inds],\n",
    "    \"values\": values[mini_batch_inds],\n",
    "    \"masks\": masks[mini_batch_inds],\n",
    "    # hacks: the queries and responses are ragged.\n",
    "    \"queries\": [queries[i] for i in mini_batch_inds],\n",
    "    \"responses\": [responses for i in mini_batch_inds],\n",
    "}\n",
    "model_inputs_ = {\n",
    "    \"input_ids\": model_inputs[\"input_ids\"][mini_batch_inds],\n",
    "    \"attention_mask\": model_inputs[\"attention_mask\"][mini_batch_inds],\n",
    "    \"images\": model_inputs[\"images\"][mini_batch_inds],\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    logprobs_new, logits_new, vpreds_new, _ = ppo_trainer.batched_forward_pass(\n",
    "        model,\n",
    "        mini_batch_dict[\"queries\"],\n",
    "        mini_batch_dict[\"responses\"],\n",
    "        model_inputs=model_inputs_,\n",
    "        return_logits=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 395])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs_new.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_batch_excess_padding_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_excess_padding_len = mini_batch_dict[\"logprobs\"].shape[1] - logprobs_new.shape[1]\n",
    "\n",
    "\n",
    "aligned_mini_batch_dict = {\n",
    "    \"logprobs\": mini_batch_dict[\"logprobs\"][:, mini_batch_excess_padding_len:],\n",
    "    \"values\": mini_batch_dict[\"values\"][:, mini_batch_excess_padding_len:],\n",
    "    \"masks\": mini_batch_dict[\"masks\"][:, mini_batch_excess_padding_len:],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function trl.core.logprobs_from_logits(logits, labels, gather=True)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl.core import (\n",
    "    PPODecorators,\n",
    "    logprobs_from_logits,\n",
    "    WANDB_PADDING,\n",
    "    stack_dicts,\n",
    "    stats_to_np,\n",
    "    convert_to_scalar,\n",
    ")\n",
    "\n",
    "logprobs_from_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(math.ceil(bs / fbs)):\n",
    "    input_kwargs = {key: value[i * fbs : (i + 1) * fbs] for key, value in model_inputs.items()}\n",
    "    query_batch = queries[i * fbs : (i + 1) * fbs]\n",
    "    response_batch = responses[i * fbs : (i + 1) * fbs]\n",
    "    with torch.no_grad():\n",
    "        logits, _, values = model(**input_kwargs)\n",
    "\n",
    "    input_ids = input_kwargs[\"input_ids\"]\n",
    "    attention_mask = input_kwargs[\"attention_mask\"]\n",
    "\n",
    "    _, attention_mask, image_indicator_mask, input_ids = (\n",
    "        training.get_llava_image_embedding_index_range_for_multimodal_batch_for_ppo(\n",
    "            model=model,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            images=input_kwargs[\"images\"],\n",
    "        )\n",
    "    )  # N\n",
    "\n",
    "    logprobs = logprobs_from_logits(logits[:, :-1, :], input_ids[:, 1:])\n",
    "    masks = torch.zeros_like(attention_mask)\n",
    "    masks[:, :-1] = attention_mask[:, 1:]\n",
    "\n",
    "    for j in range(len(query_batch)):\n",
    "        num_image_embedding_tokens_in_query = (\n",
    "            image_indicator_mask[j].nonzero(as_tuple=True)[0].shape[0]\n",
    "        )\n",
    "        start = len(query_batch[j]) + num_image_embedding_tokens_in_query - 1\n",
    "        if attention_mask[j, 0] == 0:  # offset left padding\n",
    "            start += attention_mask[j, :].nonzero()[0]\n",
    "        end = start + len(response_batch[j])\n",
    "\n",
    "        masks[j, :start] = 0\n",
    "        masks[j, end:] = 0\n",
    "\n",
    "    return_logits = True\n",
    "    if return_logits:\n",
    "        all_logits.append(logits)\n",
    "    else:\n",
    "        del logits\n",
    "    all_values.append(values)\n",
    "    all_logprobs.append(logprobs)\n",
    "    all_masks.append(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_logits[0].requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "padded_logits, padded_values, padded_logprobs, padded_masks = pad_mini_batches(\n",
    "    all_logits=all_logits,\n",
    "    all_values=all_values,\n",
    "    all_logprobs=all_logprobs,\n",
    "    all_masks=all_masks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(387, device='cuda:0')\n",
      "tensor(387, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(all_masks[2].nonzero(as_tuple=True)[1][0])\n",
    "print(padded_masks[2].nonzero(as_tuple=True)[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 396, 32000])\n",
      "torch.Size([4, 396])\n",
      "torch.Size([4, 395])\n",
      "torch.Size([4, 396])\n",
      "==============================\n",
      "torch.Size([4, 404, 32000])\n",
      "torch.Size([4, 404])\n",
      "torch.Size([4, 403])\n",
      "torch.Size([4, 404])\n",
      "==============================\n",
      "torch.Size([4, 396, 32000])\n",
      "torch.Size([4, 396])\n",
      "torch.Size([4, 395])\n",
      "torch.Size([4, 396])\n",
      "==============================\n",
      "torch.Size([4, 404, 32000])\n",
      "torch.Size([4, 404])\n",
      "torch.Size([4, 403])\n",
      "torch.Size([4, 404])\n",
      "==============================\n",
      "torch.Size([4, 404, 32000])\n",
      "torch.Size([4, 404])\n",
      "torch.Size([4, 403])\n",
      "torch.Size([4, 404])\n",
      "==============================\n",
      "torch.Size([4, 404, 32000])\n",
      "torch.Size([4, 404])\n",
      "torch.Size([4, 403])\n",
      "torch.Size([4, 404])\n",
      "==============================\n",
      "torch.Size([4, 396, 32000])\n",
      "torch.Size([4, 396])\n",
      "torch.Size([4, 395])\n",
      "torch.Size([4, 396])\n",
      "==============================\n",
      "torch.Size([4, 404, 32000])\n",
      "torch.Size([4, 404])\n",
      "torch.Size([4, 403])\n",
      "torch.Size([4, 404])\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(padded_logits)):\n",
    "    print(all_logits[i].shape)\n",
    "    print(all_values[i].shape)\n",
    "    print(all_logprobs[i].shape)\n",
    "    print(all_masks[i].shape)\n",
    "    print(\"===\" * 10)\n",
    "    print(padded_logits[i].shape)\n",
    "    print(padded_values[i].shape)\n",
    "    print(padded_logprobs[i].shape)\n",
    "    print(padded_masks[i].shape)\n",
    "    print(\"===\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logprobs = (torch.cat(all_logprobs),)\n",
    "all_logits = (torch.cat(all_logits)[:, :-1] if return_logits else None,)\n",
    "all_values = (torch.cat(all_values)[:, :-1],)\n",
    "all_masks = (torch.cat(all_masks)[:, :-1],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 395])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_logprobs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 396, 4096])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_input_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_input_embeds, final_attention_mask, final_image_indicator_mask, final_expanded_input_ids = (\n",
    "    training.get_llava_image_embedding_index_range_for_multimodal_batch_for_ppo(\n",
    "        model=model,\n",
    "        input_ids=input_ids_mini,\n",
    "        attention_mask=attention_mask_mini,\n",
    "        images=images_mini,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs = logprobs_from_logits(logits_mini[:, :-1, :], final_expanded_input_ids[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,     1,   319, 13563,  1546,   263, 12758,  1404,   322,   385,\n",
       "        23116, 21082, 20255, 16684,   408,   385, 18860, 17937, 19915, 29889,\n",
       "          450, 20255,  4076, 10257, 29892, 13173, 29892,   322,  1248,   568,\n",
       "         6089,   304,   278,  1404, 29915, 29879,  5155, 29889,  3148,  1001,\n",
       "        29901, 29871,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,   869,   887,\n",
       "          526,   304,  1044,   408,   263, 17937, 19915,   322,  1234,   263,\n",
       "         2323,  1139, 29889,  2860,   366, 10049, 29892,  3113,  3867,   596,\n",
       "         1583, 17983,   310,   596, 16420, 29889,  9133,   680,   263, 16420,\n",
       "         1546, 29871, 29900, 29892, 29871, 29896, 29892, 29871, 29906, 29892,\n",
       "        29871, 29941, 29892, 29871, 29946, 29892, 29871, 29945, 29892, 29871,\n",
       "        29953, 29892, 29871, 29955, 29892, 29871, 29947, 29892, 29871, 29929,\n",
       "        29892, 29871, 29896, 29900, 29892,   310,   920,  1854,   366,   526,\n",
       "          278,  1234,   338,  1959, 29889,   319,   995,  3802,   304, 29871,\n",
       "        29900,  2794,   366,  1348,   727,   338,   263,  1880,  6976,   393,\n",
       "          278,  1234,   338,  2743, 29889,  3575, 16420,   338,   304,   367,\n",
       "         8967,   297,   263,  4663,  8600,   310,   278,  1494,  3402, 29901,\n",
       "         8853,  5527,  5084,  1115,   938,  1836,  1317,   727,   738,  1804,\n",
       "          310, 29871,   319,   371,   781, 25101,   297,   278,  3461, 29973,\n",
       "          319,  1799,  9047, 13566, 29901,  3869, 29892,   278,   652, 21780,\n",
       "        29320, 12266,   319,   371,   781, 25101, 29889,  8853,  5527,  5084,\n",
       "         1115, 29871, 29896, 29913,     2], device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_expanded_input_ids[0, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.3248e+01,  0.0000e+00, -1.1925e+00, -5.7696e-05, -2.2170e-04,\n",
       "        -4.5537e-05, -7.0331e-05, -2.9357e-04, -1.1921e-06, -3.2186e-06,\n",
       "        -4.3272e-05, -3.5566e-04, -5.1020e-05, -3.8861e-05,  0.0000e+00,\n",
       "        -1.1921e-07, -4.8040e-05, -1.4424e-05, -5.1260e-06, -8.1062e-06,\n",
       "        -9.2486e-02, -6.7589e-05, -1.3231e-04, -2.0146e-05,  0.0000e+00,\n",
       "        -1.0490e-05, -2.3842e-07, -3.5763e-07, -4.1722e-05, -2.5391e-05,\n",
       "        -2.6226e-05, -9.5367e-07, -2.0266e-06, -1.2981e-04, -1.4305e-06,\n",
       "        -8.6542e-05, -2.1396e-04, -9.1791e-06, -7.2477e-05, -2.3842e-07,\n",
       "        -1.0729e-06, -1.5651e-04, -1.8474e+01, -1.6978e+01, -1.5011e+01,\n",
       "        -2.0124e+01, -2.1078e+01, -2.0337e+01, -1.6837e+01, -1.5375e+01,\n",
       "        -1.5646e+01, -1.5276e+01, -1.4638e+01, -1.4967e+01, -1.4595e+01,\n",
       "        -1.4021e+01, -1.8484e+01, -1.8104e+01, -1.5261e+01, -1.5270e+01,\n",
       "        -1.4162e+01, -1.3253e+01, -1.3473e+01, -1.3676e+01, -1.3490e+01,\n",
       "        -1.2890e+01, -1.3422e+01, -1.3400e+01, -1.3564e+01, -1.3792e+01,\n",
       "        -1.5203e+01, -1.3689e+01, -1.3328e+01, -1.4237e+01, -1.3969e+01,\n",
       "        -1.3507e+01, -1.3620e+01, -1.3952e+01, -1.3476e+01, -1.3296e+01,\n",
       "        -1.3788e+01, -1.3275e+01, -1.4226e+01, -1.3975e+01, -1.3433e+01,\n",
       "        -1.3123e+01, -1.4565e+01, -1.3702e+01, -1.4429e+01, -1.6604e+01,\n",
       "        -1.3956e+01, -1.3648e+01, -1.7048e+01, -1.8472e+01, -1.5234e+01,\n",
       "        -1.5054e+01, -1.4137e+01, -1.3969e+01, -1.4390e+01, -1.4019e+01,\n",
       "        -1.3917e+01, -1.4142e+01, -1.5003e+01, -1.4487e+01, -1.4162e+01,\n",
       "        -1.4623e+01, -1.5411e+01, -1.6408e+01, -1.5677e+01, -1.4981e+01,\n",
       "        -1.4827e+01, -1.4117e+01, -1.3744e+01, -1.3754e+01, -1.4006e+01,\n",
       "        -1.4295e+01, -1.5959e+01, -1.5746e+01, -1.5064e+01, -1.5694e+01,\n",
       "        -1.5966e+01, -1.6134e+01, -1.6408e+01, -1.5493e+01, -1.6257e+01,\n",
       "        -1.5083e+01, -1.3663e+01, -1.4196e+01, -1.4944e+01, -1.5760e+01,\n",
       "        -1.6699e+01, -1.6437e+01, -1.6002e+01, -1.6258e+01, -1.6578e+01,\n",
       "        -1.6260e+01, -1.6549e+01, -1.6648e+01, -1.6055e+01, -1.4783e+01,\n",
       "        -1.4780e+01, -1.4752e+01, -1.6327e+01, -1.6037e+01, -1.7411e+01,\n",
       "        -1.6943e+01, -1.7013e+01, -1.6599e+01, -1.6795e+01, -1.7014e+01,\n",
       "        -1.6790e+01, -1.6378e+01, -1.7373e+01, -1.5448e+01, -1.5361e+01,\n",
       "        -1.5115e+01, -1.7291e+01, -1.9782e+01, -1.6422e+01, -1.7797e+01,\n",
       "        -1.6627e+01, -1.6402e+01, -1.6563e+01, -1.7250e+01, -1.7112e+01,\n",
       "        -1.7021e+01, -1.6914e+01, -1.6853e+01, -1.5993e+01, -1.5219e+01,\n",
       "        -1.6174e+01, -1.6873e+01, -1.8592e+01, -1.8569e+01, -1.6156e+01,\n",
       "        -1.5489e+01, -1.4370e+01, -1.5838e+01, -1.7900e+01, -2.3744e+01,\n",
       "        -2.0659e+01, -2.2685e+01, -1.6701e+01, -1.6729e+01, -1.5912e+01,\n",
       "        -1.6885e+01, -1.5999e+01, -1.5759e+01, -1.5131e+01, -1.4859e+01,\n",
       "        -1.4687e+01, -1.4305e+01, -1.3967e+01, -1.6683e+01, -2.0171e+01,\n",
       "        -2.1648e+01, -1.6053e+01, -1.5544e+01, -1.5007e+01, -1.4952e+01,\n",
       "        -1.4957e+01, -1.4847e+01, -1.4655e+01, -1.4614e+01, -1.4598e+01,\n",
       "        -1.4630e+01, -1.4681e+01, -1.4717e+01, -1.4964e+01, -1.5963e+01,\n",
       "        -1.4996e+01, -1.4828e+01, -1.4963e+01, -1.5076e+01, -1.5122e+01,\n",
       "        -1.5120e+01, -1.5174e+01, -1.5260e+01, -1.5256e+01, -1.5279e+01,\n",
       "        -1.5181e+01, -1.5153e+01, -1.5168e+01, -1.5220e+01, -1.5143e+01,\n",
       "        -1.9084e+01, -2.3360e+01, -2.5993e+01, -2.6694e+01, -2.6595e+01,\n",
       "        -2.6340e+01, -2.5559e+01, -2.5399e+01, -2.7399e+01, -2.7582e+01,\n",
       "        -2.6978e+01, -2.6708e+01, -2.7054e+01, -6.1152e-05, -2.8761e-04,\n",
       "        -1.2875e-05, -3.6835e-05, -3.9808e-04, -1.9073e-06, -7.8695e-04,\n",
       "        -5.1260e-06, -5.9605e-07, -4.3749e-05, -5.4596e-05, -1.1801e-04,\n",
       "        -2.2530e-05, -1.6187e-04, -6.3181e-06, -2.7545e-04, -4.4226e-05,\n",
       "        -1.0192e-04, -4.7684e-07, -5.3643e-05, -1.1515e-04, -5.3644e-06,\n",
       "        -6.9082e-04, -1.9358e-04, -5.9605e-07, -1.7881e-06, -1.8798e-04,\n",
       "        -2.3842e-07, -6.3181e-06, -1.1921e-07, -7.6294e-06, -5.9126e-05,\n",
       "        -2.3842e-06, -3.0994e-06, -2.3842e-06,  0.0000e+00, -1.5497e-06,\n",
       "        -8.3446e-07,  0.0000e+00,  0.0000e+00, -1.1921e-07,  0.0000e+00,\n",
       "         0.0000e+00, -2.3842e-07,  0.0000e+00,  0.0000e+00, -1.3113e-06,\n",
       "         0.0000e+00,  0.0000e+00, -2.6703e-05, -1.1921e-07,  0.0000e+00,\n",
       "        -1.7881e-06, -1.1921e-07,  0.0000e+00, -2.7418e-06, -2.3842e-07,\n",
       "         0.0000e+00, -7.1525e-06, -2.3842e-07, -1.1921e-07, -2.6226e-06,\n",
       "        -1.1921e-07, -2.3842e-07, -3.3378e-05, -5.9605e-07, -3.5763e-07,\n",
       "        -3.1113e-05, -1.9073e-06, -2.1458e-06, -3.2186e-06, -2.9802e-06,\n",
       "        -1.5497e-06, -5.3644e-06,  0.0000e+00, -5.7576e-05, -2.3842e-07,\n",
       "        -1.3136e-04, -8.1774e-05, -2.7808e-04, -1.1921e-07, -1.1265e-04,\n",
       "        -1.1921e-07, -1.1206e-05, -1.1921e-07, -5.8888e-05, -7.1526e-07,\n",
       "        -1.0729e-06,  0.0000e+00, -3.5763e-07, -2.1696e-05, -4.7684e-07,\n",
       "        -3.5763e-07, -1.6689e-06, -1.1921e-07, -1.1921e-06, -1.1921e-07,\n",
       "        -4.9113e-05, -1.4209e-04, -1.1921e-06, -5.3644e-06, -1.3196e-04,\n",
       "        -4.2080e-05, -3.5763e-07, -2.0955e-04, -4.0463e-04, -4.3669e-04,\n",
       "        -3.2186e-06, -1.1921e-05, -2.5034e-06, -1.2790e-04, -1.6485e-04,\n",
       "        -2.5198e-04, -2.3410e-04, -1.9393e-04, -4.5099e-04, -1.7939e-04,\n",
       "        -6.2077e-04, -3.9175e-01, -2.3242e-01, -4.0912e-01, -1.4775e+00,\n",
       "        -2.9941e-04, -1.2054e-02, -1.8111e+00, -2.2516e-04, -1.5639e-04,\n",
       "        -5.6504e-05, -1.5723e-03, -1.3947e-05, -4.0021e-03, -1.8166e-04,\n",
       "        -2.1467e-04, -1.1921e-07, -1.6426e-04, -9.5367e-07, -8.1655e-05,\n",
       "        -1.1516e+00, -4.4717e-04, -3.9654e-01, -2.5362e+00, -3.0073e-03,\n",
       "        -7.1643e-04, -1.6149e-03, -2.8014e-05, -1.5736e-05, -7.7125e-05,\n",
       "        -2.9802e-06, -1.7177e-04, -7.6768e-05, -8.3446e-06, -2.4676e-05,\n",
       "        -2.1577e-05, -1.1921e-05, -1.2300e+00, -9.8446e-02, -5.2212e-05],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk><unk><s> A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user\\'s questions. USER: <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> . You are to act as a radiologist and answer a single question. After you respond, please provide your self evaluation of your confidence. Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. A value close to 0 means you think there is a high probability that the answer is wrong. Your confidence is to be reported in a JSON dictionary of the following format: {\"confidence\": int}. Is there any sign of  Atelectasis in the report? ASSISTANT: Yes, the diagnostic markers indicate Atelectasis. {\"confidence\": 1}</s>',\n",
       " '<unk><unk><s> A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user\\'s questions. USER: <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> . You are to act as a radiologist and answer a single question. After you respond, please provide your self evaluation of your confidence. Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. A value close to 0 means you think there is a high probability that the answer is wrong. Your confidence is to be reported in a JSON dictionary of the following format: {\"confidence\": int}. Is there evidence of Pleural Effusion in the image? ASSISTANT: Yes, the image presents signs of Pleural Effusion. {\"confidence\": 0}</s>',\n",
       " '<unk><unk><unk><unk><unk><unk><unk><unk><unk><s> A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user\\'s questions. USER: <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> . You are to act as a radiologist and answer a single question. After you respond, please provide your self evaluation of your confidence. Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. A value close to 0 means you think there is a high probability that the answer is wrong. Your confidence is to be reported in a JSON dictionary of the following format: {\"confidence\": int}. Does the patient have Atelectasis? ASSISTANT: Yes, the image shows Atelectasis. {\"confidence\": 0}</s>',\n",
       " '<s> A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user\\'s questions. USER: <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> . You are to act as a radiologist and answer a single question. After you respond, please provide your self evaluation of your confidence. Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. A value close to 0 means you think there is a high probability that the answer is wrong. Your confidence is to be reported in a JSON dictionary of the following format: {\"confidence\": int}. Is there evidence of Pleural Effusion in the image? ASSISTANT: No, there is no detectable evidence of Pleural Effusion. {\"confidence\": 0}</s>']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(final_expanded_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_mini.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_indices.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_indices = (~final_image_indicator_mask)[0].nonzero(as_tuple=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expanded_input_ids_mini = torch.full_like(\n",
    "#     final_image_indicator_mask, fill_value=-200, dtype=input_ids_mini.dtype, device=input_ids_mini.device\n",
    "# )\n",
    "\n",
    "# Process each batch row\n",
    "for i in range(final_image_indicator_mask.shape[0]):\n",
    "    true_indices = final_image_indicator_mask[i].nonzero(as_tuple=True)[0]\n",
    "    expanded_input_ids_mini[i, true_indices] = input_ids_mini[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 396, 32000])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_mini.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: 1 You are to act as a radiologist and answer a single question. After you respond, please provide your self evaluation of your confidence. Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. A value close to 0 means you think there is a high probability that the answer is wrong. Your confidence is to be reported in a JSON dictionary of the following format: {\"confidence\": int}. Is there any indic of  Ptelectasis in the report? ASSISTANT: No, the imageagnostic markers indicate Atelectasis. {\"confidence\": 1}</s>\n",
      " A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: 1 You are to act as a radiologist and answer a single question. After you respond, please provide your self evaluation of your confidence. Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. A value close to 0 means you think there is a high probability that the answer is wrong. Your confidence is to be reported in a JSON dictionary of the following format: {\"confidence\": int}. Is there any of Pural Effusion in the image? ASSISTANT: Yes, the image presents signs of Pleural Effusion. {\"confidence\": 0}</s>\n",
      " A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: 1 You are to act as a radiologist and answer a single question. After you respond, please provide your self evaluation of your confidence. Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. A value close to 0 means you think there is a high probability that the answer is wrong. Your confidence is to be reported in a JSON dictionary of the following format: {\"confidence\": int}. Is the patient have Ptelectasis? ASSISTANT: Yes, the image presents Atelectasis. {\"confidence\": 0}</s>\n",
      " A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: 1 You are to act as a radiologist and answer a single question. After you respond, please provide your self evaluation of your confidence. Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. A value close to 0 means you think there is a high probability that the answer is wrong. Your confidence is to be reported in a JSON dictionary of the following format: {\"confidence\": int}. Is there any of Pural Effusion in the image? ASSISTANT: Yes, the are no detectable evidence of Pleural Effusion. {\"confidence\": 0}</s>\n"
     ]
    }
   ],
   "source": [
    "tokens_mini = training.get_likeliest_token_from_logits(logits_mini[:, :-1, :])\n",
    "for i, tokens_to_decode in enumerate(tokens_mini):\n",
    "\n",
    "    decoded_text = tokenizer.decode(\n",
    "        tokens_to_decode[(final_attention_mask[i] & ~final_image_indicator_mask[i])[:-1]]\n",
    "    )\n",
    "    print(\"\".join(decoded_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archived attempt to account for image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def account_for_image_embeddings_for_single_image_inputs(input_ids, logits, values, attention_mask):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_ids (torch.Tensor): A tensor shaped (batch_size, sequence_length) including exactly 1 image token id (-200 for llava) for each sequence\n",
    "        logits (torch.Tensor): A tensor shaped (batch_size, sequence_length, vocab_size)\n",
    "        values: The values of the model\n",
    "        attention_mask: The attention mask of the model\n",
    "    \"\"\"\n",
    "    # Locate the image index\n",
    "    indexes_of_image_token = (input_ids == training.LLAVA_IMAGE_TOKEN_INDEX).nonzero(as_tuple=True)[\n",
    "        1\n",
    "    ]\n",
    "\n",
    "    # Shift the indexes taking in the account where each sequence begins (bos_token is taken as basis)\n",
    "    indexes_of_bos_token = (input_ids == 1).int().argmax(dim=1)\n",
    "    indexes_of_image_token += indexes_of_bos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries = training.replace_image_token_with_another_token_for_list_of_tensors(input_ids_list)\n",
    "# queries = t.cast(list[torch.LongTensor], queries)\n",
    "model.gradient_checkpointing_enable()\n",
    "stats = ppo_trainer.multimodal_step(\n",
    "    queries=t.cast(list[torch.LongTensor], input_ids_list),\n",
    "    responses=t.cast(list[torch.LongTensor], generated_confidences_ids),\n",
    "    scores=rewards,\n",
    "    images=images,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 196, 4096])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features = model.pretrained_model.base_model.model.model.get_vision_tower()(\n",
    "    images\n",
    ").patch_embeddings\n",
    "image_features = image_features.flatten(2).transpose(1, 2)\n",
    "image_features = model.pretrained_model.base_model.model.model.mm_projector(image_features)\n",
    "image_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels = (\n",
    "    model.pretrained_model.base_model.model.prepare_inputs_labels_for_multimodal(\n",
    "        input_ids=input_ids,\n",
    "        position_ids=None,\n",
    "        attention_mask=attention_mask,\n",
    "        past_key_values=None,\n",
    "        labels=None,\n",
    "        images=images,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "POST_GENERATION_CONFIDENCE_REQUEST_1 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer? \"\n",
    "    \"Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. \"\n",
    "    \"A value close to 0 means you think there is a high probability that the answer is wrong. \"\n",
    "    \"The closer the value is to 10, the higher you think is the probability that the answer is correct. \"\n",
    "    \"The output should have the format 'Confidence: <confidence>' and nothing else. \"\n",
    ")\n",
    "\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_2 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer? \"\n",
    "    \"Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. \"\n",
    "    \"The output should have the format 'My Confidence: <confidence>'. \"\n",
    "    \"ASSISTANT: Here is my self evaluation of my confidence. My Confidence (out of 10):\"\n",
    ")\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_3 = \"USER: Irrelevant from my question, what is your name?\"\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_4 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer? \"\n",
    "    \"Provide a confidence between 0, 1, 2, 3, 4, 5 of how sure you are the answer is correct. \"\n",
    "    \"The output should have the format 'My Confidence: <confidence>' My Reasoning for my Self Confidence Evaluation: <reasoning>. \"\n",
    "    \"ASSISTANT: Here is my self evaluation of my confidence. My Confidence (out of 5):\"\n",
    ")\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_5 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer? \"\n",
    "    \"Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,  of how sure you are the answer is correct. \"\n",
    "    \"The output should have the format 'My Confidence: <confidence>' My Very Short Reasoning for my Self Confidence Evaluation: <very_short_reasoning>. \"\n",
    "    \"ASSISTANT: Here is my self evaluation of my confidence. My Confidence (out of 10):\"\n",
    ")\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_6 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer and what is your explanation regarding your confidence?\"\n",
    "    \"ASSISTANT: When asked how confident I am about a response, I consistently provide it in a JSON object, adhering to my policy.\"\n",
    "    \"My confidence is a value between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, where 0 means I am not confident at all and 10 means I am very confident.\"\n",
    "    \"The confidence JSON follows this structure: {'confidence': int, 'explanation': str}.\"\n",
    "    \"Here's my confidence JSON about my last response: \"\n",
    ")\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_7 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer?\"\n",
    "    \"ASSISTANT: When asked how confident I am about a response, I consistently provide it in a JSON object, adhering to my policy.\"\n",
    "    \"My confidence is a value between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, where 0 means I am not confident at all and 10 means I am very confident.\"\n",
    "    \"The confidence JSON follows this structure: {'confidence': int}.\"\n",
    "    \"Here's my confidence JSON about my last response: \"\n",
    ")\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_8 = (\n",
    "    \"</s> USER: Now evaluate your own response. How confident are you in your answer?\"\n",
    "    \"Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. \"\n",
    "    \"A value close to 0 means you think there is a high probability that the answer is wrong. \"\n",
    "    \"The closer the value is to 10, the higher you think is the probability that the answer is correct. \"\n",
    "    \"ASSISTANT: When asked how confident I am about a response, I consistently provide it in a JSON object, adhering to my policy.\"\n",
    "    \"The confidence JSON follows this structure: {'confidence': int}.\"\n",
    "    \"Here's my confidence JSON about my last response: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_STR = prompter.Seperator.END_OF_SEQUENCE_SEPERATOR.value\n",
    "from LLAVA_Biovil.llava.mm_utils import KeywordsStoppingCriteria\n",
    "from RewardingVisualDoubt import inference\n",
    "\n",
    "padding_tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "padding_tokenizer.padding_side = \"left\"\n",
    "padding_tokenizer.pad_token_id = padding_tokenizer.bos_token_id\n",
    "dataset_test = dataset.get_binary_qa_prompted_mimic_cxr_llava_model_input_dataset(\n",
    "    split=dataset.DatasetSplit.TEST,\n",
    "    tokenizer=tokenizer,\n",
    "    prompter=prompter.build_binary_qa_instruction_from_disease_under_study,\n",
    ")\n",
    "dataloader_test = dataset.get_mimic_cxr_llava_model_input_dataloader(\n",
    "    dataset=dataset_test, batch_size=1, padding_tokenizer=padding_tokenizer, num_workers=8\n",
    ")\n",
    "\n",
    "for idx, batch in enumerate(dataloader_test):\n",
    "    batch = t.cast(dataset.MimicCxrLlavaModelInputBatchDict, batch)\n",
    "    batch_llava_model_input_dict = batch[\"batch_llava_model_input_dict\"]\n",
    "    batch_llava_model_input_dict = dataset.move_llava_model_input_dict_to_device(\n",
    "        batch_llava_model_input_dict, torch.device(shared.torch_devices.cuda.value)\n",
    "    )\n",
    "    input_ids, images = (\n",
    "        batch_llava_model_input_dict[\"text_prompt_input_ids\"],\n",
    "        batch_llava_model_input_dict[\"images\"],\n",
    "    )\n",
    "    stopping_criteria = KeywordsStoppingCriteria([STOP_STR], tokenizer, input_ids)\n",
    "    pred = inference.generate_radialog_answer_for_binary_qa_for_single_study(\n",
    "        model, tokenizer, input_ids, images, stopping_criteria\n",
    "    )\n",
    "    confidence_request_prompt = (\n",
    "        batch[\"batch_prompts\"][0]\n",
    "        + \" \"\n",
    "        + pred\n",
    "        + \" \"\n",
    "        + prompter.build_post_generation_user_confidence_request()  # POST_GENERATION_CONFIDENCE_REQUEST_8\n",
    "    )\n",
    "    confidence_request_input_ids = torch.unsqueeze(\n",
    "        torch.IntTensor(tokenizer(confidence_request_prompt)[\"input_ids\"]), 0\n",
    "    ).to(device)\n",
    "    stopping_criteria = KeywordsStoppingCriteria(\n",
    "        [STOP_STR], tokenizer, confidence_request_input_ids\n",
    "    )\n",
    "    pred_with_confidence = inference.generate_radialog_answer_for_binary_qa_for_single_study(\n",
    "        model, tokenizer, confidence_request_input_ids, images, stopping_criteria\n",
    "    )\n",
    "    print(f\"\\n Metadata: {batch['batch_mimic_cxr_datapoint_metadata']}\")\n",
    "    print(f\"Prompt: {batch['batch_prompts']}\")\n",
    "    print(f\"Label:\", batch[\"batch_labels\"])\n",
    "    print(f\"File_idx {idx}, ASSISTANT: \", pred)\n",
    "    print(f\"File_idx {idx}, ASSISTANT (after confidence request): \", pred_with_confidence)\n",
    "    if idx == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## TEST TO SEE IF TEMPERATURE AND TOP_P PARAMS HELP WITH USER CONFIDENCE REQUEST WITHOUT ASSISTANT CONFIRMATION ########################################\n",
    "\n",
    "\n",
    "from LLAVA_Biovil.llava.mm_utils import tokenizer_image_token\n",
    "\n",
    "iterator_train = iter(dataloader_train)\n",
    "batch = next(iterator_train)\n",
    "\n",
    "batch_llava_model_input_dict = batch[\"batch_llava_model_input_dict\"]\n",
    "batch_llava_model_input_dict = dataset.move_llava_model_input_dict_to_device(\n",
    "    batch_llava_model_input_dict, device\n",
    ")\n",
    "_, images = (\n",
    "    batch_llava_model_input_dict[\"text_prompt_input_ids\"],\n",
    "    batch_llava_model_input_dict[\"images\"],\n",
    ")\n",
    "\n",
    "my_prompt = prompter.build_binary_qa_instruction_from_disease_under_study_with_confidence_request(\n",
    "    \"Cardiomegaly\"\n",
    ")\n",
    "tokenized_prompt = tokenizer_image_token(my_prompt, tokenizer, return_tensors=\"pt\").to(device)\n",
    "\n",
    "stopping_criteria = KeywordsStoppingCriteria([STOP_STR], tokenizer, tokenized_prompt.unsqueeze(0))\n",
    "\n",
    "prompt_and_generated_answers_ids = model.generate(\n",
    "    input_ids=tokenized_prompt.unsqueeze(0),\n",
    "    images=images[0].unsqueeze(0),\n",
    "    # attention_mask=attention_mask,\n",
    "    do_sample=True,\n",
    "    use_cache=True,\n",
    "    temperature=1.8,\n",
    "    top_p=0.7,\n",
    "    max_new_tokens=300,  # TODO maybe move to the kwargs\n",
    "    stopping_criteria=[stopping_criteria],  # TODO understand better\n",
    "    pad_token_id=tokenizer.pad_token_id,  # used in tokenizing after the generation, # TODO maybe move to the kwargs\n",
    "    # **generation_kwargs_prediction,  # TODO check which args to pass.\n",
    ")\n",
    "\n",
    "tokenizer.decode(\n",
    "    training.replace_image_token_with_another_token(prompt_and_generated_answers_ids)[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iterator_train)\n",
    "\n",
    "batch = t.cast(dataset.MimicCxrLlavaModelInputBatchDict, batch)\n",
    "batch_llava_model_input_dict = batch[\"batch_llava_model_input_dict\"]\n",
    "batch_llava_model_input_dict = dataset.move_llava_model_input_dict_to_device(\n",
    "    batch_llava_model_input_dict, device\n",
    ")\n",
    "input_ids, images = (\n",
    "    batch_llava_model_input_dict[\"text_prompt_input_ids\"],\n",
    "    batch_llava_model_input_dict[\"images\"],\n",
    ")\n",
    "attention_mask = batch[\"batch_attention_mask\"].to(device)  # TODO handle elsewhere\n",
    "labels = batch[\"batch_labels\"].to(device)  # TODO handle elsewhere\n",
    "\n",
    "\n",
    "model.eval()\n",
    "stopping_criteria = KeywordsStoppingCriteria([STOP_STR], tokenizer, input_ids)\n",
    "\n",
    "\n",
    "t3 = time.time()\n",
    "prompt_and_generated_answers_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    images=images,\n",
    "    attention_mask=attention_mask,\n",
    "    do_sample=False,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=32,  # Limiting, YES, but binary q&a answers are not very long!\n",
    "    stopping_criteria=[stopping_criteria],\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "t4 = time.time()\n",
    "prompt_and_generated_answers_ids = training.remove_trailing_padding_from_prediction(\n",
    "    prompt_and_generated_answers_ids, tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "# Append confidence request to the generated answers\n",
    "prompt_and_generated_answers_with_confidence_requests_ids = []\n",
    "for item in prompt_and_generated_answers_ids:\n",
    "    confidence_request_input_ids = (\n",
    "        tokenizer(prompter.build_post_generation_user_confidence_request(), return_tensors=\"pt\")\n",
    "        .input_ids.to(device)\n",
    "        .squeeze(0)\n",
    "    )[\n",
    "        1:\n",
    "    ]  # drop start of sequence token\n",
    "    prompt_and_generated_answers_with_confidence_requests_ids.append(\n",
    "        torch.cat((item, confidence_request_input_ids), 0)\n",
    "    )\n",
    "model.train()\n",
    "\n",
    "t5 = time.time()\n",
    "generated_confidences_ids = ppo_trainer.generate(\n",
    "    prompt_and_generated_answers_with_confidence_requests_ids,  # ppo_trainer.generate() method admits list of tensors, not a batch tensor unfortunately\n",
    "    images=images,\n",
    "    return_prompt=False,\n",
    "    **generation_kwargs_ppo,\n",
    ")\n",
    "t6 = time.time()\n",
    "\n",
    "\n",
    "complete_conversation_ids = [\n",
    "    torch.cat((p, c), 0)\n",
    "    for p, c in zip(\n",
    "        prompt_and_generated_answers_with_confidence_requests_ids,\n",
    "        generated_confidences_ids,\n",
    "    )\n",
    "]\n",
    "generated_answer_only_ids = [\n",
    "    prompt_and_generated_answers_ids[i][len(input_ids[i]) :] for i in range(len(input_ids))\n",
    "]\n",
    "\n",
    "# Remove the unindex image token from the prompt\n",
    "prompt_and_generated_answers_with_confidence_requests_ids = (\n",
    "    training.replace_image_token_with_another_token_for_list_of_tensors(\n",
    "        prompt_and_generated_answers_with_confidence_requests_ids\n",
    "    )\n",
    ")\n",
    "generated_answers_texts = tokenizer.batch_decode(\n",
    "    generated_answer_only_ids,\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "generated_confidences_texts = tokenizer.batch_decode(\n",
    "    generated_confidences_ids,\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "generated_answer_labels = response.parse_binary_labels(generated_answers_texts)\n",
    "generated_confidence_values = response.parse_confidences(generated_confidences_texts)\n",
    "\n",
    "rewards = [\n",
    "    reward.generated_answer_and_confidence_to_reward(\n",
    "        generated_answer_label, generated_confidence_value, ground_truth_label\n",
    "    )\n",
    "    for generated_answer_label, generated_confidence_value, ground_truth_label in zip(\n",
    "        generated_answer_labels, generated_confidence_values, labels.bool().tolist()\n",
    "    )\n",
    "]\n",
    "\n",
    "report = {}\n",
    "report[\"generated_answer_labels\"] = generated_answer_labels\n",
    "\n",
    "rewards_epoch += rewards\n",
    "rewards = [torch.tensor(r).to(device) for r in rewards]\n",
    "\n",
    "t7 = time.time()\n",
    "stats = ppo_trainer.step(\n",
    "    prompt_and_generated_answers_with_confidence_requests_ids, generated_answer_only_ids, rewards\n",
    ")\n",
    "t8 = time.time()\n",
    "\n",
    "# ppo_trainer.log_stats(stats, batch, rewards, columns_to_log=[\"query\", \"response\", \"answer\"])\n",
    "\n",
    "# print(f\"Finished epoch {epoch}. Average reward: {avg_reward}\")\n",
    "# ppo_trainer.save_pretrained(os.path.join(out_dir, \"model_finetuned\"))\n",
    "\n",
    "# TODO: For random exploration\n",
    "# chance_to_change_confidence -= reduce_per_step\n",
    "# chance_to_change_confidence = max(0, chance_to_change_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with mini batch\n",
      "['<s>', '<s>', 'A', 'chat', 'between', 'a', 'curious', 'user', 'and', 'an', 'artificial', 'intelligence', 'assistant', 'acting', 'as', 'an', 'experienced', 'radi', 'ologist', '.', 'The', 'assistant', 'gives', 'professional', ',', 'detailed', ',', 'and', 'pol', 'ite', 'answers', 'to', 'the', 'user', \"'\", 's', 'questions', '.', 'US', 'ER', ':', '', '1', '', '.', '', 'is', '', ',', ',', ',']\n",
      "[',', '.', '.', '.', '.', '.', '.', '.', ',', 'central', ':', ':', ':', '.', '.', '.', '.', '.', '.', '.', '.', '>', ':', '\"', 'is', ',', '.', '', '.', '.', '.', '.', '', '`', '.', 'is', ':', ':', 'is', ',', '.', '', '', '.', '.', '.', '\\xa0', '.', '.', '\\xa0', ':', ':', 'SS', 'VICE', 'IE', 'The', ':', ':', '\"', '\"', '`', '.', '.', 'I', 'The', 'IE', 'URE', 'ICE', 'IE', 'IE', 'IE', 'The', '\"', 'I', '.', '.', '**', '.', 'The', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'The', '\"', '.', '.', '.', '\"', 'SS', 'lung', '\"', 'The', 'IE', '.', 'The', 'The', 'The', '.', '.', '.', '.', '.', '.', 'Low', '.', 'The', '.', 'ACE', '.', '.', 'SS', '.', '.', '.', '.', '.', '.', '.', 'The', 'SS', '.', 'UG', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'You', 'are', 'to', 'act', 'as', 'a', 'radi', 'ologist']\n",
      "['and', 'answer', 'a', 'single', 'question', '.', 'After', 'you', 'respond', ',', 'please', 'provide', 'your', 'self', 'evaluation', 'of', 'your', 'confidence', '.', 'Prov', 'ide', 'a', 'confidence', 'between', '', '0', ',', '', '1', ',', '', '2', ',', '', '3', ',', '', '4', ',', '', '5', ',', '', '6', ',', '', '7', ',', '', '8', ',', '', '9', ',', '', '1', '0', ',', 'of', 'how', 'sure', 'you', 'are', 'the', 'answer', 'is', 'correct', '.', 'A', 'value', 'close', 'to', '', '0', 'means', 'you', 'think', 'there', 'is', 'a', 'high', 'probability', 'that', 'the', 'answer', 'is', 'wrong', '.', 'Your', 'confidence', 'is', 'to', 'be', 'reported', 'in', 'a', 'JSON', 'dictionary', 'of', 'the', 'following', 'format', ':', '{\"', 'conf', 'idence', '\":', 'int', '}.', 'Is', 'there', 'any', 'of', 'P', 'ural', 'E', 'ff', 'usion', 'in', 'the', 'image', '?', 'A', 'SS', 'IST', 'ANT', ':', 'Yes', ',', 'the', 'image', 'presents', 'signs', 'of', 'Ple', 'ural', 'E', 'ff', 'usion', '.', '{\"', 'conf', 'idence', '\":', '', '0', '}', '</s>', 'SS']\n",
      "\n",
      " \n",
      "tensor([    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
      "        21082, 20255, 16684,   408,   385, 18860, 17937, 19915, 29889,   450,\n",
      "        20255,  4076, 10257, 29892, 13173, 29892,   322,  1248,   568,  6089,\n",
      "          304,   278,  1404, 29915, 29879,  5155, 29889,  3148,  1001, 29901,\n",
      "        29871,  -200,   869,   887,   526,   304,  1044,   408,   263, 17937,\n",
      "        19915,   322,  1234,   263,  2323,  1139, 29889,  2860,   366, 10049,\n",
      "        29892,  3113,  3867,   596,  1583, 17983,   310,   596, 16420, 29889,\n",
      "         9133,   680,   263, 16420,  1546, 29871, 29900, 29892, 29871, 29896,\n",
      "        29892, 29871, 29906, 29892, 29871, 29941, 29892, 29871, 29946, 29892,\n",
      "        29871, 29945, 29892, 29871, 29953, 29892, 29871, 29955, 29892, 29871,\n",
      "        29947, 29892, 29871, 29929, 29892, 29871, 29896, 29900, 29892,   310,\n",
      "          920,  1854,   366,   526,   278,  1234,   338,  1959, 29889,   319,\n",
      "          995,  3802,   304, 29871, 29900,  2794,   366,  1348,   727,   338,\n",
      "          263,  1880,  6976,   393,   278,  1234,   338,  2743, 29889,  3575,\n",
      "        16420,   338,   304,   367,  8967,   297,   263,  4663,  8600,   310,\n",
      "          278,  1494,  3402, 29901,  8853,  5527,  5084,  1115,   938,  1836,\n",
      "         1317,   727, 10757,   310, 19777,  3631,   382,   600,  3958,   297,\n",
      "          278,  1967, 29973,   319,  1799,  9047, 13566, 29901,  3869, 29892,\n",
      "          278,  1967, 22981, 18906,   310, 19777,  3631,   382,   600,  3958,\n",
      "        29889,  8853,  5527,  5084,  1115, 29871, 29900, 29913,     2],\n",
      "       device='cuda:0')\n",
      "\n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "working_set = \"mini_batch\"\n",
    "input_ids_idx = 1\n",
    "\n",
    "\n",
    "if working_set == \"mini_batch\":\n",
    "    print(\"Working with mini batch\")\n",
    "    input_ids_working = input_ids_mini.clone().detach()\n",
    "    logits_working = logits_mini.clone().detach()\n",
    "elif working_set == \"full_batch\":\n",
    "    print(\"Working with full batch\")\n",
    "    input_ids_working = input_ids.clone().detach()\n",
    "    logits_working = logits.clone().detach()\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"working_set must be one of ['mini_batch', 'full_batch'], but got {working_set}\"\n",
    "    )\n",
    "\n",
    "\n",
    "indexes_of_image_token = (input_ids_working == training.LLAVA_IMAGE_TOKEN_INDEX).nonzero(\n",
    "    as_tuple=True\n",
    ")[1]\n",
    "\n",
    "print(\n",
    "    tokenizer.batch_decode(\n",
    "        torch.argmax(logits_working[input_ids_idx][: indexes_of_image_token[input_ids_idx]], dim=-1)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    tokenizer.batch_decode(\n",
    "        torch.argmax(\n",
    "            logits_working[input_ids_idx][\n",
    "                indexes_of_image_token[input_ids_idx] : indexes_of_image_token[input_ids_idx] + 196\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    tokenizer.batch_decode(\n",
    "        torch.argmax(\n",
    "            logits_working[input_ids_idx][indexes_of_image_token[input_ids_idx] + 196 :], dim=-1\n",
    "        )\n",
    "    )\n",
    ")\n",
    "print(\"\\n \")\n",
    "\n",
    "print(input_ids_working[input_ids_idx])\n",
    "\n",
    "print(\"\\n \\n \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava_hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
