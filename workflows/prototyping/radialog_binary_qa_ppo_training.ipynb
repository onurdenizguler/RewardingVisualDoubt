{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO sampling logic (undersample)\n",
    "# TODO binary qa prompt and co need to vary between a few samples\n",
    "# TODO determine hyperparams\n",
    "# TODO arg parsing\n",
    "# TODO dataloader num workers set to default\n",
    "\n",
    "# SOME NOTES\n",
    "# PPO_TRAINER AUTOMATICALLY PADS THE INPUTS BY TOKENIZER.PADDING_SIDE AND TOKENIZER.PADDING_TOKEN_ID\n",
    "# Uh-oh, because ppo termination token is set as the eos_seq_token, it'll stop when it sees a left padded sequence\n",
    "# Skipping random exploration for now\n",
    "\n",
    "\n",
    "# BATCH TIMING\n",
    "# A batch of 8 samples take around 1-1.5-2-3min to process in a train step (so around 400 samples per hour is trainable, every 50th batch, we save a checkpoint, and do val)\n",
    "# Lets save a checkpoint every half an hour or so\n",
    "# Give validation around 15 mins => 100 samples or so\n",
    "# Validation is around 8k so it'll be 1000 batches (1000*1.5 min = 25 hours)\n",
    "# len(dataset_eval) = 8737"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204b780bc8a141268a39d6080890ffba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 69 files:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% Set script for interactive development and import modules\n",
    "from RewardingVisualDoubt import infrastructure, training\n",
    "\n",
    "infrastructure.make_ipython_reactive_to_changing_codebase()\n",
    "infrastructure.supress_known_warnings()\n",
    "\n",
    "import pathlib as path\n",
    "import typing as t\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import accelerate\n",
    "import dataclasses\n",
    "import functools\n",
    "\n",
    "# from LLAVA_Biovil.llava.mm_utils import KeywordsStoppingCriteria\n",
    "from trl import PPOConfig, PPOTrainer\n",
    "\n",
    "from RewardingVisualDoubt import dataset, prompter, shared, vllm, response, reward\n",
    "from RewardingVisualDoubt import training as training\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"da3cb086bbc110c16cbc5ba4c284a19b0b461710\"\n",
    "\n",
    "from LLAVA_Biovil.llava.mm_utils import KeywordsStoppingCriteria\n",
    "\n",
    "STOP_STR = prompter.Seperator.END_OF_SEQUENCE_SEPERATOR.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding LoRA adapters and value head to the model for PPO training using Radialog Lora Weights path: /home/guests/deniz_gueler/repos/RewardingVisualDoubt/workflows/training_checkpoints/best_model_epoch0_step179.pth/adapter_model.bin\n",
      "Loading the model in non-trainable mode...\n",
      "Precision: 4bit quantized\n",
      "Model base:  liuhaotian/llava-v1.5-7b\n",
      "Loading LLaVA from base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9748799fb741cf8f1f055c713c2368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43558eace68545c3bd1bb72eb22e99ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading additional LLaVA weights...\n",
      "Downloading https://cdn-lfs.hf.co/repos/63/2f/632fbb459426d5c3e8e64aa8be737ccf0c8ba541980f23a79ecf1ab6e87df8b4/b2399d73dc2a68b9f3a1950e864ae0ecd24093fb07aa459d7e65807ebdc0fb77?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27biovil_t_image_model_proj_size_128.pt%3B+filename%3D%22biovil_t_image_model_proj_size_128.pt%22%3B&Expires=1744374786&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NDM3NDc4Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy82My8yZi82MzJmYmI0NTk0MjZkNWMzZThlNjRhYThiZTczN2NjZjBjOGJhNTQxOTgwZjIzYTc5ZWNmMWFiNmU4N2RmOGI0L2IyMzk5ZDczZGMyYTY4YjlmM2ExOTUwZTg2NGFlMGVjZDI0MDkzZmIwN2FhNDU5ZDdlNjU4MDdlYmRjMGZiNzc%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=GLWo1SvfxTaTIglO1ojgVZV%7EP5QN1hGo6mzIekryAgIPI%7EcwvdFKNgSeuMSyNDKhUsbalrAh1Jck%7ESQDbD1b1pqj4-THEZc-5lExlTlafbDI2jcWRJwPLLXxuwxNKatKB8EtAoutBJkQ-3uxNePRX5m-wy1CgvEIVUsUOZL03sXN5YU1OPuPCYX8d048lvbAX2Ra7WKCrg5YjgdvOLRgTmJUX6xuMHw9lneIzfpfazp3QM5x0KdvcVlTrZSwtyjEkG3yc9Aakhnk09GTUgM4ptizcuybxAlgWetHKIlUzdafD7pEoj%7ED64kF2Z3LNaidr9jJ4lfDX6SJQ3SeQSrBDw__&Key-Pair-Id=K3RPWS32NSSJCE to /tmp/biovil_t_image_model_proj_size_128.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109745561/109745561 [00:00<00:00, 218901060.96it/s]\n",
      "WARNING:root:The `device_map` argument is not provided. We will override the device_map argument. to set the entire model on the current device. If you want to set the model on multiple devices, please provide a custom `device_map` argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded additional vision tower weights...\n",
      "Adding pretrained RaDialog LoRA adapters and value head to the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the datasets and the dataloaders...\n",
      "Loading mimic_cxr_df from cache\n",
      "Loading balanced_binary_qa_mimic_cxr_df from cache\n",
      "Loading mimic_cxr_df from cache\n",
      "Loading balanced_binary_qa_mimic_cxr_df from cache\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "######################################## 0. Define the environment ########################################\n",
    "\n",
    "DEFAULT_BATCH_SIZE = 16\n",
    "DEFAULT_OUTPUT_DIR = path.Path(\"output\")\n",
    "\n",
    "\n",
    "batch_size = DEFAULT_BATCH_SIZE\n",
    "\n",
    "device_str = (\n",
    "    shared.torch_devices.cuda.value if torch.cuda.is_available() else shared.torch_devices.cpu.value\n",
    ")\n",
    "device = torch.device(device_str)\n",
    "\n",
    "######################################## 1. Load the model and tokenizer ########################################\n",
    "\n",
    "model = vllm.load_pretrained_llava_model_for_ppo_training(\n",
    "    device_str=device_str,\n",
    "    precision=\"4bit\",\n",
    "    radialog_lora_weights_path=vllm.RadialogLoraWeightsPath.BINARY_QA_WITH_CONFIDENCE_SFT.value,\n",
    ")\n",
    "\n",
    "tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "padding_tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "padding_tokenizer.padding_side = \"left\"  # Why? Because: A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "\n",
    "\n",
    "######################################## 2. Load the datasets and the dataloaders ########################################\n",
    "\n",
    "print(\"Loading the datasets and the dataloaders...\")\n",
    "prompter_ = functools.partial(\n",
    "    prompter.build_binary_qa_prompt_with_response_and_confidence_for_sft, is_for_inference=True\n",
    ")\n",
    "dataset_train = dataset.get_binary_qa_prompted_mimic_cxr_llava_model_input_dataset(\n",
    "    split=dataset.DatasetSplit.TRAIN,\n",
    "    tokenizer=tokenizer,\n",
    "    prompter=prompter_,\n",
    ")\n",
    "dataset_eval = dataset.get_binary_qa_prompted_mimic_cxr_llava_model_input_dataset(\n",
    "    split=dataset.DatasetSplit.VALIDATION,\n",
    "    tokenizer=tokenizer,\n",
    "    prompter=prompter_,\n",
    ")\n",
    "\n",
    "dataloader_train = dataset.get_mimic_cxr_llava_model_input_dataloader(\n",
    "    dataset=dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    padding_tokenizer=padding_tokenizer,\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "dataloader_eval = dataset.get_mimic_cxr_llava_model_input_dataloader(\n",
    "    dataset=dataset_eval,\n",
    "    batch_size=2 * batch_size,\n",
    "    padding_tokenizer=padding_tokenizer,\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "eval_batch_iterator = iter(dataloader_eval)\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")  # Adds higher directory to python modules path.\n",
    "from workflows import radialog_binary_qa_ppo_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(input_kwargs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 396, 32000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_of_image_token = (input_ids == training.LLAVA_IMAGE_TOKEN_INDEX).nonzero(as_tuple=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([46, 47, 51, 47, 49, 52, 46, 48, 47, 51, 47, 41, 46, 46, 52, 45],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes_of_image_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
       "         21082, 20255, 16684,   408,   385, 18860, 17937, 19915, 29889,   450,\n",
       "         20255,  4076, 10257, 29892, 13173, 29892,   322,  1248,   568,  6089,\n",
       "           304,   278,  1404, 29915, 29879,  5155, 29889,  3148,  1001, 29901,\n",
       "         29871,  -200,   869,   887,   526,   304,  1044,   408,   263, 17937,\n",
       "         19915,   322,  1234,   263,  2323,  1139, 29889,  2860,   366, 10049,\n",
       "         29892,  3113,  3867,   596,  1583, 17983,   310,   596, 16420, 29889,\n",
       "          9133,   680,   263, 16420,  1546, 29871, 29900, 29892, 29871, 29896,\n",
       "         29892, 29871, 29906, 29892, 29871, 29941, 29892, 29871, 29946, 29892,\n",
       "         29871, 29945, 29892, 29871, 29953, 29892, 29871, 29955, 29892, 29871,\n",
       "         29947, 29892, 29871, 29929, 29892, 29871, 29896, 29900, 29892,   310,\n",
       "           920,  1854,   366,   526,   278,  1234,   338,  1959, 29889,   319,\n",
       "           995,  3802,   304, 29871, 29900,  2794,   366,  1348,   727,   338,\n",
       "           263,  1880,  6976,   393,   278,  1234,   338,  2743, 29889,  3575,\n",
       "         16420,   338,   304,   367,  8967,   297,   263,  4663,  8600,   310,\n",
       "           278,  1494,  3402, 29901,  8853,  5527,  5084,  1115,   938,  1836,\n",
       "          1317,   727,   738,  4221,   362,   310,   319,   371,   781, 25101,\n",
       "           297,   278,  3461, 29973,   319,  1799,  9047, 13566, 29901,  3869,\n",
       "         29892,  4221,   800,   310,   319,   371,   781, 25101,   526,  2198,\n",
       "         29889,  8853,  5527,  5084,  1115, 29871, 29906, 29913,     2],\n",
       "        [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
       "         21082, 20255, 16684,   408,   385, 18860, 17937, 19915, 29889,   450,\n",
       "         20255,  4076, 10257, 29892, 13173, 29892,   322,  1248,   568,  6089,\n",
       "           304,   278,  1404, 29915, 29879,  5155, 29889,  3148,  1001, 29901,\n",
       "         29871,  -200,   869,   887,   526,   304,  1044,   408,   263, 17937,\n",
       "         19915,   322,  1234,   263,  2323,  1139, 29889,  2860,   366, 10049,\n",
       "         29892,  3113,  3867,   596,  1583, 17983,   310,   596, 16420, 29889,\n",
       "          9133,   680,   263, 16420,  1546, 29871, 29900, 29892, 29871, 29896,\n",
       "         29892, 29871, 29906, 29892, 29871, 29941, 29892, 29871, 29946, 29892,\n",
       "         29871, 29945, 29892, 29871, 29953, 29892, 29871, 29955, 29892, 29871,\n",
       "         29947, 29892, 29871, 29929, 29892, 29871, 29896, 29900, 29892,   310,\n",
       "           920,  1854,   366,   526,   278,  1234,   338,  1959, 29889,   319,\n",
       "           995,  3802,   304, 29871, 29900,  2794,   366,  1348,   727,   338,\n",
       "           263,  1880,  6976,   393,   278,  1234,   338,  2743, 29889,  3575,\n",
       "         16420,   338,   304,   367,  8967,   297,   263,  4663,  8600,   310,\n",
       "           278,  1494,  3402, 29901,  8853,  5527,  5084,  1115,   938,  1836,\n",
       "          1317,   727,   738,  4221,   362,   310,  9160, 14910,   387, 14997,\n",
       "           297,   278,  3461, 29973,   319,  1799,  9047, 13566, 29901,  1939,\n",
       "         29892,   278,  1967,   947,   451, 12266,  9160, 14910,   387, 14997,\n",
       "         29889,  8853,  5527,  5084,  1115, 29871, 29896, 29913,     2],\n",
       "        [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     1,\n",
       "           319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116, 21082,\n",
       "         20255, 16684,   408,   385, 18860, 17937, 19915, 29889,   450, 20255,\n",
       "          4076, 10257, 29892, 13173, 29892,   322,  1248,   568,  6089,   304,\n",
       "           278,  1404, 29915, 29879,  5155, 29889,  3148,  1001, 29901, 29871,\n",
       "          -200,   869,   887,   526,   304,  1044,   408,   263, 17937, 19915,\n",
       "           322,  1234,   263,  2323,  1139, 29889,  2860,   366, 10049, 29892,\n",
       "          3113,  3867,   596,  1583, 17983,   310,   596, 16420, 29889,  9133,\n",
       "           680,   263, 16420,  1546, 29871, 29900, 29892, 29871, 29896, 29892,\n",
       "         29871, 29906, 29892, 29871, 29941, 29892, 29871, 29946, 29892, 29871,\n",
       "         29945, 29892, 29871, 29953, 29892, 29871, 29955, 29892, 29871, 29947,\n",
       "         29892, 29871, 29929, 29892, 29871, 29896, 29900, 29892,   310,   920,\n",
       "          1854,   366,   526,   278,  1234,   338,  1959, 29889,   319,   995,\n",
       "          3802,   304, 29871, 29900,  2794,   366,  1348,   727,   338,   263,\n",
       "          1880,  6976,   393,   278,  1234,   338,  2743, 29889,  3575, 16420,\n",
       "           338,   304,   367,  8967,   297,   263,  4663,  8600,   310,   278,\n",
       "          1494,  3402, 29901,  8853,  5527,  5084,  1115,   938,  1836,  5538,\n",
       "           278, 16500,   505, 18601,  9481,  1575, 29973,   319,  1799,  9047,\n",
       "         13566, 29901,  3869, 29892,   278, 16500,   756, 18601,  9481,  1575,\n",
       "         29889,  8853,  5527,  5084,  1115, 29871, 29900, 29913,     2],\n",
       "        [    2,     2,     2,     2,     2,     2,     2,     2,     1,   319,\n",
       "         13563,  1546,   263, 12758,  1404,   322,   385, 23116, 21082, 20255,\n",
       "         16684,   408,   385, 18860, 17937, 19915, 29889,   450, 20255,  4076,\n",
       "         10257, 29892, 13173, 29892,   322,  1248,   568,  6089,   304,   278,\n",
       "          1404, 29915, 29879,  5155, 29889,  3148,  1001, 29901, 29871,  -200,\n",
       "           869,   887,   526,   304,  1044,   408,   263, 17937, 19915,   322,\n",
       "          1234,   263,  2323,  1139, 29889,  2860,   366, 10049, 29892,  3113,\n",
       "          3867,   596,  1583, 17983,   310,   596, 16420, 29889,  9133,   680,\n",
       "           263, 16420,  1546, 29871, 29900, 29892, 29871, 29896, 29892, 29871,\n",
       "         29906, 29892, 29871, 29941, 29892, 29871, 29946, 29892, 29871, 29945,\n",
       "         29892, 29871, 29953, 29892, 29871, 29955, 29892, 29871, 29947, 29892,\n",
       "         29871, 29929, 29892, 29871, 29896, 29900, 29892,   310,   920,  1854,\n",
       "           366,   526,   278,  1234,   338,  1959, 29889,   319,   995,  3802,\n",
       "           304, 29871, 29900,  2794,   366,  1348,   727,   338,   263,  1880,\n",
       "          6976,   393,   278,  1234,   338,  2743, 29889,  3575, 16420,   338,\n",
       "           304,   367,  8967,   297,   263,  4663,  8600,   310,   278,  1494,\n",
       "          3402, 29901,  8853,  5527,  5084,  1115,   938,  1836,  1317,   727,\n",
       "           738,  4221,   362,   310,   349, 29765,   720,   272,  1165,   297,\n",
       "           278,  3461, 29973,   319,  1799,  9047, 13566, 29901,  3869, 29892,\n",
       "           278,  1967, 22981, 18906,   310,   349, 29765,   720,   272,  1165,\n",
       "         29889,  8853,  5527,  5084,  1115, 29871, 29906, 29913,     2]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_kwargs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2, 1], device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[-1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([46, 47, 51, 47, 49, 52, 46, 48, 47, 51, 47, 41, 46, 46, 52, 45],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes_of_image_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5,  6, 10,  6,  8, 11,  5,  7,  6, 10,  6,  0,  5,  5, 11,  4],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(input_ids == 1).int().argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def account_for_image_embeddings_for_single_image_inputs(input_ids, logits, values, attention_mask):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_ids (torch.Tensor): A tensor shaped (batch_size, sequence_length) including exactly 1 image token id (-200 for llava) for each sequence\n",
    "        logits (torch.Tensor): A tensor shaped (batch_size, sequence_length, vocab_size)\n",
    "        values: The values of the model\n",
    "        attention_mask: The attention mask of the model\n",
    "    \"\"\"\n",
    "    # Locate the image index\n",
    "    indexes_of_image_token = (input_ids == training.LLAVA_IMAGE_TOKEN_INDEX).nonzero(as_tuple=True)[\n",
    "        1\n",
    "    ]\n",
    "\n",
    "    # Shift the indexes taking in the account where each sequence begins (bos_token is taken as basis)\n",
    "    indexes_of_bos_token = (input_ids == 1).int().argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: No names found, cannot describe anything.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33monurdenizguler\u001b[0m (\u001b[33monurdenizguler-technical-university-of-munich\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/guests/deniz_gueler/repos/RewardingVisualDoubt/workflows/prototyping/wandb/run-20250411_133518-66sjhnjz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/onurdenizguler-technical-university-of-munich/trl/runs/66sjhnjz' target=\"_blank\">splendid-tree-14</a></strong> to <a href='https://wandb.ai/onurdenizguler-technical-university-of-munich/trl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/onurdenizguler-technical-university-of-munich/trl' target=\"_blank\">https://wandb.ai/onurdenizguler-technical-university-of-munich/trl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/onurdenizguler-technical-university-of-munich/trl/runs/66sjhnjz' target=\"_blank\">https://wandb.ai/onurdenizguler-technical-university-of-munich/trl/runs/66sjhnjz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:238: UserWarning: No dataset is provided. Make sure to set config.batch_size to the correct value before training.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "######################################## 3. Define the PPO and generation configurations ########################################\n",
    "epochs = 1\n",
    "lr = 5e-5\n",
    "log_with = \"foo\"\n",
    "out_dir = \"output\"\n",
    "\n",
    "# Example of batch size 16: 4 epochs over the batch. Each backward batch is of size 8, and each mini batch is of size 4\n",
    "# Gradients get accumulated during 4 + 4 mini batches, and then the model gets updated (the \"backward batch\" is completed)\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    learning_rate=lr,\n",
    "    task_name=\"gpt\",\n",
    "    ppo_epochs=4,  # Default value from TRL library is 4 (i.e. will go over the batch 4 times)\n",
    "    batch_size=DEFAULT_BATCH_SIZE,\n",
    "    backward_batch_size=int(DEFAULT_BATCH_SIZE / 2),  # Default value from TRL library is 1\n",
    "    mini_batch_size=int(DEFAULT_BATCH_SIZE / 4),\n",
    "    gradient_accumulation_steps=4,\n",
    "    log_with=\"wandb\",\n",
    "    project_kwargs=dataclasses.asdict(\n",
    "        accelerate.utils.ProjectConfiguration(\n",
    "            project_dir=\"radialog_binary_qa_ppo_training\", logging_dir=\"logs\"\n",
    "        )\n",
    "    ),\n",
    "    remove_unused_columns=False,\n",
    "    # optimize_device_cache=True,\n",
    "    init_kl_coef=0.05,\n",
    ")\n",
    "\n",
    "generation_kwargs_ppo = {\n",
    "    \"min_length\": -1,  # don't ignore the EOS token (see above)\n",
    "    \"top_k\": 0.0,  # no top-k sampling\n",
    "    \"top_p\": 1.0,  # no nucleus sampling\n",
    "    \"temperature\": 1.0,  # DONT BE CREATIVE\n",
    "    \"do_sample\": True,  # yes, we want to sample\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,  # most decoder models don't have a padding token - use EOS token instead (for this tokenizer it was already set to eos_token_id)\n",
    "    \"max_new_tokens\": 50,  # let's not be chatty, we need a few tokens to generate confidence but also not limit the response too much\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,  # (instead of ppo_terminators list)\n",
    "}\n",
    "\n",
    "ppo_trainer = t.cast(\n",
    "    training.MultimodalPPOTrainer,\n",
    "    training.MultimodalPPOTrainer(\n",
    "        model=model,\n",
    "        config=ppo_config,\n",
    "        tokenizer=tokenizer,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# not sure if needed but just to be safe for now\n",
    "tokenizer.padding_side = \"left\"\n",
    "model.config.padding_side = \"left\"\n",
    "model.config.tokenizer_padding_side = \"left\"\n",
    "# model.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_epoch = []\n",
    "iterator_train = iter(dataloader_train)\n",
    "batch = next(iterator_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    print(i)\n",
    "    batch = next(iterator_train)\n",
    "    rewards, batch_report = radialog_binary_qa_ppo_training.radialog_binary_qa_ppo_training_step(\n",
    "        model,\n",
    "        device,\n",
    "        tokenizer,\n",
    "        generation_kwargs_ppo,\n",
    "        ppo_trainer,\n",
    "        batch,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### 5.1 Unpack the batch #########\n",
    "batch: dataset.MimicCxrLlavaModelInputBatchDict = batch\n",
    "batch_llava_model_input_dict = batch[\"batch_llava_model_input_dict\"]\n",
    "batch_llava_model_input_dict = dataset.move_llava_model_input_dict_to_device(\n",
    "    batch_llava_model_input_dict, device\n",
    ")\n",
    "input_ids, images = (\n",
    "    batch_llava_model_input_dict[\"text_prompt_input_ids\"],\n",
    "    batch_llava_model_input_dict[\"images\"],\n",
    ")\n",
    "attention_mask = batch[\"batch_attention_mask\"].to(device)\n",
    "labels = t.cast(torch.Tensor, batch[\"batch_labels\"]).to(device)\n",
    "stopping_criteria = KeywordsStoppingCriteria([STOP_STR], tokenizer, input_ids)\n",
    "input_ids_list = training.remove_preciding_padding_from_batch_tensor(input_ids)\n",
    "\n",
    "# model.train()\n",
    "model.gradient_checkpointing_disable()\n",
    "generated_confidences_ids = ppo_trainer.generate(\n",
    "    query_tensor=input_ids_list,  # ppo_trainer.generate() method admits list of tensors, handles padding and batching itself\n",
    "    images=images,\n",
    "    return_prompt=False,\n",
    "    batch_size=DEFAULT_BATCH_SIZE,\n",
    "    use_cache=True,  # => not compatible with gradient checkpointing!\n",
    "    stopping_criteria=[stopping_criteria],\n",
    "    **generation_kwargs_ppo,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_confidences_texts = tokenizer.batch_decode(generated_confidences_ids)\n",
    "generated_answer_labels = response.parse_binary_labels(generated_confidences_texts)\n",
    "generated_confidence_values = response.parse_confidences(generated_confidences_texts)\n",
    "\n",
    "rewards = [\n",
    "    reward.generated_answer_and_confidence_to_reward(\n",
    "        generated_answer_label, generated_confidence_value, ground_truth_label\n",
    "    )\n",
    "    for generated_answer_label, generated_confidence_value, ground_truth_label in zip(\n",
    "        generated_answer_labels, generated_confidence_values, labels.bool().tolist()\n",
    "    )\n",
    "]\n",
    "\n",
    "rewards = t.cast(\n",
    "    list[torch.FloatTensor],\n",
    "    [torch.tensor(r).to(device) for r in rewards],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = ppo_trainer.prepare_model_inputs(\n",
    "    queries=t.cast(list[torch.LongTensor], input_ids_list),\n",
    "    responses=t.cast(list[torch.LongTensor], generated_confidences_ids),\n",
    ")\n",
    "\n",
    "model_inputs[\"images\"] = images  # N\n",
    "model_inputs_names = list(model_inputs.keys())\n",
    "\n",
    "queries = (t.cast(list[torch.LongTensor], input_ids_list),)\n",
    "responses = (t.cast(list[torch.LongTensor], generated_confidences_ids),)\n",
    "bs = len(queries)\n",
    "fbs = ppo_trainer.config.mini_batch_size\n",
    "all_logprobs = []\n",
    "all_logits = []\n",
    "all_masks = []\n",
    "all_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_kwargs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     base_model_output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpretrained_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43minput_kwargs\u001b[49m, output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_kwargs' is not defined"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    base_model_output = model.pretrained_model(**input_kwargs, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 395, 32000])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_state = base_model_output.hidden_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 395, 4096])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = model.v_head(last_hidden_state).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 209])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 390, 32000])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the indexes where the tensor \"my_tensor\" equals -200\n",
    "sequence = input_kwargs[\"input_ids\"][0]\n",
    "indexes_of_image_token = (sequence == -200).nonzero(as_tuple=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logitsss = model(input_ids=sequence[:20].unsqueeze(0))[1][\"logits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     1,   319, 13563,  1546,   263, 12758,  1404,\n",
       "          322,   385, 23116, 21082, 20255, 16684,   408,   385, 18860, 17937,\n",
       "        19915, 29889,   450, 20255,  4076, 10257, 29892, 13173, 29892,   322,\n",
       "         1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155, 29889,\n",
       "         3148,  1001, 29901, 29871,  -200,   869,   887,   526,   304,  1044,\n",
       "          408,   263, 17937, 19915,   322,  1234,   263,  2323,  1139, 29889,\n",
       "         2860,   366, 10049, 29892,  3113,  3867,   596,  1583, 17983,   310,\n",
       "          596, 16420, 29889,  9133,   680,   263, 16420,  1546, 29871, 29900,\n",
       "        29892, 29871, 29896, 29892, 29871, 29906, 29892, 29871, 29941, 29892,\n",
       "        29871, 29946, 29892, 29871, 29945, 29892, 29871, 29953, 29892, 29871,\n",
       "        29955, 29892, 29871, 29947, 29892, 29871, 29929, 29892, 29871, 29896,\n",
       "        29900, 29892,   310,   920,  1854,   366,   526,   278,  1234,   338,\n",
       "         1959, 29889,   319,   995,  3802,   304, 29871, 29900,  2794,   366,\n",
       "         1348,   727,   338,   263,  1880,  6976,   393,   278,  1234,   338,\n",
       "         2743, 29889,  3575, 16420,   338,   304,   367,  8967,   297,   263,\n",
       "         4663,  8600,   310,   278,  1494,  3402, 29901,  8853,  5527,  5084,\n",
       "         1115,   938,  1836,  5538,   278, 16500,   505,   319,   371,   781,\n",
       "        25101, 29973,   319,  1799,  9047, 13566, 29901,  3869, 29892,  4221,\n",
       "          800,   310,   319,   371,   781, 25101,   526,  2198, 29889,  8853,\n",
       "         5527,  5084,  1115, 29871, 29947, 29913,     2], device='cuda:0')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -4.4936, -10.1075,   2.7072,  ...,   2.1521,  -3.0400,   1.7643],\n",
       "        [ -2.5677,  -0.9008,   5.9041,  ...,   3.0179,  -2.7296,   1.1002],\n",
       "        [ -2.2757,   0.0464,   2.4819,  ...,   2.1444,  -3.4312,  -1.7103],\n",
       "        ...,\n",
       "        [ -3.0267,   1.4197,   3.3870,  ...,   2.2771,  -0.7604,  -0.3429],\n",
       "        [ -3.3076,   2.2019,   2.7390,  ...,   1.8549,  -1.8728,  -0.1617],\n",
       "        [ -2.6943,   2.8725,   2.5231,  ...,   2.4108,  -1.4761,  -1.0808]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0][indexes_of_image_token - 12 : indexes_of_image_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 32000])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'A',\n",
       " '.',\n",
       " ',',\n",
       " 'MS',\n",
       " 'PA',\n",
       " 'PA',\n",
       " 'PA']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(torch.argmax(logitsss.squeeze(0), dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'A',\n",
       " 'chat',\n",
       " 'between',\n",
       " 'a',\n",
       " 'curious',\n",
       " 'user',\n",
       " 'and',\n",
       " 'an',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'assistant',\n",
       " 'acting',\n",
       " 'as',\n",
       " 'an',\n",
       " 'experienced',\n",
       " 'radi',\n",
       " 'ologist',\n",
       " '.',\n",
       " 'The',\n",
       " 'assistant',\n",
       " 'gives',\n",
       " 'professional',\n",
       " ',',\n",
       " 'detailed',\n",
       " ',',\n",
       " 'and',\n",
       " 'pol',\n",
       " 'ite',\n",
       " 'answers',\n",
       " 'to',\n",
       " 'the',\n",
       " 'user',\n",
       " \"'\",\n",
       " 's',\n",
       " 'questions',\n",
       " '.',\n",
       " 'US',\n",
       " 'ER',\n",
       " ':',\n",
       " '',\n",
       " '1']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(torch.argmax(logits[0][: indexes_of_image_token - 12], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "i = 3\n",
    "\n",
    "input_kwargs = {key: value[i * fbs : (i + 1) * fbs] for key, value in model_inputs.items()}\n",
    "query_batch = queries[i * fbs : (i + 1) * fbs]\n",
    "response_batch = responses[i * fbs : (i + 1) * fbs]\n",
    "with torch.no_grad():\n",
    "    logits, _, values = model(**input_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_kwargs[\"attention_mask\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 390])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 390, 32000])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = input_kwargs[\"input_ids\"]\n",
    "attention_mask = input_kwargs[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 395, 32000])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[:, :-1, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "logp = F.log_softmax(logits[:, :-1, :], dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_without_image_token = training.replace_image_token_with_another_token(\n",
    "    input_ids.clone(), replacement_token_id=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
       "         21082, 20255, 16684,   408,   385, 18860, 17937, 19915, 29889,   450,\n",
       "         20255,  4076, 10257, 29892, 13173, 29892,   322,  1248,   568,  6089,\n",
       "           304,   278,  1404, 29915, 29879,  5155, 29889,  3148,  1001, 29901,\n",
       "         29871,  -200,   869,   887,   526,   304,  1044,   408,   263, 17937,\n",
       "         19915,   322,  1234,   263,  2323,  1139, 29889,  2860,   366, 10049,\n",
       "         29892,  3113,  3867,   596,  1583, 17983,   310,   596, 16420, 29889,\n",
       "          9133,   680,   263, 16420,  1546, 29871, 29900, 29892, 29871, 29896,\n",
       "         29892, 29871, 29906, 29892, 29871, 29941, 29892, 29871, 29946, 29892,\n",
       "         29871, 29945, 29892, 29871, 29953, 29892, 29871, 29955, 29892, 29871,\n",
       "         29947, 29892, 29871, 29929, 29892, 29871, 29896, 29900, 29892,   310,\n",
       "           920,  1854,   366,   526,   278,  1234,   338,  1959, 29889,   319,\n",
       "           995,  3802,   304, 29871, 29900,  2794,   366,  1348,   727,   338,\n",
       "           263,  1880,  6976,   393,   278,  1234,   338,  2743, 29889,  3575,\n",
       "         16420,   338,   304,   367,  8967,   297,   263,  4663,  8600,   310,\n",
       "           278,  1494,  3402, 29901,  8853,  5527,  5084,  1115,   938,  1836,\n",
       "          1317,   727,   738,  1804,   310, 29871,   319,   371,   781, 25101,\n",
       "           297,   278,  3461, 29973,   319,  1799,  9047, 13566, 29901,  3869,\n",
       "         29892,   278,   652, 21780, 29320, 12266,   319,   371,   781, 25101,\n",
       "         29889,  8853,  5527,  5084,  1115, 29871, 29896, 29913,     2],\n",
       "        [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
       "         21082, 20255, 16684,   408,   385, 18860, 17937, 19915, 29889,   450,\n",
       "         20255,  4076, 10257, 29892, 13173, 29892,   322,  1248,   568,  6089,\n",
       "           304,   278,  1404, 29915, 29879,  5155, 29889,  3148,  1001, 29901,\n",
       "         29871,  -200,   869,   887,   526,   304,  1044,   408,   263, 17937,\n",
       "         19915,   322,  1234,   263,  2323,  1139, 29889,  2860,   366, 10049,\n",
       "         29892,  3113,  3867,   596,  1583, 17983,   310,   596, 16420, 29889,\n",
       "          9133,   680,   263, 16420,  1546, 29871, 29900, 29892, 29871, 29896,\n",
       "         29892, 29871, 29906, 29892, 29871, 29941, 29892, 29871, 29946, 29892,\n",
       "         29871, 29945, 29892, 29871, 29953, 29892, 29871, 29955, 29892, 29871,\n",
       "         29947, 29892, 29871, 29929, 29892, 29871, 29896, 29900, 29892,   310,\n",
       "           920,  1854,   366,   526,   278,  1234,   338,  1959, 29889,   319,\n",
       "           995,  3802,   304, 29871, 29900,  2794,   366,  1348,   727,   338,\n",
       "           263,  1880,  6976,   393,   278,  1234,   338,  2743, 29889,  3575,\n",
       "         16420,   338,   304,   367,  8967,   297,   263,  4663,  8600,   310,\n",
       "           278,  1494,  3402, 29901,  8853,  5527,  5084,  1115,   938,  1836,\n",
       "          1317,   727, 10757,   310, 19777,  3631,   382,   600,  3958,   297,\n",
       "           278,  1967, 29973,   319,  1799,  9047, 13566, 29901,  3869, 29892,\n",
       "           278,  1967, 22981, 18906,   310, 19777,  3631,   382,   600,  3958,\n",
       "         29889,  8853,  5527,  5084,  1115, 29871, 29900, 29913,     2],\n",
       "        [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     1,   319, 13563,\n",
       "          1546,   263, 12758,  1404,   322,   385, 23116, 21082, 20255, 16684,\n",
       "           408,   385, 18860, 17937, 19915, 29889,   450, 20255,  4076, 10257,\n",
       "         29892, 13173, 29892,   322,  1248,   568,  6089,   304,   278,  1404,\n",
       "         29915, 29879,  5155, 29889,  3148,  1001, 29901, 29871,  -200,   869,\n",
       "           887,   526,   304,  1044,   408,   263, 17937, 19915,   322,  1234,\n",
       "           263,  2323,  1139, 29889,  2860,   366, 10049, 29892,  3113,  3867,\n",
       "           596,  1583, 17983,   310,   596, 16420, 29889,  9133,   680,   263,\n",
       "         16420,  1546, 29871, 29900, 29892, 29871, 29896, 29892, 29871, 29906,\n",
       "         29892, 29871, 29941, 29892, 29871, 29946, 29892, 29871, 29945, 29892,\n",
       "         29871, 29953, 29892, 29871, 29955, 29892, 29871, 29947, 29892, 29871,\n",
       "         29929, 29892, 29871, 29896, 29900, 29892,   310,   920,  1854,   366,\n",
       "           526,   278,  1234,   338,  1959, 29889,   319,   995,  3802,   304,\n",
       "         29871, 29900,  2794,   366,  1348,   727,   338,   263,  1880,  6976,\n",
       "           393,   278,  1234,   338,  2743, 29889,  3575, 16420,   338,   304,\n",
       "           367,  8967,   297,   263,  4663,  8600,   310,   278,  1494,  3402,\n",
       "         29901,  8853,  5527,  5084,  1115,   938,  1836,  5538,   278, 16500,\n",
       "           505,   319,   371,   781, 25101, 29973,   319,  1799,  9047, 13566,\n",
       "         29901,  3869, 29892,   278,  1967,  3697,   319,   371,   781, 25101,\n",
       "         29889,  8853,  5527,  5084,  1115, 29871, 29900, 29913,     2],\n",
       "        [    2,     2,     2,     2,     2,     2,     2,     2,     1,   319,\n",
       "         13563,  1546,   263, 12758,  1404,   322,   385, 23116, 21082, 20255,\n",
       "         16684,   408,   385, 18860, 17937, 19915, 29889,   450, 20255,  4076,\n",
       "         10257, 29892, 13173, 29892,   322,  1248,   568,  6089,   304,   278,\n",
       "          1404, 29915, 29879,  5155, 29889,  3148,  1001, 29901, 29871,  -200,\n",
       "           869,   887,   526,   304,  1044,   408,   263, 17937, 19915,   322,\n",
       "          1234,   263,  2323,  1139, 29889,  2860,   366, 10049, 29892,  3113,\n",
       "          3867,   596,  1583, 17983,   310,   596, 16420, 29889,  9133,   680,\n",
       "           263, 16420,  1546, 29871, 29900, 29892, 29871, 29896, 29892, 29871,\n",
       "         29906, 29892, 29871, 29941, 29892, 29871, 29946, 29892, 29871, 29945,\n",
       "         29892, 29871, 29953, 29892, 29871, 29955, 29892, 29871, 29947, 29892,\n",
       "         29871, 29929, 29892, 29871, 29896, 29900, 29892,   310,   920,  1854,\n",
       "           366,   526,   278,  1234,   338,  1959, 29889,   319,   995,  3802,\n",
       "           304, 29871, 29900,  2794,   366,  1348,   727,   338,   263,  1880,\n",
       "          6976,   393,   278,  1234,   338,  2743, 29889,  3575, 16420,   338,\n",
       "           304,   367,  8967,   297,   263,  4663,  8600,   310,   278,  1494,\n",
       "          3402, 29901,  8853,  5527,  5084,  1115,   938,  1836,  1317,   727,\n",
       "         10757,   310, 19777,  3631,   382,   600,  3958,   297,   278,  1967,\n",
       "         29973,   319,  1799,  9047, 13566, 29901,  1939, 29892,   727,   338,\n",
       "           694,  6459,   519, 10757,   310, 19777,  3631,   382,   600,  3958,\n",
       "         29889,  8853,  5527,  5084,  1115, 29871, 29900, 29913,     2]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logpy = torch.gather(logp, 2, input_ids_without_image_token[:, 1:].unsqueeze(2)).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Size does not match at dimension 0 expected index [16, 181, 1] to be smaller than self [4, 389, 32000] apart from dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logprobs_from_logits\n\u001b[0;32m----> 3\u001b[0m logprobs \u001b[38;5;241m=\u001b[39m \u001b[43mlogprobs_from_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m masks \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(attention_mask)\n\u001b[1;32m      5\u001b[0m masks[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m attention_mask[:, \u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/miniconda3/envs/llava_hf/lib/python3.10/site-packages/trl/core.py:99\u001b[0m, in \u001b[0;36mlogprobs_from_logits\u001b[0;34m(logits, labels, gather)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gather:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logp\n\u001b[0;32m---> 99\u001b[0m logpy \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logpy\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Size does not match at dimension 0 expected index [16, 181, 1] to be smaller than self [4, 389, 32000] apart from dimension 2"
     ]
    }
   ],
   "source": [
    "from trl.core import logprobs_from_logits\n",
    "\n",
    "logprobs = logprobs_from_logits(logits[:, :-1, :], input_ids[:, 1:])\n",
    "masks = torch.zeros_like(attention_mask)\n",
    "masks[:, :-1] = attention_mask[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First pass for logprobs\n",
      "torch.Size([16, 207])\n",
      "torch.Size([4, 395])\n",
      "torch.Size([4, 206])\n",
      "torch.Size([16, 207])\n",
      "torch.Size([4, 393])\n",
      "torch.Size([4, 206])\n",
      "torch.Size([16, 207])\n",
      "torch.Size([4, 402])\n",
      "torch.Size([4, 206])\n",
      "torch.Size([16, 207])\n",
      "torch.Size([4, 390])\n",
      "torch.Size([4, 206])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 395 but got size 393 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# queries = training.replace_image_token_with_another_token_for_list_of_tensors(input_ids_list)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# queries = t.cast(list[torch.LongTensor], queries)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mgradient_checkpointing_enable()\n\u001b[0;32m----> 4\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[43mppo_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultimodal_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqueries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLongTensor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLongTensor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerated_confidences_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llava_hf/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/RewardingVisualDoubt/src/RewardingVisualDoubt/training.py:155\u001b[0m, in \u001b[0;36mMultimodalPPOTrainer.multimodal_step\u001b[0;34m(self, queries, responses, scores, images)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst pass for logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 155\u001b[0m     all_logprobs, logits_or_none, values, masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatched_forward_pass\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_kl_penalty\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# for when the model is a peft model\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_peft_model \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39munwrap_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\u001b[38;5;241m.\u001b[39mpretrained_model,\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable_adapter\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    163\u001b[0m     ):\n",
      "File \u001b[0;32m~/miniconda3/envs/llava_hf/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/RewardingVisualDoubt/src/RewardingVisualDoubt/training.py:416\u001b[0m, in \u001b[0;36mMultimodalPPOTrainer.batched_forward_pass\u001b[0;34m(self, model, queries, responses, model_inputs, return_logits)\u001b[0m\n\u001b[1;32m    410\u001b[0m     all_logprobs\u001b[38;5;241m.\u001b[39mappend(logprobs)\n\u001b[1;32m    411\u001b[0m     all_masks\u001b[38;5;241m.\u001b[39mappend(masks)\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    414\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcat(all_logprobs),\n\u001b[1;32m    415\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcat(all_logits)[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m return_logits \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m--> 416\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_values\u001b[49m\u001b[43m)\u001b[49m[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    417\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcat(all_masks)[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    418\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 395 but got size 393 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# queries = training.replace_image_token_with_another_token_for_list_of_tensors(input_ids_list)\n",
    "# queries = t.cast(list[torch.LongTensor], queries)\n",
    "model.gradient_checkpointing_enable()\n",
    "stats = ppo_trainer.multimodal_step(\n",
    "    queries=t.cast(list[torch.LongTensor], input_ids_list),\n",
    "    responses=t.cast(list[torch.LongTensor], generated_confidences_ids),\n",
    "    scores=rewards,\n",
    "    images=images,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "POST_GENERATION_CONFIDENCE_REQUEST_1 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer? \"\n",
    "    \"Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. \"\n",
    "    \"A value close to 0 means you think there is a high probability that the answer is wrong. \"\n",
    "    \"The closer the value is to 10, the higher you think is the probability that the answer is correct. \"\n",
    "    \"The output should have the format 'Confidence: <confidence>' and nothing else. \"\n",
    ")\n",
    "\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_2 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer? \"\n",
    "    \"Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. \"\n",
    "    \"The output should have the format 'My Confidence: <confidence>'. \"\n",
    "    \"ASSISTANT: Here is my self evaluation of my confidence. My Confidence (out of 10):\"\n",
    ")\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_3 = \"USER: Irrelevant from my question, what is your name?\"\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_4 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer? \"\n",
    "    \"Provide a confidence between 0, 1, 2, 3, 4, 5 of how sure you are the answer is correct. \"\n",
    "    \"The output should have the format 'My Confidence: <confidence>' My Reasoning for my Self Confidence Evaluation: <reasoning>. \"\n",
    "    \"ASSISTANT: Here is my self evaluation of my confidence. My Confidence (out of 5):\"\n",
    ")\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_5 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer? \"\n",
    "    \"Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,  of how sure you are the answer is correct. \"\n",
    "    \"The output should have the format 'My Confidence: <confidence>' My Very Short Reasoning for my Self Confidence Evaluation: <very_short_reasoning>. \"\n",
    "    \"ASSISTANT: Here is my self evaluation of my confidence. My Confidence (out of 10):\"\n",
    ")\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_6 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer and what is your explanation regarding your confidence?\"\n",
    "    \"ASSISTANT: When asked how confident I am about a response, I consistently provide it in a JSON object, adhering to my policy.\"\n",
    "    \"My confidence is a value between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, where 0 means I am not confident at all and 10 means I am very confident.\"\n",
    "    \"The confidence JSON follows this structure: {'confidence': int, 'explanation': str}.\"\n",
    "    \"Here's my confidence JSON about my last response: \"\n",
    ")\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_7 = (\n",
    "    \"USER: Now evaluate your own response. How confident are you in your answer?\"\n",
    "    \"ASSISTANT: When asked how confident I am about a response, I consistently provide it in a JSON object, adhering to my policy.\"\n",
    "    \"My confidence is a value between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, where 0 means I am not confident at all and 10 means I am very confident.\"\n",
    "    \"The confidence JSON follows this structure: {'confidence': int}.\"\n",
    "    \"Here's my confidence JSON about my last response: \"\n",
    ")\n",
    "\n",
    "POST_GENERATION_CONFIDENCE_REQUEST_8 = (\n",
    "    \"</s> USER: Now evaluate your own response. How confident are you in your answer?\"\n",
    "    \"Provide a confidence between 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, of how sure you are the answer is correct. \"\n",
    "    \"A value close to 0 means you think there is a high probability that the answer is wrong. \"\n",
    "    \"The closer the value is to 10, the higher you think is the probability that the answer is correct. \"\n",
    "    \"ASSISTANT: When asked how confident I am about a response, I consistently provide it in a JSON object, adhering to my policy.\"\n",
    "    \"The confidence JSON follows this structure: {'confidence': int}.\"\n",
    "    \"Here's my confidence JSON about my last response: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_STR = prompter.Seperator.END_OF_SEQUENCE_SEPERATOR.value\n",
    "from LLAVA_Biovil.llava.mm_utils import KeywordsStoppingCriteria\n",
    "from RewardingVisualDoubt import inference\n",
    "\n",
    "padding_tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "padding_tokenizer.padding_side = \"left\"\n",
    "padding_tokenizer.pad_token_id = padding_tokenizer.bos_token_id\n",
    "dataset_test = dataset.get_binary_qa_prompted_mimic_cxr_llava_model_input_dataset(\n",
    "    split=dataset.DatasetSplit.TEST,\n",
    "    tokenizer=tokenizer,\n",
    "    prompter=prompter.build_binary_qa_instruction_from_disease_under_study,\n",
    ")\n",
    "dataloader_test = dataset.get_mimic_cxr_llava_model_input_dataloader(\n",
    "    dataset=dataset_test, batch_size=1, padding_tokenizer=padding_tokenizer, num_workers=8\n",
    ")\n",
    "\n",
    "for idx, batch in enumerate(dataloader_test):\n",
    "    batch = t.cast(dataset.MimicCxrLlavaModelInputBatchDict, batch)\n",
    "    batch_llava_model_input_dict = batch[\"batch_llava_model_input_dict\"]\n",
    "    batch_llava_model_input_dict = dataset.move_llava_model_input_dict_to_device(\n",
    "        batch_llava_model_input_dict, torch.device(shared.torch_devices.cuda.value)\n",
    "    )\n",
    "    input_ids, images = (\n",
    "        batch_llava_model_input_dict[\"text_prompt_input_ids\"],\n",
    "        batch_llava_model_input_dict[\"images\"],\n",
    "    )\n",
    "    stopping_criteria = KeywordsStoppingCriteria([STOP_STR], tokenizer, input_ids)\n",
    "    pred = inference.generate_radialog_answer_for_binary_qa_for_single_study(\n",
    "        model, tokenizer, input_ids, images, stopping_criteria\n",
    "    )\n",
    "    confidence_request_prompt = (\n",
    "        batch[\"batch_prompts\"][0]\n",
    "        + \" \"\n",
    "        + pred\n",
    "        + \" \"\n",
    "        + prompter.build_post_generation_user_confidence_request()  # POST_GENERATION_CONFIDENCE_REQUEST_8\n",
    "    )\n",
    "    confidence_request_input_ids = torch.unsqueeze(\n",
    "        torch.IntTensor(tokenizer(confidence_request_prompt)[\"input_ids\"]), 0\n",
    "    ).to(device)\n",
    "    stopping_criteria = KeywordsStoppingCriteria(\n",
    "        [STOP_STR], tokenizer, confidence_request_input_ids\n",
    "    )\n",
    "    pred_with_confidence = inference.generate_radialog_answer_for_binary_qa_for_single_study(\n",
    "        model, tokenizer, confidence_request_input_ids, images, stopping_criteria\n",
    "    )\n",
    "    print(f\"\\n Metadata: {batch['batch_mimic_cxr_datapoint_metadata']}\")\n",
    "    print(f\"Prompt: {batch['batch_prompts']}\")\n",
    "    print(f\"Label:\", batch[\"batch_labels\"])\n",
    "    print(f\"File_idx {idx}, ASSISTANT: \", pred)\n",
    "    print(f\"File_idx {idx}, ASSISTANT (after confidence request): \", pred_with_confidence)\n",
    "    if idx == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## TEST TO SEE IF TEMPERATURE AND TOP_P PARAMS HELP WITH USER CONFIDENCE REQUEST WITHOUT ASSISTANT CONFIRMATION ########################################\n",
    "\n",
    "\n",
    "from LLAVA_Biovil.llava.mm_utils import tokenizer_image_token\n",
    "\n",
    "iterator_train = iter(dataloader_train)\n",
    "batch = next(iterator_train)\n",
    "\n",
    "batch_llava_model_input_dict = batch[\"batch_llava_model_input_dict\"]\n",
    "batch_llava_model_input_dict = dataset.move_llava_model_input_dict_to_device(\n",
    "    batch_llava_model_input_dict, device\n",
    ")\n",
    "_, images = (\n",
    "    batch_llava_model_input_dict[\"text_prompt_input_ids\"],\n",
    "    batch_llava_model_input_dict[\"images\"],\n",
    ")\n",
    "\n",
    "my_prompt = prompter.build_binary_qa_instruction_from_disease_under_study_with_confidence_request(\n",
    "    \"Cardiomegaly\"\n",
    ")\n",
    "tokenized_prompt = tokenizer_image_token(my_prompt, tokenizer, return_tensors=\"pt\").to(device)\n",
    "\n",
    "stopping_criteria = KeywordsStoppingCriteria([STOP_STR], tokenizer, tokenized_prompt.unsqueeze(0))\n",
    "\n",
    "prompt_and_generated_answers_ids = model.generate(\n",
    "    input_ids=tokenized_prompt.unsqueeze(0),\n",
    "    images=images[0].unsqueeze(0),\n",
    "    # attention_mask=attention_mask,\n",
    "    do_sample=True,\n",
    "    use_cache=True,\n",
    "    temperature=1.8,\n",
    "    top_p=0.7,\n",
    "    max_new_tokens=300,  # TODO maybe move to the kwargs\n",
    "    stopping_criteria=[stopping_criteria],  # TODO understand better\n",
    "    pad_token_id=tokenizer.pad_token_id,  # used in tokenizing after the generation, # TODO maybe move to the kwargs\n",
    "    # **generation_kwargs_prediction,  # TODO check which args to pass.\n",
    ")\n",
    "\n",
    "tokenizer.decode(\n",
    "    training.replace_image_token_with_another_token(prompt_and_generated_answers_ids)[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iterator_train)\n",
    "\n",
    "batch = t.cast(dataset.MimicCxrLlavaModelInputBatchDict, batch)\n",
    "batch_llava_model_input_dict = batch[\"batch_llava_model_input_dict\"]\n",
    "batch_llava_model_input_dict = dataset.move_llava_model_input_dict_to_device(\n",
    "    batch_llava_model_input_dict, device\n",
    ")\n",
    "input_ids, images = (\n",
    "    batch_llava_model_input_dict[\"text_prompt_input_ids\"],\n",
    "    batch_llava_model_input_dict[\"images\"],\n",
    ")\n",
    "attention_mask = batch[\"batch_attention_mask\"].to(device)  # TODO handle elsewhere\n",
    "labels = batch[\"batch_labels\"].to(device)  # TODO handle elsewhere\n",
    "\n",
    "\n",
    "model.eval()\n",
    "stopping_criteria = KeywordsStoppingCriteria([STOP_STR], tokenizer, input_ids)\n",
    "\n",
    "\n",
    "t3 = time.time()\n",
    "prompt_and_generated_answers_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    images=images,\n",
    "    attention_mask=attention_mask,\n",
    "    do_sample=False,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=32,  # Limiting, YES, but binary q&a answers are not very long!\n",
    "    stopping_criteria=[stopping_criteria],\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "t4 = time.time()\n",
    "prompt_and_generated_answers_ids = training.remove_trailing_padding_from_prediction(\n",
    "    prompt_and_generated_answers_ids, tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "# Append confidence request to the generated answers\n",
    "prompt_and_generated_answers_with_confidence_requests_ids = []\n",
    "for item in prompt_and_generated_answers_ids:\n",
    "    confidence_request_input_ids = (\n",
    "        tokenizer(prompter.build_post_generation_user_confidence_request(), return_tensors=\"pt\")\n",
    "        .input_ids.to(device)\n",
    "        .squeeze(0)\n",
    "    )[\n",
    "        1:\n",
    "    ]  # drop start of sequence token\n",
    "    prompt_and_generated_answers_with_confidence_requests_ids.append(\n",
    "        torch.cat((item, confidence_request_input_ids), 0)\n",
    "    )\n",
    "model.train()\n",
    "\n",
    "t5 = time.time()\n",
    "generated_confidences_ids = ppo_trainer.generate(\n",
    "    prompt_and_generated_answers_with_confidence_requests_ids,  # ppo_trainer.generate() method admits list of tensors, not a batch tensor unfortunately\n",
    "    images=images,\n",
    "    return_prompt=False,\n",
    "    **generation_kwargs_ppo,\n",
    ")\n",
    "t6 = time.time()\n",
    "\n",
    "\n",
    "complete_conversation_ids = [\n",
    "    torch.cat((p, c), 0)\n",
    "    for p, c in zip(\n",
    "        prompt_and_generated_answers_with_confidence_requests_ids,\n",
    "        generated_confidences_ids,\n",
    "    )\n",
    "]\n",
    "generated_answer_only_ids = [\n",
    "    prompt_and_generated_answers_ids[i][len(input_ids[i]) :] for i in range(len(input_ids))\n",
    "]\n",
    "\n",
    "# Remove the unindex image token from the prompt\n",
    "prompt_and_generated_answers_with_confidence_requests_ids = (\n",
    "    training.replace_image_token_with_another_token_for_list_of_tensors(\n",
    "        prompt_and_generated_answers_with_confidence_requests_ids\n",
    "    )\n",
    ")\n",
    "generated_answers_texts = tokenizer.batch_decode(\n",
    "    generated_answer_only_ids,\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "generated_confidences_texts = tokenizer.batch_decode(\n",
    "    generated_confidences_ids,\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "generated_answer_labels = response.parse_binary_labels(generated_answers_texts)\n",
    "generated_confidence_values = response.parse_confidences(generated_confidences_texts)\n",
    "\n",
    "rewards = [\n",
    "    reward.generated_answer_and_confidence_to_reward(\n",
    "        generated_answer_label, generated_confidence_value, ground_truth_label\n",
    "    )\n",
    "    for generated_answer_label, generated_confidence_value, ground_truth_label in zip(\n",
    "        generated_answer_labels, generated_confidence_values, labels.bool().tolist()\n",
    "    )\n",
    "]\n",
    "\n",
    "report = {}\n",
    "report[\"generated_answer_labels\"] = generated_answer_labels\n",
    "\n",
    "rewards_epoch += rewards\n",
    "rewards = [torch.tensor(r).to(device) for r in rewards]\n",
    "\n",
    "t7 = time.time()\n",
    "stats = ppo_trainer.step(\n",
    "    prompt_and_generated_answers_with_confidence_requests_ids, generated_answer_only_ids, rewards\n",
    ")\n",
    "t8 = time.time()\n",
    "\n",
    "# ppo_trainer.log_stats(stats, batch, rewards, columns_to_log=[\"query\", \"response\", \"answer\"])\n",
    "\n",
    "# print(f\"Finished epoch {epoch}. Average reward: {avg_reward}\")\n",
    "# ppo_trainer.save_pretrained(os.path.join(out_dir, \"model_finetuned\"))\n",
    "\n",
    "# TODO: For random exploration\n",
    "# chance_to_change_confidence -= reduce_per_step\n",
    "# chance_to_change_confidence = max(0, chance_to_change_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(rewards_epoch) // batch_size):\n",
    "    print(sum(rewards_epoch[i * batch_size : (i + 1) * batch_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_confidence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_answer_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_answers_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_confidences_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidences_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((t8 - t7) * 1000, \"time it took to ppo step\")\n",
    "print((t6 - t5) * 1000, \"time it took to generate confidences\")\n",
    "print((t4 - t3) * 1000, \"time it took to generate answers\")\n",
    "print((t2 - t1) * 1000, \"time it took to get batch\")\n",
    "\n",
    "print(\"total time it took\", int((t8 - t1)), \"seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava_hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
