{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e9ee3e082ac4351b6539993d784f82c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 69 files:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Loading model in trainable mode...\n",
      "Model base:  liuhaotian/llava-v1.5-7b\n",
      "Loading LLaVA from base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c0c3d6ccd14aac8dbdc9027f28b3c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "093ff6ab3ff14afcafbc0f39685265ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading additional LLaVA weights...\n",
      "Loading LoRA weights...\n",
      "Model is loaded with unmerged and trainable LoRA weights...\n",
      "Using downloaded and verified file: /tmp/biovil_t_image_model_proj_size_128.pt\n",
      "Loaded additional vision tower weights...\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.getipython import get_ipython\n",
    "\n",
    "ipython_client = get_ipython()\n",
    "if ipython_client:\n",
    "    ipython_client.run_line_magic(magic_name=\"load_ext\", line=\"autoreload\")\n",
    "    ipython_client.run_line_magic(magic_name=\"autoreload\", line=\"2\")\n",
    "\n",
    "from RewardingVisualDoubt import dataset, inference, mimic_cxr, prompter, shared, vllm\n",
    "\n",
    "tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "\n",
    "# %% load the model\n",
    "model = vllm.load_pretrained_llava_model(is_lora_trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine trainable parameters of the Llava model and its TRL-ready version (in training mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without value head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the non-TRL Llava-LoRA model's trainable parameters\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n"
     ]
    }
   ],
   "source": [
    "print(\"Printing the non-TRL Llava-LoRA model's trainable parameters\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After value head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the TRL-Llava model's trainable parameters\n",
      "pretrained_model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "v_head.summary.weight torch.Size([1, 4096])\n",
      "v_head.summary.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "trl_model = vllm.add_value_head_to_LlavaLlamaForCausalLM_model(model)\n",
    "print(\"Printing the TRL-Llava model's trainable parameters\")\n",
    "for name, param in trl_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affirm non-trainable params in non-training (inference) mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model in non-trainable mode...\n",
      "Model base:  liuhaotian/llava-v1.5-7b\n",
      "Loading LLaVA from base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f28e093329284049b8869f6fd04ec4b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd390fd7f604e24bc0fa3fc912f17fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading additional LLaVA weights...\n",
      "Loading LoRA weights...\n",
      "Merging LoRA weights...\n",
      "Model is loaded with merged and unloaded LoRA weights...\n",
      "Using downloaded and verified file: /tmp/biovil_t_image_model_proj_size_128.pt\n",
      "Loaded additional vision tower weights...\n"
     ]
    }
   ],
   "source": [
    "# %% load the model\n",
    "model = vllm.load_pretrained_llava_model(is_lora_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the non-TRL Llava-LoRA model's trainable parameters\n"
     ]
    }
   ],
   "source": [
    "print(\"Printing the non-TRL Llava-LoRA model's trainable parameters\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=18460230, study_id=53631792, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p18/p18460230/s53631792/369dc5bd-70bd89d0-2d90fa80-f319ec1d-fb2802aa.jpg', disease=<ChexpertFinding.PLEURAL_EFFUSION: 'Pleural Effusion'>, label=<ChexpertLabel.POSITIVE: 1.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pleural Effusion?  ASSISTANT:\"]\n",
      "Label: tensor([1.], dtype=torch.float32)\n",
      "File_idx 0, ASSISTANT:  Yes, the image shows pleural effusion.\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=13263843, study_id=52138943, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p13/p13263843/s52138943/de739d0b-2345495b-255f0e3b-00ccbf4c-ab4d3400.jpg', disease=<ChexpertFinding.PLEURAL_EFFUSION: 'Pleural Effusion'>, label=<ChexpertLabel.POSITIVE: 1.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pleural Effusion?  ASSISTANT:\"]\n",
      "Label: tensor([1.], dtype=torch.float32)\n",
      "File_idx 1, ASSISTANT:  Yes, the image shows pleural effusion.\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=16050730, study_id=57637607, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p16/p16050730/s57637607/adb48138-344feb7e-14e31d10-2639c54e-0b5a95d7.jpg', disease=<ChexpertFinding.PNEUMOTHORAX: 'Pneumothorax'>, label=<ChexpertLabel.NEGATIVE: 0.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pneumothorax?  ASSISTANT:\"]\n",
      "Label: tensor([0.], dtype=torch.float32)\n",
      "File_idx 2, ASSISTANT:  No, the image shows no Pneumothorax.\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=11569093, study_id=57204814, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p11/p11569093/s57204814/0d2a50a2-3711662a-d7838521-4dc58d09-3732a3ad.jpg', disease=<ChexpertFinding.SUPPORT_DEVICES: 'Support Devices'>, label=<ChexpertLabel.POSITIVE: 1.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Support Devices?  ASSISTANT:\"]\n",
      "Label: tensor([1.], dtype=torch.float32)\n",
      "File_idx 3, ASSISTANT:  Yes, there is evidence of that in the image.\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=15192710, study_id=58836461, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p15/p15192710/s58836461/dc93422b-fd5ec685-19eb4eba-fb31f8d0-b60d8b47.jpg', disease=<ChexpertFinding.PNEUMOTHORAX: 'Pneumothorax'>, label=<ChexpertLabel.NEGATIVE: 0.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pneumothorax?  ASSISTANT:\"]\n",
      "Label: tensor([0.], dtype=torch.float32)\n",
      "File_idx 4, ASSISTANT:  No, the image shows no Pneumothorax.\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=16622813, study_id=50921860, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p16/p16622813/s50921860/066a59e3-316782a3-2d4238bc-d5354678-1ec6dcd9.jpg', disease=<ChexpertFinding.CARDIOMEGALY: 'Cardiomegaly'>, label=<ChexpertLabel.NEGATIVE: 0.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Cardiomegaly?  ASSISTANT:\"]\n",
      "Label: tensor([0.], dtype=torch.float32)\n",
      "File_idx 5, ASSISTANT:  No, there is no evidence of that in the image.\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=15114531, study_id=59999832, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p15/p15114531/s59999832/0636d0c0-a771097e-ac0c52a9-9124a5d0-95b0bc51.jpg', disease=<ChexpertFinding.CARDIOMEGALY: 'Cardiomegaly'>, label=<ChexpertLabel.NEGATIVE: 0.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Cardiomegaly?  ASSISTANT:\"]\n",
      "Label: tensor([0.], dtype=torch.float32)\n",
      "File_idx 6, ASSISTANT:  No, there is no evidence of that in the image.\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=11880923, study_id=50720959, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p11/p11880923/s50720959/6aff92fc-a55af9c9-b11a0394-d2d62191-122cdf01.jpg', disease=<ChexpertFinding.SUPPORT_DEVICES: 'Support Devices'>, label=<ChexpertLabel.POSITIVE: 1.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Support Devices?  ASSISTANT:\"]\n",
      "Label: tensor([1.], dtype=torch.float32)\n",
      "File_idx 7, ASSISTANT:  Yes, there is evidence of that in the image.\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=18929056, study_id=54821838, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p18/p18929056/s54821838/2e63cbea-9e89b6ef-7aa9d94c-5c2f5dbd-2969f6e4.jpg', disease=<ChexpertFinding.EDEMA: 'Edema'>, label=<ChexpertLabel.NEGATIVE: 0.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Edema?  ASSISTANT:\"]\n",
      "Label: tensor([0.], dtype=torch.float32)\n",
      "File_idx 8, ASSISTANT:  No, there is no evidence of that in the image.\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=15131736, study_id=57458228, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p15/p15131736/s57458228/344efa4b-02fb5b16-9db4229a-51955f21-7522b595.jpg', disease=<ChexpertFinding.PNEUMONIA: 'Pneumonia'>, label=<ChexpertLabel.POSITIVE: 1.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pneumonia?  ASSISTANT:\"]\n",
      "Label: tensor([1.], dtype=torch.float32)\n",
      "File_idx 9, ASSISTANT:  No, there is no evidence of that in the image.\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=11512104, study_id=52755492, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p11/p11512104/s52755492/daadbbe5-bd29b8d3-fe366d5f-a0e138d8-df1c2298.jpg', disease=<ChexpertFinding.CARDIOMEGALY: 'Cardiomegaly'>, label=<ChexpertLabel.POSITIVE: 1.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Cardiomegaly?  ASSISTANT:\"]\n",
      "Label: tensor([1.], dtype=torch.float32)\n",
      "File_idx 10, ASSISTANT:  Yes, the image shows cardiomegaly.\n"
     ]
    }
   ],
   "source": [
    "from LLAVA_Biovil.llava.mm_utils import KeywordsStoppingCriteria\n",
    "from LLAVA_Biovil.llava.conversation import SeparatorStyle, conv_vicuna_v1\n",
    "import torch\n",
    "\n",
    "STOP_STR = (\n",
    "    conv_vicuna_v1.copy().sep\n",
    "    if conv_vicuna_v1.copy().sep_style != SeparatorStyle.TWO\n",
    "    else conv_vicuna_v1.copy().sep2\n",
    ")\n",
    "\n",
    "tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "dataset_test = dataset.get_binary_qa_prompted_mimic_cxr_llava_model_input_dataset(\n",
    "    split=dataset.DatasetSplit.TEST,\n",
    "    tokenizer=tokenizer,\n",
    "    prompter=prompter.build_binary_qa_instruction_from_disease_under_study,\n",
    ")\n",
    "dataloader_test = dataset.get_mimic_cxr_llava_model_input_dataloader(\n",
    "    dataset=dataset_test, batch_size=1, padding_value=tokenizer.eos_token_id, num_workers=8\n",
    ")\n",
    "\n",
    "for idx, datapoint in enumerate(dataloader_test):\n",
    "    llava_model_input_dict = datapoint[0]\n",
    "    llava_model_input_dict = dataset.move_llava_model_input_dict_to_device(\n",
    "        llava_model_input_dict, torch.device(shared.torch_devices.cuda.value)\n",
    "    )\n",
    "    input_ids, images = (\n",
    "        llava_model_input_dict[\"text_prompt_input_ids\"],\n",
    "        llava_model_input_dict[\"images\"],\n",
    "    )\n",
    "    stopping_criteria = KeywordsStoppingCriteria([STOP_STR], tokenizer, input_ids)\n",
    "    pred = inference.generate_radialog_answer_for_binary_qa_for_single_study(\n",
    "        model, tokenizer, input_ids, images, stopping_criteria\n",
    "    )\n",
    "    print(f\"\\n Metadata: {datapoint[3]}\")\n",
    "    print(f\"Prompt: {datapoint[2]}\")\n",
    "    print(f\"Label:\", datapoint[1])\n",
    "    print(f\"File_idx {idx}, ASSISTANT: \", pred)\n",
    "    if idx == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFT LoRA Config (in trainable mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft_type: LORA\n",
      "auto_mapping: None\n",
      "base_model_name_or_path: liuhaotian/llava-v1.5-7b\n",
      "revision: None\n",
      "task_type: CAUSAL_LM\n",
      "inference_mode: False\n",
      "r: 128\n",
      "target_modules: ['gate_proj', 'v_proj', 'o_proj', 'k_proj', 'down_proj', 'up_proj', 'q_proj']\n",
      "lora_alpha: 256\n",
      "lora_dropout: 0.05\n",
      "fan_in_fan_out: False\n",
      "bias: none\n",
      "modules_to_save: None\n",
      "init_lora_weights: True\n",
      "layers_to_transform: None\n",
      "layers_pattern: None\n"
     ]
    }
   ],
   "source": [
    "import dataclasses\n",
    "\n",
    "for field in dataclasses.fields(vllm._extract_lora_config_from_model(model)):\n",
    "    print(f\"{field.name}: {getattr(vllm._extract_lora_config_from_model(model), field.name)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LlavaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4484b7bea1ef4b4689e8c487581ae2f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 69 files:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlavaConfig {\n",
       "  \"_name_or_path\": \"/home/guests/deniz_gueler/.cache/huggingface/hub/models--ChantalPellegrini--RaDialog-interactive-radiology-report-generation/snapshots/14b6bf53a4105c7080f6f4ec6ff2f3b806a580a3\",\n",
       "  \"architectures\": [\n",
       "    \"LlavaLlamaForCausalLM\"\n",
       "  ],\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"freeze_mm_mlp_adapter\": false,\n",
       "  \"freeze_mm_vision_resampler\": false,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"image_aspect_ratio\": \"pad\",\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 11008,\n",
       "  \"max_length\": 4096,\n",
       "  \"max_position_embeddings\": 4096,\n",
       "  \"mm_hidden_size\": 512,\n",
       "  \"mm_projector_lr\": 2e-05,\n",
       "  \"mm_projector_type\": \"mlp2x_gelu\",\n",
       "  \"mm_resampler_type\": null,\n",
       "  \"mm_use_im_patch_token\": false,\n",
       "  \"mm_use_im_start_end\": false,\n",
       "  \"mm_vision_select_feature\": \"patch\",\n",
       "  \"mm_vision_select_layer\": -2,\n",
       "  \"mm_vision_tower\": \"biovil\",\n",
       "  \"model_type\": \"llava\",\n",
       "  \"mv_type\": \"concat\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"tokenizer_model_max_length\": 1300,\n",
       "  \"tokenizer_padding_side\": \"right\",\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.31.0\",\n",
       "  \"tune_mm_mlp_adapter\": false,\n",
       "  \"tune_mm_vision_resampler\": false,\n",
       "  \"unfreeze_mm_vision_tower\": false,\n",
       "  \"use_cache\": false,\n",
       "  \"use_mm_proj\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vllm._get_finetuned_llava_config(model_path=vllm._get_hf_model_path(vllm.FINETUNED_LLAVA_REPO_ID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Methods for Future Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft.tuners.lora import mark_only_lora_as_trainable\n",
    "\n",
    "mark_only_lora_as_trainable(model)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        param.requires_grad = True  # Keep LoRA trainable\n",
    "    else:\n",
    "        param.requires_grad = False  # Freeze base model if needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava_hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
