{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f1380cd8c6404f9fe8dd88fb6e5225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 69 files:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guests/deniz_gueler/miniconda3/envs/llava_hf/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model in non-trainable mode...\n",
      "Model base:  liuhaotian/llava-v1.5-7b\n",
      "Loading LLaVA from base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b309499da8134d53998d3a0e256ee870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420b56687b54470a97a3bb7a56e5d4bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading additional LLaVA weights...\n",
      "Using downloaded and verified file: /tmp/biovil_t_image_model_proj_size_128.pt\n",
      "Loaded additional vision tower weights...\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.getipython import get_ipython\n",
    "\n",
    "ipython_client = get_ipython()\n",
    "if ipython_client:\n",
    "    ipython_client.run_line_magic(magic_name=\"load_ext\", line=\"autoreload\")\n",
    "    ipython_client.run_line_magic(magic_name=\"autoreload\", line=\"2\")\n",
    "\n",
    "from RewardingVisualDoubt import (\n",
    "    dataset,\n",
    "    inference,\n",
    "    mimic_cxr,\n",
    "    prompter,\n",
    "    shared,\n",
    "    vllm,\n",
    "    infrastructure,\n",
    ")\n",
    "\n",
    "import torch\n",
    "\n",
    "tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "\n",
    "# %% load the model\n",
    "# model = vllm.load_pretrained_llava_model(\n",
    "#     is_lora_trainable=False, load_quantized=False, quantization_type=\"8bit\", skip_lora_adapters=True\n",
    "# )\n",
    "\n",
    "model = vllm.load_pretrained_llava_model(skip_lora_adapters=True)\n",
    "model = vllm.add_pretrained_RaDialog_lora_adapters_and_value_head_to_LlavaLlamaForCausalLM_model(\n",
    "    model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Causal LM Model to TRL-LoRA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peft_type = (\"LORA\",)\n",
    "# auto_mapping = (None,)\n",
    "# base_model_name_or_path = (\"liuhaotian/llava-v1.5-7b\",)\n",
    "# evision = (None,)\n",
    "# task_type = (\"CAUSAL_LM\",)\n",
    "# inference_mode = (True,)\n",
    "# r = (128,)\n",
    "# target_modules = ([\"gate_proj\", \"v_proj\", \"o_proj\", \"k_proj\", \"down_proj\", \"up_proj\", \"q_proj\"],)\n",
    "# lora_alpha = (256,)\n",
    "# lora_dropout = (0.05,)\n",
    "# fan_in_fan_out = (False,)\n",
    "# bias = (\"none\",)\n",
    "# modules_to_save = (None,)\n",
    "# init_lora_weights = (True,)\n",
    "# layers_to_transform = (None,)\n",
    "# layers_pattern = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "\n",
    "# model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "\n",
    "# peft_type: \"LORA\"\n",
    "# base_model_name_or_path \"liuhaotian/llava-v1.5-7b\",\n",
    "# inference_mode: true,\n",
    "# auto_mapping: null,\n",
    "# revision: null\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=128,\n",
    "    target_modules=[\"gate_proj\", \"v_proj\", \"o_proj\", \"k_proj\", \"down_proj\", \"up_proj\", \"q_proj\"],\n",
    "    lora_alpha=256,\n",
    "    lora_dropout=0.05,\n",
    "    fan_in_fan_out=False,\n",
    "    bias=\"none\",\n",
    "    modules_to_save=None,\n",
    "    init_lora_weights=False,  # True,\n",
    "    layers_to_transform=None,\n",
    "    layers_pattern=None,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "trl_lora_model = AutoModelForCausalLMWithValueHead.from_pretrained(model, peft_config=lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "TRAINED_LORA_WEIGHTS_DIR = (\n",
    "    \"/home/guests/deniz_gueler/repos/RewardingVisualDoubt/data/RaDialog_adapter_model.bin\"\n",
    ")\n",
    "\n",
    "adapter_state_dict = torch.load(TRAINED_LORA_WEIGHTS_DIR, map_location=\"cpu\")\n",
    "\n",
    "# Filter only LoRA layers if necessary\n",
    "mapped_state_dict = {}\n",
    "\n",
    "for k, v in adapter_state_dict.items():\n",
    "    # Replace the prefix to match the loaded model structure\n",
    "    new_key = k.replace(\"base_model.\", \"pretrained_model.base_model.\")\n",
    "    new_key = new_key.split(\".weight\")[0] + \".default\" + \".weight\"\n",
    "    mapped_state_dict[new_key] = v\n",
    "\n",
    "missing_keys, unexpected_keys = trl_lora_model.load_state_dict(mapped_state_dict, strict=False)\n",
    "\n",
    "assert unexpected_keys == []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine trainable parameters of the Llava model and its TRL-ready version (in training mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without value head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the non-TRL Llava-LoRA model's trainable parameters\n"
     ]
    }
   ],
   "source": [
    "print(\"Printing the non-TRL Llava-LoRA model's trainable parameters\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After value head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trl_model = vllm.add_value_head_to_LlavaLlamaForCausalLM_model(model)\n",
    "print(\"Printing the TRL-Llava model's trainable parameters\")\n",
    "for name, param in trl_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affirm non-trainable params in non-training (inference) mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% load the model\n",
    "model = vllm.load_pretrained_llava_model(is_lora_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the non-TRL Llava-LoRA model's trainable parameters\n",
      "pretrained_model.base_model.model.model.embed_tokens.weight torch.Size([32001, 4096])\n",
      "pretrained_model.base_model.model.model.layers.0.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.0.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.0.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.0.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.0.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.0.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.0.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.0.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.0.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.1.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.1.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.1.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.1.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.1.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.1.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.1.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.1.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.1.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.2.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.2.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.2.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.2.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.2.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.2.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.2.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.2.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.2.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.3.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.3.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.3.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.3.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.3.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.3.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.3.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.3.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.3.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.4.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.4.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.4.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.4.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.4.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.4.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.4.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.4.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.4.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.5.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.5.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.5.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.5.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.5.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.5.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.5.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.5.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.5.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.6.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.6.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.6.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.6.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.6.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.6.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.6.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.6.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.6.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.7.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.7.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.7.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.7.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.7.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.7.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.7.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.7.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.7.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.8.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.8.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.8.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.8.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.8.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.8.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.8.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.8.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.8.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.9.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.9.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.9.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.9.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.9.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.9.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.9.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.9.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.9.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.10.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.10.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.10.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.10.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.10.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.10.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.10.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.10.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.10.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.11.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.11.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.11.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.11.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.11.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.11.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.11.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.11.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.11.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.12.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.12.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.12.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.12.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.12.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.12.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.12.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.12.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.12.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.13.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.13.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.13.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.13.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.13.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.13.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.13.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.13.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.13.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.14.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.14.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.14.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.14.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.14.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.14.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.14.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.14.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.14.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.15.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.15.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.15.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.15.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.15.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.15.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.15.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.15.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.15.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.16.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.16.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.16.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.16.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.16.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.16.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.16.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.16.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.16.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.17.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.17.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.17.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.17.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.17.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.17.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.17.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.17.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.17.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.18.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.18.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.18.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.18.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.18.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.18.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.18.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.18.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.18.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.19.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.19.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.19.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.19.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.19.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.19.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.19.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.19.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.19.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.20.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.20.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.20.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.20.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.20.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.20.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.20.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.20.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.20.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.21.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.21.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.21.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.21.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.21.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.21.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.21.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.21.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.21.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.22.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.22.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.22.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.22.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.22.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.22.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.22.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.22.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.22.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.23.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.23.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.23.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.23.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.23.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.23.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.23.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.23.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.23.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.24.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.24.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.24.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.24.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.24.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.24.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.24.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.24.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.24.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.25.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.25.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.25.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.25.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.25.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.25.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.25.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.25.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.25.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.26.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.26.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.26.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.26.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.26.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.26.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.26.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.26.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.26.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.27.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.27.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.27.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.27.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.27.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.27.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.27.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.27.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.27.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.28.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.28.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.28.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.28.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.28.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.28.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.28.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.28.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.28.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.29.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.29.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.29.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.29.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.29.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.29.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.29.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.29.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.29.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.30.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.30.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.30.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.30.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.30.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.30.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.30.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.30.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.30.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.31.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.31.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.31.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.31.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.31.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.31.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "pretrained_model.base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight torch.Size([128, 4096])\n",
      "pretrained_model.base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight torch.Size([11008, 128])\n",
      "pretrained_model.base_model.model.model.layers.31.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "pretrained_model.base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight torch.Size([128, 11008])\n",
      "pretrained_model.base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight torch.Size([4096, 128])\n",
      "pretrained_model.base_model.model.model.layers.31.input_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.layers.31.post_attention_layernorm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.norm.weight torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.mm_projector.0.weight torch.Size([4096, 512])\n",
      "pretrained_model.base_model.model.model.mm_projector.0.bias torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.mm_projector.2.weight torch.Size([4096, 4096])\n",
      "pretrained_model.base_model.model.model.mm_projector.2.bias torch.Size([4096])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.missing_previous_emb torch.Size([1, 256, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.conv1.weight torch.Size([64, 3, 7, 7])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.bn1.weight torch.Size([64])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.bn1.bias torch.Size([64])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer1.0.conv1.weight torch.Size([64, 64, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer1.0.bn1.weight torch.Size([64])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer1.0.bn1.bias torch.Size([64])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer1.0.bn2.weight torch.Size([64])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer1.0.bn2.bias torch.Size([64])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer1.0.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer1.0.bn3.weight torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer1.0.bn3.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer1.0.downsample.0.weight torch.Size([256, 64, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer1.0.downsample.1.weight torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer1.0.downsample.1.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer1.1.conv1.weight torch.Size([64, 256, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer1.1.bn1.weight torch.Size([64])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer1.1.bn1.bias torch.Size([64])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer1.1.bn2.weight torch.Size([64])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer1.1.bn2.bias torch.Size([64])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer1.1.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer1.1.bn3.weight torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer1.1.bn3.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer1.2.conv1.weight torch.Size([64, 256, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer1.2.bn1.weight torch.Size([64])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer1.2.bn1.bias torch.Size([64])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer1.2.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer1.2.bn2.weight torch.Size([64])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer1.2.bn2.bias torch.Size([64])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer1.2.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer1.2.bn3.weight torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer1.2.bn3.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.0.conv1.weight torch.Size([128, 256, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.0.bn1.weight torch.Size([128])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.0.bn1.bias torch.Size([128])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.0.bn2.weight torch.Size([128])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.0.bn2.bias torch.Size([128])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.0.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.0.bn3.weight torch.Size([512])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.0.bn3.bias torch.Size([512])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.0.downsample.1.weight torch.Size([512])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.0.downsample.1.bias torch.Size([512])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.1.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.1.bn1.weight torch.Size([128])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.1.bn1.bias torch.Size([128])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.1.bn2.weight torch.Size([128])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.1.bn2.bias torch.Size([128])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.1.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.1.bn3.weight torch.Size([512])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.1.bn3.bias torch.Size([512])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.2.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.2.bn1.weight torch.Size([128])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.2.bn1.bias torch.Size([128])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.2.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.2.bn2.weight torch.Size([128])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.2.bn2.bias torch.Size([128])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.2.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.2.bn3.weight torch.Size([512])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.2.bn3.bias torch.Size([512])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.3.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.3.bn1.weight torch.Size([128])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.3.bn1.bias torch.Size([128])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.3.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.3.bn2.weight torch.Size([128])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.3.bn2.bias torch.Size([128])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.3.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.3.bn3.weight torch.Size([512])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer2.3.bn3.bias torch.Size([512])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.0.conv1.weight torch.Size([256, 512, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.0.bn1.weight torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.0.bn1.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.0.bn2.weight torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.0.bn2.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.0.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.0.bn3.weight torch.Size([1024])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.0.bn3.bias torch.Size([1024])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.0.downsample.0.weight torch.Size([1024, 512, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.0.downsample.1.weight torch.Size([1024])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.0.downsample.1.bias torch.Size([1024])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.1.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.1.bn1.weight torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.1.bn1.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.1.bn2.weight torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.1.bn2.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.1.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.1.bn3.weight torch.Size([1024])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.1.bn3.bias torch.Size([1024])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.2.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.2.bn1.weight torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.2.bn1.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.2.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.2.bn2.weight torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.2.bn2.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.2.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.2.bn3.weight torch.Size([1024])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.2.bn3.bias torch.Size([1024])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.3.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.3.bn1.weight torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.3.bn1.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.3.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.3.bn2.weight torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.3.bn2.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.3.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.3.bn3.weight torch.Size([1024])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.3.bn3.bias torch.Size([1024])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.4.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.4.bn1.weight torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.4.bn1.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.4.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.4.bn2.weight torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.4.bn2.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.4.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.4.bn3.weight torch.Size([1024])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.4.bn3.bias torch.Size([1024])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.5.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.5.bn1.weight torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.5.bn1.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.5.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.5.bn2.weight torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.5.bn2.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.5.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.5.bn3.weight torch.Size([1024])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer3.5.bn3.bias torch.Size([1024])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer4.0.conv1.weight torch.Size([512, 1024, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer4.0.bn1.weight torch.Size([512])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer4.0.bn1.bias torch.Size([512])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer4.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer4.0.bn2.weight torch.Size([512])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer4.0.bn2.bias torch.Size([512])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer4.0.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer4.0.bn3.weight torch.Size([2048])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer4.0.bn3.bias torch.Size([2048])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer4.0.downsample.0.weight torch.Size([2048, 1024, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer4.0.downsample.1.weight torch.Size([2048])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer4.0.downsample.1.bias torch.Size([2048])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer4.1.conv1.weight torch.Size([512, 2048, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer4.1.bn1.weight torch.Size([512])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer4.1.bn1.bias torch.Size([512])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer4.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer4.1.bn2.weight torch.Size([512])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer4.1.bn2.bias torch.Size([512])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer4.1.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer4.1.bn3.weight torch.Size([2048])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer4.1.bn3.bias torch.Size([2048])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer4.2.conv1.weight torch.Size([512, 2048, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer4.2.bn1.weight torch.Size([512])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer4.2.bn1.bias torch.Size([512])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer4.2.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer4.2.bn2.weight torch.Size([512])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer4.2.bn2.bias torch.Size([512])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer4.2.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer4.2.bn3.weight torch.Size([2048])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.layer4.2.bn3.bias torch.Size([2048])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.fc.weight torch.Size([1000, 2048])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.encoder.fc.bias torch.Size([1000])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.backbone_to_vit.weight torch.Size([256, 2048, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.type_embed torch.Size([2, 1, 256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.0.norm1.weight torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.0.norm1.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.0.attn.proj_q.weight torch.Size([256, 256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.0.attn.proj_k.weight torch.Size([256, 256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.0.attn.proj_v.weight torch.Size([256, 256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.0.attn.proj.weight torch.Size([256, 256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.0.attn.proj.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.0.norm2.weight torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.0.norm2.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.0.mlp.fc1.weight torch.Size([256, 256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.0.mlp.fc1.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.0.mlp.fc2.weight torch.Size([256, 256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.0.mlp.fc2.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.1.norm1.weight torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.1.norm1.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.1.attn.proj_q.weight torch.Size([256, 256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.1.attn.proj_k.weight torch.Size([256, 256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.1.attn.proj_v.weight torch.Size([256, 256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.1.attn.proj.weight torch.Size([256, 256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.1.attn.proj.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.1.norm2.weight torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.1.norm2.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.1.mlp.fc1.weight torch.Size([256, 256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.1.mlp.fc1.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.1.mlp.fc2.weight torch.Size([256, 256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.1.mlp.fc2.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.2.norm1.weight torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.2.norm1.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.2.attn.proj_q.weight torch.Size([256, 256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.2.attn.proj_k.weight torch.Size([256, 256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.2.attn.proj_v.weight torch.Size([256, 256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.2.attn.proj.weight torch.Size([256, 256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.2.attn.proj.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.2.norm2.weight torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.2.norm2.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.2.mlp.fc1.weight torch.Size([256, 256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.2.mlp.fc1.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.2.mlp.fc2.weight torch.Size([256, 256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.blocks.2.mlp.fc2.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.norm_post.weight torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.encoder.vit_pooler.norm_post.bias torch.Size([256])\n",
      "pretrained_model.base_model.model.model.vision_tower.projector.model.0.weight torch.Size([128, 512, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.projector.model.1.weight torch.Size([128])\n",
      "pretrained_model.base_model.model.model.vision_tower.projector.model.1.bias torch.Size([128])\n",
      "pretrained_model.base_model.model.model.vision_tower.projector.model.3.weight torch.Size([128, 128, 1, 1])\n",
      "pretrained_model.base_model.model.model.vision_tower.projector.model.3.bias torch.Size([128])\n",
      "pretrained_model.base_model.model.lm_head.weight torch.Size([32001, 4096])\n",
      "v_head.summary.weight torch.Size([1, 4096])\n",
      "v_head.summary.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "print(\"Printing the non-TRL Llava-LoRA model's trainable parameters\")\n",
    "for name, param in model.named_parameters():\n",
    "    # if param.requires_grad:\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Test the model: Generate a few responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RewardingVisualDoubt import dataset, vllm, prompter, mimic_cxr, shared, inference\n",
    "from LLAVA_Biovil.llava.mm_utils import KeywordsStoppingCriteria\n",
    "from LLAVA_Biovil.llava.conversation import SeparatorStyle, conv_vicuna_v1\n",
    "import torch\n",
    "\n",
    "STOP_STR = (\n",
    "    conv_vicuna_v1.copy().sep\n",
    "    if conv_vicuna_v1.copy().sep_style != SeparatorStyle.TWO\n",
    "    else conv_vicuna_v1.copy().sep2\n",
    ")\n",
    "\n",
    "tokenizer = vllm.load_pretrained_llava_tokenizer_with_image_support(\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME\n",
    ")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "dataset_test = dataset.get_binary_qa_prompted_mimic_cxr_llava_model_input_dataset(\n",
    "    split=dataset.DatasetSplit.TEST,\n",
    "    tokenizer=tokenizer,\n",
    "    prompter=prompter.build_binary_qa_instruction_from_disease_under_study,\n",
    ")\n",
    "dataloader_test = dataset.get_mimic_cxr_llava_model_input_dataloader(\n",
    "    dataset=dataset_test, batch_size=1, padding_value=tokenizer.eos_token_id, num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=18460230, study_id=53631792, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p18/p18460230/s53631792/369dc5bd-70bd89d0-2d90fa80-f319ec1d-fb2802aa.jpg', disease=<ChexpertFinding.PLEURAL_EFFUSION: 'Pleural Effusion'>, label=<ChexpertLabel.POSITIVE: 1.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pleural Effusion?  ASSISTANT:\"]\n",
      "Label: tensor([1.])\n",
      "File_idx 0, ASSISTANT:  Yes, the image shows pleural effusion.\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=13263843, study_id=52138943, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p13/p13263843/s52138943/de739d0b-2345495b-255f0e3b-00ccbf4c-ab4d3400.jpg', disease=<ChexpertFinding.PLEURAL_EFFUSION: 'Pleural Effusion'>, label=<ChexpertLabel.POSITIVE: 1.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pleural Effusion?  ASSISTANT:\"]\n",
      "Label: tensor([1.])\n",
      "File_idx 1, ASSISTANT:  Yes, the image shows pleural effusion.\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=16050730, study_id=57637607, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p16/p16050730/s57637607/adb48138-344feb7e-14e31d10-2639c54e-0b5a95d7.jpg', disease=<ChexpertFinding.PNEUMOTHORAX: 'Pneumothorax'>, label=<ChexpertLabel.NEGATIVE: 0.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pneumothorax?  ASSISTANT:\"]\n",
      "Label: tensor([0.])\n",
      "File_idx 2, ASSISTANT:  No, the image shows no Pneumothorax.\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=11569093, study_id=57204814, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p11/p11569093/s57204814/0d2a50a2-3711662a-d7838521-4dc58d09-3732a3ad.jpg', disease=<ChexpertFinding.SUPPORT_DEVICES: 'Support Devices'>, label=<ChexpertLabel.POSITIVE: 1.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Support Devices?  ASSISTANT:\"]\n",
      "Label: tensor([1.])\n",
      "File_idx 3, ASSISTANT:  Yes, the image shows support devices.\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=15192710, study_id=58836461, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p15/p15192710/s58836461/dc93422b-fd5ec685-19eb4eba-fb31f8d0-b60d8b47.jpg', disease=<ChexpertFinding.PNEUMOTHORAX: 'Pneumothorax'>, label=<ChexpertLabel.NEGATIVE: 0.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pneumothorax?  ASSISTANT:\"]\n",
      "Label: tensor([0.])\n",
      "File_idx 4, ASSISTANT:  No, the image shows no pneumothorax.\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=16622813, study_id=50921860, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p16/p16622813/s50921860/066a59e3-316782a3-2d4238bc-d5354678-1ec6dcd9.jpg', disease=<ChexpertFinding.CARDIOMEGALY: 'Cardiomegaly'>, label=<ChexpertLabel.NEGATIVE: 0.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Cardiomegaly?  ASSISTANT:\"]\n",
      "Label: tensor([0.])\n",
      "File_idx 5, ASSISTANT:  No, there is no evidence of that in the image.\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=15114531, study_id=59999832, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p15/p15114531/s59999832/0636d0c0-a771097e-ac0c52a9-9124a5d0-95b0bc51.jpg', disease=<ChexpertFinding.CARDIOMEGALY: 'Cardiomegaly'>, label=<ChexpertLabel.NEGATIVE: 0.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Cardiomegaly?  ASSISTANT:\"]\n",
      "Label: tensor([0.])\n",
      "File_idx 6, ASSISTANT:  No, there is no evidence of that in the image.\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=11880923, study_id=50720959, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p11/p11880923/s50720959/6aff92fc-a55af9c9-b11a0394-d2d62191-122cdf01.jpg', disease=<ChexpertFinding.SUPPORT_DEVICES: 'Support Devices'>, label=<ChexpertLabel.POSITIVE: 1.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Support Devices?  ASSISTANT:\"]\n",
      "Label: tensor([1.])\n",
      "File_idx 7, ASSISTANT:  Yes, there is evidence of that in the image.\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=18929056, study_id=54821838, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p18/p18929056/s54821838/2e63cbea-9e89b6ef-7aa9d94c-5c2f5dbd-2969f6e4.jpg', disease=<ChexpertFinding.EDEMA: 'Edema'>, label=<ChexpertLabel.NEGATIVE: 0.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Edema?  ASSISTANT:\"]\n",
      "Label: tensor([0.])\n",
      "File_idx 8, ASSISTANT:  No, there is no evidence of that in the image.\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=15131736, study_id=57458228, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p15/p15131736/s57458228/344efa4b-02fb5b16-9db4229a-51955f21-7522b595.jpg', disease=<ChexpertFinding.PNEUMONIA: 'Pneumonia'>, label=<ChexpertLabel.POSITIVE: 1.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Pneumonia?  ASSISTANT:\"]\n",
      "Label: tensor([1.])\n",
      "File_idx 9, ASSISTANT:  No, there is no evidence of that in the image.\n",
      "\n",
      " Metadata: [MimicCxrBinaryQADatapoint(subject_id=11512104, study_id=52755492, img_path='/home/data/DIVA/mimic/mimic-cxr-jpg/2.0.0/files/p11/p11512104/s52755492/daadbbe5-bd29b8d3-fe366d5f-a0e138d8-df1c2298.jpg', disease=<ChexpertFinding.CARDIOMEGALY: 'Cardiomegaly'>, label=<ChexpertLabel.POSITIVE: 1.0>)]\n",
      "Prompt: [\"A chat between a curious user and an artificial intelligence assistant acting as an experienced radiologist. The assistant gives professional, detailed, and polite answers to the user's questions. USER: <image>. You are to act as a radiologist and answer the following question: Is the following disease visible in the given X-ray image: Cardiomegaly?  ASSISTANT:\"]\n",
      "Label: tensor([1.])\n",
      "File_idx 10, ASSISTANT:  Yes, the image shows cardiomegaly.\n"
     ]
    }
   ],
   "source": [
    "for idx, datapoint in enumerate(dataloader_test):\n",
    "    llava_model_input_dict = datapoint[0]\n",
    "    llava_model_input_dict = dataset.move_llava_model_input_dict_to_device(\n",
    "        llava_model_input_dict, torch.device(shared.torch_devices.cuda.value)\n",
    "    )\n",
    "    input_ids, images = (\n",
    "        llava_model_input_dict[\"text_prompt_input_ids\"],\n",
    "        llava_model_input_dict[\"images\"],\n",
    "    )\n",
    "    stopping_criteria = KeywordsStoppingCriteria([STOP_STR], tokenizer, input_ids)\n",
    "    model.train()\n",
    "    pred = inference.generate_radialog_answer_for_binary_qa_for_single_study(\n",
    "        model, tokenizer, input_ids, images, stopping_criteria\n",
    "    )\n",
    "    print(f\"\\n Metadata: {datapoint[3]}\")\n",
    "    print(f\"Prompt: {datapoint[2]}\")\n",
    "    print(f\"Label:\", datapoint[1])\n",
    "    print(f\"File_idx {idx}, ASSISTANT: \", pred)\n",
    "    if idx == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFT LoRA Config (in trainable mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "for field in dataclasses.fields(vllm._extract_lora_config_from_model(model)):\n",
    "    print(f\"{field.name}: {getattr(vllm._extract_lora_config_from_model(model), field.name)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LlavaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vllm._get_finetuned_llava_config(model_path=vllm._get_hf_model_path(vllm.FINETUNED_LLAVA_REPO_ID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Methods for Future Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft.tuners.lora import mark_only_lora_as_trainable\n",
    "\n",
    "mark_only_lora_as_trainable(model)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        param.requires_grad = True  # Keep LoRA trainable\n",
    "    else:\n",
    "        param.requires_grad = False  # Freeze base model if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LLAVA_Biovil.llava.model import builder\n",
    "from RewardingVisualDoubt import vllm\n",
    "\n",
    "model = builder.load_pretrained_model(\n",
    "    model_path=vllm._get_hf_model_path(vllm.FINETUNED_LLAVA_REPO_ID),\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME,\n",
    "    model_name=vllm.LLAVA_LORA_ADAPTER,\n",
    "    load_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model to local dir and load from local dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"merged_model_fp16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RewardingVisualDoubt import vllm\n",
    "\n",
    "model = vllm.load_pretrained_llava_model(\n",
    "    model_path=\"/home/guests/deniz_gueler/repos/RewardingVisualDoubt/examinations/merged_model_fp16\",\n",
    "    model_base=vllm.LLAVA_BASE_MODEL_NAME,\n",
    "    load_quantized=False,\n",
    "    quantization_type=\"4bit\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava_hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
